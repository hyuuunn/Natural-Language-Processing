{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9vsOXPY3Wqe"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofn4FR4V3Ynv",
    "outputId": "6f4fb05f-7bdf-4377-ba9d-883d8d14c6a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQMm9UPdBhCP",
    "outputId": "a658cccc-83ec-49d3-ed43-94c190bb93c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4 MB 4.4 MB/s \n",
      "\u001b[?25hCollecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 50.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
      "Collecting beautifulsoup4==4.6.0\n",
      "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 5.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.6.3\n",
      "    Uninstalling beautifulsoup4-4.6.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.6.3\n",
      "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2j4Q8HvpGrgd",
    "outputId": "5bc8765e-027c-4965-9c4f-e2374ca6b7f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
      "remote: Enumerating objects: 91, done.\u001b[K\n",
      "remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91\u001b[K\n",
      "Unpacking objects: 100% (91/91), done.\n",
      "/content/Mecab-ko-for-Google-Colab\n",
      "Installing konlpy.....\n",
      "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Done\n",
      "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
      "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
      "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
      "--2022-01-02 02:08:45--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
      "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db, 2406:da00:ff00::6b17:d1f5, ...\n",
      "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=EhQfXe2fyp6J6EcOvZLAv%2FEUNU0%3D&Expires=1641090940&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
      "--2022-01-02 02:08:45--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=EhQfXe2fyp6J6EcOvZLAv%2FEUNU0%3D&Expires=1641090940&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
      "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.37.156\n",
      "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.37.156|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1414979 (1.3M) [application/x-tar]\n",
      "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
      "\n",
      "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  7.63MB/s    in 0.2s    \n",
      "\n",
      "2022-01-02 02:08:46 (7.63 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
      "\n",
      "Done\n",
      "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
      "Done\n",
      "Change Directory to mecab-0.996-ko-0.9.2.......\n",
      "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
      "configure\n",
      "make\n",
      "make check\n",
      "make install\n",
      "ldconfig\n",
      "Done\n",
      "Change Directory to /content\n",
      "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
      "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
      "--2022-01-02 02:10:23--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
      "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db, 2406:da00:ff00::22c5:2ef4, ...\n",
      "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=6QZ0yLnmenirIXK9J5Nn3PUe34g%3D&Expires=1641091039&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
      "--2022-01-02 02:10:23--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=6QZ0yLnmenirIXK9J5Nn3PUe34g%3D&Expires=1641091039&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
      "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 54.231.197.233\n",
      "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|54.231.197.233|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 49775061 (47M) [application/x-tar]\n",
      "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
      "\n",
      "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  53.7MB/s    in 0.9s    \n",
      "\n",
      "2022-01-02 02:10:24 (53.7 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
      "\n",
      "Done\n",
      "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
      "Done\n",
      "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
      "Done\n",
      "installing........\n",
      "configure\n",
      "make\n",
      "make install\n",
      "apt-get update\n",
      "apt-get upgrade\n",
      "apt install curl\n",
      "apt install git\n",
      "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
      "Done\n",
      "Successfully Installed\n",
      "Now you can use Mecab\n",
      "from konlpy.tag import Mecab\n",
      "mecab = Mecab()\n",
      "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
      "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
      "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
     ]
    }
   ],
   "source": [
    "# Colab에 Mecab 설치\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "%cd Mecab-ko-for-Google-Colab\n",
    "!bash install_mecab-ko_on_colab190912.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3pxdKtsDK4E"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import *\n",
    "\n",
    "hannanum = Hannanum()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "okt = Okt()\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3z0QlzhGEJl"
   },
   "source": [
    "위 형태소 분석기들은 공통적으로 아래의 함수를 제공\n",
    "nouns : 명사 추출  \n",
    "morphs : 형태소 추출  \n",
    "pos : 품사 부착"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJt5ERvooTeG"
   },
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtEhbe_NzkbN"
   },
   "source": [
    "문장 토큰화는 단어 토큰화와는 달리 주어진 텍스트를 문장 단위로 나눌 경우에 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pquF1QYwKLcr"
   },
   "source": [
    "## 영어 : Sentence Tokenization (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMm66t0jiYgQ"
   },
   "source": [
    "문자열이 주어졌을 때, 문장 단위로 나눈다고 가정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ds6aO6Wi0dK"
   },
   "source": [
    "Yonsei University is a private research university in Seoul, South Korea. Yonsei University is deemed as one of the three most prestigious institutions in the country. It is particularly respected in the studies of medicine and business administration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGHWe5ymikx-"
   },
   "outputs": [],
   "source": [
    "temp = 'Yonsei University is a private research university in Seoul, South Korea. Yonsei University is deemed as one of the three most prestigious institutions in the country. It is particularly respected in the studies of medicine and business administration.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgROm78jjq39"
   },
   "source": [
    "문자열.split('자르는 기준')을 사용하면 해당 기준으로 문자열들을 분리하여 리스트 형태로 반환.  \n",
    "아래의 코드는 온점을 기준으로 문자열을 자르는 코드."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njRK3NrbiyFE",
    "outputId": "35efb15d-0e73-45e6-f34e-532dc106b425"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yonsei University is a private research university in Seoul, South Korea',\n",
       " 'Yonsei University is deemed as one of the three most prestigious institutions in the country',\n",
       " 'It is particularly respected in the studies of medicine and business administration.']"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.split('. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic8zCrMlKRI3"
   },
   "source": [
    "직관적으로 생각해봤을 때는 ?나 온점(.)이나 ! 기준으로 문장을 잘라내면 되지 않을까라고 생각할 수 있지만, 꼭 그렇지만은 않음. !나 ?는 문장의 구분을 위한 꽤 명확한 구분자(boundary) 역할을 하지만 온점은 꼭 그렇지 않기 때문. -> 온점은 문장의 끝이 아니더라도 등장할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcqfdjJpKUvp"
   },
   "source": [
    "**IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 ukairia777@gmail.com로 결과 좀 보내줘. 그러고나서 점심 먹으러 가자.**  \n",
    "**Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92Of0w2EKY0D"
   },
   "source": [
    "온점을 기준으로 문장을 구분할 경우에는 예외사항이 너무 많음.  \n",
    "**NLTK에서는 영어 문장의 토큰화를 수행하는 sent_tokenize를 지원. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9AKFUkjKNXF"
   },
   "outputs": [],
   "source": [
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpjJ4F_DKugH",
    "outputId": "6f712ada-fc6b-496e-c7ca-48ff5002a5ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seKDLuUbKyn3",
    "outputId": "9e3ad2fb-fb72-4c4d-81f3-c21cbaddf339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-BWco73K3x9"
   },
   "source": [
    "## 한국어 : Sentence Tokenization(KSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXDQxDBaK5nm",
    "outputId": "9a807b77-d88a-46c2-c1ea-b0d3e836a6d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-3.3.1.1.tar.gz (42.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 42.4 MB 1.3 MB/s \n",
      "\u001b[?25hCollecting emoji\n",
      "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
      "\u001b[K     |████████████████████████████████| 170 kB 51.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: kss, emoji\n",
      "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kss: filename=kss-3.3.1.1-py3-none-any.whl size=42449239 sha256=20d1735ca4c00a784ed0d7ab6f920a8b33bf11135b40c8bcd6b1f796ba5d8660\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/9d/1d/52871154eff5273abb86b96f4f984c1cd67c5bde64239b060a\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=4f461c692c177861c730a041c04fa84b3369c6c8142f01e1c8df21d1e9ab44f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
      "Successfully built kss emoji\n",
      "Installing collected packages: emoji, kss\n",
      "Successfully installed emoji-1.6.1 kss-3.3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwP2-MfNK9vD",
    "outputId": "db8db3b7-b073-4224-e935-6673986fdb8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 이제 해보면 알걸요?'\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i78rR5-3OrZ"
   },
   "source": [
    "# 한국어 띄어쓰기 패키지(pykospacing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtOclk7s3Woi"
   },
   "source": [
    "한국어의 경우 영어와는 달리 띄어쓰기가 잘 지켜지지 않음. 띄어쓰기 패키지를 통해서 띄어쓰기가 되어있지 않은 문장을 보정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDb1v53PyIQs"
   },
   "source": [
    "pip install tensorflow  \n",
    "pip install keras  \n",
    " \n",
    "git 설치 (https://coding-factory.tistory.com/245)  \n",
    "\n",
    "그 다음에  \n",
    "pip install git+https://github.com/haven-jeon/PyKoSpacing.git  \n",
    "\n",
    "를 실행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kuyhCuSo3SEi",
    "outputId": "71b19b48-3ff0-407d-ba85-4ea7cc5fd7e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
      "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-vnqf685e\n",
      "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-vnqf685e\n",
      "Collecting tensorflow==2.5.2\n",
      "  Downloading tensorflow-2.5.2-cp37-cp37m-manylinux2010_x86_64.whl (454.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 454.4 MB 22 kB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py==3.1.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.5) (3.1.0)\n",
      "Collecting argparse>=1.4.0\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py==3.1.0->pykospacing==0.5) (1.19.5)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py==3.1.0->pykospacing==0.5) (1.5.2)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.2->pykospacing==0.5) (0.37.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.2->pykospacing==0.5) (0.4.0)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 42.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.2->pykospacing==0.5) (0.12.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.2->pykospacing==0.5) (1.1.0)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.2->pykospacing==0.5) (1.15.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.2->pykospacing==0.5) (0.2.0)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 51.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.2->pykospacing==0.5) (1.1.2)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.2->pykospacing==0.5) (2.7.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.2->pykospacing==0.5) (1.6.3)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.2->pykospacing==0.5) (3.17.3)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 67.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.2->pykospacing==0.5) (3.3.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (2.23.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (57.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (4.8.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.2->pykospacing==0.5) (3.6.0)\n",
      "Building wheels for collected packages: pykospacing, wrapt\n",
      "  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pykospacing: filename=pykospacing-0.5-py3-none-any.whl size=2268581 sha256=b07a257dcb16daf6dfe039c8a730417e1f51c7110fbdc0e930b741868336144b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gl8pn818/wheels/9b/93/81/a2a7dc8c66ede5bf30634d20635f32b95eac7ca2ea8844058b\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68719 sha256=d520d9644060bef7816ba1a72dd7eea3ec473162f1164ad101dfa415c7db3163\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built pykospacing wrapt\n",
      "Installing collected packages: typing-extensions, grpcio, wrapt, tensorflow-estimator, keras-nightly, flatbuffers, tensorflow, argparse, pykospacing\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.41.1\n",
      "    Uninstalling grpcio-1.41.1:\n",
      "      Successfully uninstalled grpcio-1.41.1\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.13.3\n",
      "    Uninstalling wrapt-1.13.3:\n",
      "      Successfully uninstalled wrapt-1.13.3\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.7.0\n",
      "    Uninstalling tensorflow-2.7.0:\n",
      "      Successfully uninstalled tensorflow-2.7.0\n",
      "Successfully installed argparse-1.4.0 flatbuffers-1.12 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 pykospacing-0.5 tensorflow-2.5.2 tensorflow-estimator-2.5.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "argparse",
         "typing_extensions"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kr80b_uo3n_i"
   },
   "outputs": [],
   "source": [
    "sent = '오지호는 극중 두 얼굴의 사나이 성준 역을 맡았다. 성준은 국내 유일의 태백권 전승자를 가리는 결전의 날을 앞두고 20년간 동고동락한 사형인 진수(정의욱 분)를 찾으러 속세로 내려온 인물이다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-y9RF5V31Gw",
    "outputId": "8a6a80e2-b6d2-4825-a9bc-e71f64c0d092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오지호는극중두얼굴의사나이성준역을맡았다.성준은국내유일의태백권전승자를가리는결전의날을앞두고20년간동고동락한사형인진수(정의욱분)를찾으러속세로내려온인물이다.\n"
     ]
    }
   ],
   "source": [
    "new_sent = sent.replace(\" \", '') # 띄어쓰기가 없는 문장 임의로 만들기\n",
    "print(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRytcsoy3cvR",
    "outputId": "ecad7faf-dc21-402c-a832-ec3a51812dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오지호는 극중 두 얼굴의 사나이 성준 역을 맡았다. 성준은 국내 유일의 태백권 전승자를 가리는 결전의 날을 앞두고 20년간 동고동락한 사형인 진수(정의욱 분)를 찾으러 속세로 내려온 인물이다.\n",
      "오지호는 극중 두 얼굴의 사나이 성준 역을 맡았다. 성준은 국내 유일의 태백권 전승자를 가리는 결전의 날을 앞두고 20년간 동고동락한 사형인 진수(정의욱 분)를 찾으러 속세로 내려온 인물이다.\n"
     ]
    }
   ],
   "source": [
    "from pykospacing import Spacing\n",
    "spacing = Spacing()\n",
    "kospacing_sent = spacing(new_sent)\n",
    "print(sent)\n",
    "print(kospacing_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIdd-FB5Ln2r"
   },
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5RMscXSLzWF"
   },
   "source": [
    "텍스트 정규화(Text Normalization)은 통일할 수 있는 단어들은 하나로 통일하기 위한 전처리. 보통 정규 표현식이나 자신만의 규칙을 정의할 수도 있지만 Stemming이나 Lemmatization도 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB3PvwnvLrsb"
   },
   "source": [
    "## 영어 : Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpNtDgeMLtFr"
   },
   "source": [
    "어간(Stem)을 추출하는 작업을 어간 추출(stemming)이라고 함. 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있음. 다시 말해, 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VpNxDOvMJH8"
   },
   "source": [
    "가령, 포터 스테머의 포터 알고리즘의 어간 추출은 이러한 규칙들을 가짐.  \n",
    "ALIZE → AL  \n",
    "ANCE → 제거  \n",
    "ICAL → IC  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrz3HHV2MRHE"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVD5IJUMMZE9"
   },
   "outputs": [],
   "source": [
    "porterstemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "M2xsnOIXMUIm",
    "outputId": "a0cc2903-5ce8-4a99-a848-06c7c6388839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "Lu5pnAyZMWTz",
    "outputId": "05ba55c0-9e96-4b13-a1f2-30444fc3a396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "print([porterstemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krn274GkMiV2"
   },
   "source": [
    "조금 더 간단한 예제로 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "1VdyFjRYMmJH",
    "outputId": "7859212a-fe90-4e42-e8b7-6bde6e3f4363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words=['formalize', 'allowance', 'electricical']\n",
    "print([porterstemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdhSbOSwT70z"
   },
   "source": [
    "## 영어 : Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUXhHIenUIP7"
   },
   "source": [
    "표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단. 예를 들어서 am, are, is는 서로 다른 스펠링이지만 그 뿌리 단어는 be라고 볼 수 있음. 이 때, 이 단어들의 표제어는 be라고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7E4nsOpeUG4A",
    "outputId": "d94b26da-4656-4f68-c793-8ff1b5d1c882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oNo8EeZT9_q",
    "outputId": "78ab983e-5bb3-4c71-d959-616ab14d76f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnetlemmatizer = WordNetLemmatizer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([wordnetlemmatizer.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9MGzYEfUcJT"
   },
   "source": [
    "##한국어 : Stemming and Normalization(OKT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "htlAV5SDUd9w",
    "outputId": "09a28efc-755c-4796-9308-7ae73f64dd07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['북한', '은', '하루', '새', '3', '차례', '에', '걸쳐', '대미', '·', '대남', '압박', '메시지', '를', '내놓았다', '.']\n",
      "['북한', '은', '하루', '새', '3', '차례', '에', '걸치다', '대미', '·', '대남', '압박', '메시지', '를', '내놓다', '.']\n",
      "[('북한', 'Noun'), ('은', 'Josa'), ('하루', 'Noun'), ('새', 'Noun'), ('3', 'Number'), ('차례', 'Noun'), ('에', 'Josa'), ('걸쳐', 'Verb'), ('대미', 'Noun'), ('·', 'Punctuation'), ('대남', 'Noun'), ('압박', 'Noun'), ('메시지', 'Noun'), ('를', 'Josa'), ('내놓았다', 'Verb'), ('.', 'Punctuation')]\n",
      "[('북한', 'Noun'), ('은', 'Josa'), ('하루', 'Noun'), ('새', 'Noun'), ('3', 'Number'), ('차례', 'Noun'), ('에', 'Josa'), ('걸치다', 'Verb'), ('대미', 'Noun'), ('·', 'Punctuation'), ('대남', 'Noun'), ('압박', 'Noun'), ('메시지', 'Noun'), ('를', 'Josa'), ('내놓다', 'Verb'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "text = \"북한은 하루새 3차례에 걸쳐 대미·대남 압박 메시지를 내놓았다.\"\n",
    "print(okt.morphs(text))\n",
    "print(okt.morphs(text, stem=True))\n",
    "print(okt.pos(text))\n",
    "print(okt.pos(text, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "Ac17Z6L9Vora",
    "outputId": "d0362f26-9dc9-4698-ccfe-4dfc19791a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['웃기는', '소리', '하지마', '랔', 'ㅋㅋㅋ']\n",
      "['웃기는', '소리', '하지마라', 'ㅋㅋㅋ']\n",
      "[('웃기는', 'Verb'), ('소리', 'Noun'), ('하지마', 'Verb'), ('랔', 'Noun'), ('ㅋㅋㅋ', 'KoreanParticle')]\n",
      "[('웃기는', 'Verb'), ('소리', 'Noun'), ('하지마라', 'Verb'), ('ㅋㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "text = \"웃기는 소리하지마랔ㅋㅋㅋ\"\n",
    "print(okt.morphs(text))\n",
    "print(okt.morphs(text, norm=True))\n",
    "print(okt.pos(text))\n",
    "print(okt.pos(text, norm=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlZwcrE-8yXf"
   },
   "source": [
    "## 한국어 Norm : 반복되는 문자 정제(soynlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5EKaT_CLJE4"
   },
   "source": [
    "SoyNLP는 복합 명사를 추출할 수 있는 토크나이저도 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TzZMQ_X68xpA",
    "outputId": "ce1fe0b1-aa40-4393-ded3-24573d559efa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soynlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/50/6913dc52a86a6b189419e59f9eef1b8d599cffb6f44f7bb91854165fc603/soynlp-0.0.493-py3-none-any.whl (416kB)\n",
      "\r",
      "\u001b[K     |▉                               | 10kB 18.0MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 20kB 22.9MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 30kB 25.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 40kB 28.4MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 51kB 30.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 61kB 25.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 71kB 26.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 81kB 26.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 92kB 25.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 102kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 112kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 122kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 133kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 143kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 153kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 163kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 174kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 184kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 194kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 204kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 215kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 225kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 235kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 245kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 256kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 266kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 276kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 286kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 296kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 307kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 317kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 327kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 337kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 348kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 358kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 368kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 378kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 389kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 399kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 409kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 419kB 27.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.19.5)\n",
      "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.0.1)\n",
      "Installing collected packages: soynlp\n",
      "Successfully installed soynlp-0.0.493\n"
     ]
    }
   ],
   "source": [
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FII3bN4589KY"
   },
   "outputs": [],
   "source": [
    "from soynlp.normalizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdOY2-3h9QLp"
   },
   "source": [
    "ㅋㅋ, ㅎㅎ 등의 이모티콘의 경우 불필요하게 연속되는 경우가 많은데  \n",
    "ㅋㅋ, ㅋㅋㅋ, ㅋㅋㅋㅋ와 같은 경우를 모두 서로 다른 단어로 처리하는 것은 불필요.  \n",
    "이에 반복되는 것은 하나로 정규화시켜주는 것이 좋음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJGqME8-85ds",
    "outputId": "f9b7b5e3-ba5b-46ff-db15-35a6460c4ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n"
     ]
    }
   ],
   "source": [
    "print(emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠ', num_repeats=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcTKi4Hl9l8c"
   },
   "source": [
    "의미없게 반복되는 것은 비단 이모티콘에 한정되지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "dfqwwMNI9ruc",
    "outputId": "a1da3d6a-adfe-4ebc-c12b-b03fdc8a469b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와하하핫\n",
      "와하하핫\n",
      "와하하핫\n"
     ]
    }
   ],
   "source": [
    "print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하하하하하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하하하하핫', num_repeats=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuTbHcsV6BLg"
   },
   "source": [
    "# 한국어 Norm : 맞춤법 교정(hanspell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2RvB15Sp6lAM",
    "outputId": "154e0325-6677-48a7-bc92-586c6f56aee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ssut/py-hanspell.git\n",
      "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-t05c3j7l\n",
      "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-t05c3j7l\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
      "Building wheels for collected packages: py-hanspell\n",
      "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-hanspell: filename=py_hanspell-1.1-cp36-none-any.whl size=4854 sha256=96ea873185291573e8f16275d20c9e7becabe1a7b9320a6d9e2815b83ca9e9cb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-khhkoro9/wheels/0a/25/d1/e5e96476dbb1c318cc26c992dd493394fe42b0c204b3e65588\n",
      "Successfully built py-hanspell\n",
      "Installing collected packages: py-hanspell\n",
      "Successfully installed py-hanspell-1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jxeyqES-6D9G",
    "outputId": "eb242480-1626-4070-ebfe-036e3d8ff9e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지\n"
     ]
    }
   ],
   "source": [
    "from hanspell import spell_checker\n",
    " \n",
    "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2SJ8fxG7oMk"
   },
   "source": [
    "hanspell은 사실 띄어쓰기 교정 또한 하고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "E92iLj4E7K3i",
    "outputId": "cb8aeae6-b541-418d-d48e-c0b87571d202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오지호는 극 중 두 얼굴의 사나이 성준 역을 맡았다. 성준은 국내 유일의 태백권 전승자를 가리는 결전의 날을 앞두고 20년간 동고동락한 사형인 진수(정인욱 분)를 찾으러 속세로 내려온 인물이다.\n",
      "오지호는 극중 두 얼굴의 사나이 성준 역을 맡았다. 성준은 국내 유일의 태백권 전승자를 가리는 결전의 날을 앞두고 20년간 동고동락한 사형인 진수(정의욱 분)를 찾으러 속세로 내려온 인물이다.\n"
     ]
    }
   ],
   "source": [
    "sent = \"오지호는극중두얼굴의사나이성준역을맡았다.성준은국내유일의태백권전승자를가리는결전의날을앞두고20년간동고동락한사형인진수(정의욱분)를찾으러속세로내려온인물이다.\"\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)\n",
    "print(kospacing_sent) # 앞서 사용한 kospacing 패키지에서 얻은 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLes_LQ7_TN1"
   },
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMRKDNVd4Zrt"
   },
   "source": [
    "패턴을 감지하는 방식."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJxZ57f84eOV"
   },
   "source": [
    "import re 를 통해서 정규표현식 패키지를 임포트."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LFFgBMY_YDY"
   },
   "source": [
    "정규 표현식을 이용한 정제  \n",
    "영어 알파벳 범위 : a-zA-Z  \n",
    "한글 범위 : ㄱ-ㅎㅏ-ㅣ가-힣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jyroLR24Xd3"
   },
   "source": [
    "참고 링크 : https://wikidocs.net/4308"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0o_f1rAGoID"
   },
   "source": [
    "## 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RNZuN5xAEXu"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "Ku_h2uo-_asa",
    "outputId": "75720ab9-88bb-400d-d022-21ed0dc5daeb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_sent = \"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\"\n",
    "eng_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTR30RQSMEh7",
    "outputId": "9dac810b-bc65-4b0a-f4a9-38197300096c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yeah, do you expect people to read the FAQ, etc. and actually accept hard\n",
      "atheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\n",
      "of steam!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jim,\n",
      "\n",
      "Sorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\n",
      "denial about the faith you need to get by.  Oh well, just pretend that it will\n",
      "all end happily ever after anyway.  Maybe if you start a new newsgroup,\n",
      "alt.atheist.hard, you won't be bummin' so much?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \n",
      "--\n",
      "Bake Timmons, III\n"
     ]
    }
   ],
   "source": [
    "print(eng_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "IAQYUaIYC5jG",
    "outputId": "223d55a0-9f10-4c6c-f8f2-59b07b338be8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'       Yeah  do you expect people to read the FAQ  etc  and actually accept hard atheism   No  you need a little leap of faith  Jimmy   Your logic runs out of steam         Jim   Sorry I can t pity you  Jim   And I m sorry that you have these feelings of denial about the faith you need to get by   Oh well  just pretend that it will all end happily ever after anyway   Maybe if you start a new newsgroup  alt atheist hard  you won t be bummin  so much        Bye Bye  Big Jim   Don t forget your Flintstone s Chewables          Bake Timmons  III'"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_sent = re.sub(\"[^a-zA-Z]\", \" \", eng_sent)\n",
    "eng_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "982pJ81W_md-",
    "outputId": "9a60d591-6613-44c2-9c82-4e9490e4fb0c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Yeah expect people read actually accept hard atheism need little leap faith Jimmy Your logic runs steam Sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway Maybe start newsgroup atheist hard bummin much forget your Flintstone Chewables Bake Timmons'"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_sent = ' '.join([w for w in eng_sent.split() if len(w)>3])\n",
    "eng_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyMx1Yq-GqIh"
   },
   "source": [
    "## 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pxYLF-v2EK8R",
    "outputId": "2b9c479e-b9fb-45fb-9285-62e8875f06b0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'와 이런 것도 영화라고....ㅋㅋㅋ 차라리 뮤직비디오를 만드는 게 나을 뻔!!'"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_sent = \"와 이런 것도 영화라고....ㅋㅋㅋ 차라리 뮤직비디오를 만드는 게 나을 뻔!!\"\n",
    "kor_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "QIJdKO7OETyJ",
    "outputId": "7c09b8e1-d555-4d60-f322-c9b3bb785e2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'와 이런 것도 영화라고    ㅋㅋㅋ 차라리 뮤직비디오를 만드는 게 나을 뻔  '"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_sent = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]\", \" \", kor_sent)\n",
    "kor_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WnHP0EkwEffz",
    "outputId": "b67ce5fd-8187-4eeb-f35d-2a2c837cba9c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'와 이런 것도 영화라고        차라리 뮤직비디오를 만드는 게 나을 뻔  '"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 완성형 한글만 남기는 경우\n",
    "kor_sent = re.sub(\"[^가-힣]\", \" \", kor_sent)\n",
    "kor_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "65o0gVstE5wq",
    "outputId": "89faf609-8fba-4dbb-8cf3-e4429a1718f5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔 '"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 띄어쓰기가 반복될 경우 1개로 줄인다.\n",
    "kor_sent = re.sub(\"[ ]{2,}\", \" \", kor_sent)\n",
    "kor_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-1tJ4cUWT5k"
   },
   "source": [
    "#Stopwords (중요하지 않은 단어 삭제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O5T28W_WXrq"
   },
   "source": [
    "영어에서의 I, my, me, over, the와 같은 단어들이나 한국어의  조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우가 있음. 이러한 단어들을 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSw-u1EjWoKA",
    "outputId": "2fce194f-4b59-4857-df2d-78cc32b91a75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Kh0KJttWQ7E",
    "outputId": "0efd5dd8-f7c6-4df8-bafd-fedd80aba49c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords  \n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY38lpX8GxfC"
   },
   "source": [
    "## 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "PYDgcvEjW0XJ",
    "outputId": "85db241f-bdb0-41b9-a805-c13891aa4009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        result.append(w) \n",
    "\n",
    "print('원문 :', word_tokens) \n",
    "print('불용어 제거 후 :', result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnMZmqhxGytg"
   },
   "source": [
    "## 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "LsRkWP6YG0BY",
    "outputId": "0125dc16-2816-41c6-9ca9-4a5ffc0e187c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.0.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJi5--34G6fs"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHQab9tbG_4U"
   },
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "huEic2cZXCDq",
    "outputId": "38d43392-4091-4f8c-b6c2-891f8b345d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['와', '이런', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만드는', '게', '나을', '뻔']\n"
     ]
    }
   ],
   "source": [
    "example = \"와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔\"\n",
    "word_tokens = okt.morphs(example)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "7fsdEf_CHGJ7",
    "outputId": "4e392ec8-3f7f-490d-93a3-77085204f166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : ['와', '이런', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만드는', '게', '나을', '뻔']\n",
      "불용어 제거 후 : ['이런', '영화', '라고', '차라리', '뮤직비디오', '만드는', '게', '나을', '뻔']\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다', '것']\n",
    "# 위의 불용어는 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
    "\n",
    "result=[word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print('원문 :', word_tokens) \n",
    "print('불용어 제거 후 :', result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pA7YRvwaH2HR"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "AdhSbOSwT70z",
    "-9MGzYEfUcJT",
    "zlZwcrE-8yXf",
    "MuTbHcsV6BLg",
    "fLes_LQ7_TN1",
    "A0o_f1rAGoID",
    "UyMx1Yq-GqIh",
    "sY38lpX8GxfC",
    "k9zsoaPWersA",
    "e3WAEwbakd7f",
    "M4dlH7lCkk4X",
    "HaMDuF_vSs6j",
    "o_IqLzCDSvPt"
   ],
   "name": "딥 러닝을 이용한 자연어 처리 1강 - 전처리.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
