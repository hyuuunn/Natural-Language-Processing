{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJrYyJOqxE5b"
   },
   "source": [
    "# BLEU 튜토리얼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXwVevuU2A1t"
   },
   "source": [
    "## Sentence BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkmVYcvN15Xt"
   },
   "source": [
    "파이썬 자연어 처리 패키지 NLTK는 기계가 생성한 텍스트 데이터에 대해서 성능을 평가하는 BLEU score를 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "DwlomX-d1TSa",
    "outputId": "e733e272-62e5-4542-8778-3e20acdd5b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVK3sPmU2EZt"
   },
   "source": [
    "## Corpus BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx0V1kVP20hi"
   },
   "source": [
    "NLTK는 절이나 문장과 같은 다수의 문장의 묶음에 대해서 BLEU를 측정하는 corpus_bleu()를 제공. 여기서 references는 '토큰들의 리스트의 리스트의 리스트' 삼중 리스트여야 함. 또한 candidate는 '토큰들의 리스트의 리스트'이어야 함. 설명만으로는 조금 헷갈리므로 예제를 통해 이해."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "QpDcrpU-26Lu",
    "outputId": "f7fc1f05-e620-4c07-ddec-a647d45b1b30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# two references for one document\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "references = [[['this', 'is', 'a', 'test'], ['this', 'is' 'test']]]\n",
    "candidates = [['this', 'is', 'a', 'test']]\n",
    "score = corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P21LWP303yn5"
   },
   "source": [
    "## Cumulative and Individual BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3xs_bcp34V9"
   },
   "source": [
    "NTLK의 BLEU는 다른 n-gram들에 대해서 가중치를 달리하여 계산할 수 있도록 해줌.  \n",
    "예를 들어 unigram에만 가중을 100% 주고, 다른 2, 3, 4-gram에 대해서 가중을 주고 싶지않다면 다음과 같이 수행할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "laqnq_Ta3xys",
    "outputId": "710985a5-23d2-4984-b9fd-a110c3762733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# 1-gram individual BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyeevWR-5Co1"
   },
   "source": [
    "위 예제를 각각의 n-gram에 대해서 수행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "XSVXwTao5EHJ",
    "outputId": "83926f6c-2c50-4762-c717-0b75f8cb12a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 1.000000\n",
      "Individual 2-gram: 1.000000\n",
      "Individual 3-gram: 1.000000\n",
      "Individual 4-gram: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# n-gram individual BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'a', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQnnhCfiiaZS"
   },
   "source": [
    "각 개별적인 n-gram 스코어로부터 각각에게 가중을 주어 가중 기하 평균을 구함.  \n",
    "이를 BLEU-4라고 부름. 이는 sentence_bleu나 corpus_bleu의 기본값."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "U51MFK7y59JY",
    "outputId": "186d5807-e6bb-4510-ccd1-36c5fdc09cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067811865475\n",
      "0.7071067811865475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# 4-gram cumulative BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(score)\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eU-tIuTauHBE"
   },
   "source": [
    "BLEU-1, BLEU-2, BLEU-3, BLEU-4를 각각 구하면 다음과 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "nJkNdjXA6AVO",
    "outputId": "b7e4fbc8-3689-47f9-a33d-bf123939efcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.750000\n",
      "Cumulative 2-gram: 0.500000\n",
      "Cumulative 3-gram: 0.632878\n",
      "Cumulative 4-gram: 0.707107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# cumulative BLEU scores\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSh3oUsDuozU"
   },
   "source": [
    "## Worked Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0Iu4PXPurL2"
   },
   "source": [
    "다양한 예제를 통해 이해.  \n",
    "'the quick brown fox jumped over the lazy dog' 이와 같은 문장이 있다고 해봤을 때."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Ic6HA79l6CN6",
    "outputId": "3117c25f-5b68-4e4c-9408-10c818e0698b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# prefect match\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wIlI-AEu_Y0"
   },
   "source": [
    "'quick'을 'fast'로 바꿔봄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "TooPehi56IUo",
    "outputId": "fda9d6c6-989c-4cb9-f6f8-9dd253514c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7506238537503395\n"
     ]
    }
   ],
   "source": [
    "# one word different\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'fast', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUTS2229vBv3"
   },
   "source": [
    "기본값으로 BLEU-4가 실행되는데, 값이 다소 떨어진 것을 확인할 수 있음.  \n",
    "이번에는 lazy 또한 sleepy로 바꿔봄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CGQsGLwL6L5K",
    "outputId": "289ed9eb-6e1d-42fa-f2e5-cb0aa502178b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4854917717073234\n"
     ]
    }
   ],
   "source": [
    "# two words different\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'fast', 'brown', 'fox', 'jumped', 'over', 'the', 'sleepy', 'dog']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcuNGTr6veHE"
   },
   "source": [
    "candidate에 있는 모든 단어가 reference와 일치하지 않는다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "OpVYxfPi6OVw",
    "outputId": "47f626df-4fae-4be8-a2cf-0e8392226ce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# all words different\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2Z0Qwu9vkzd"
   },
   "source": [
    "이번에는 마지막 단어 2개를 일부로 누락."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Yb4Tr3Y36RGC",
    "outputId": "02eb435e-b794-4284-a7e9-1192f26fd302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7514772930752859\n"
     ]
    }
   ],
   "source": [
    "# shorter candidate\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vlePRvzvqnP"
   },
   "source": [
    "이번에는 굳이 뒤에 단어를 2개 추가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "NwCv_rwc6SMK",
    "outputId": "225bb7e3-4e36-4a49-ae87-17c00cd23dbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7860753021519787\n"
     ]
    }
   ],
   "source": [
    "# longer candidate\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', 'from', 'space']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A_vGybvv2fB"
   },
   "source": [
    "이번에는 단어를 맨 앞에 2개만 남겨두고 전부 지워봄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "h6u3ZWG86Vb-",
    "outputId": "a2655f0d-e535-4b59-cd2c-90fe6ac5bc03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0301973834223185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# very short\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'quick']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4NDe9ctv_Sk"
   },
   "source": [
    "3-gram 이상은 카운트할 수 없다는 에러가 발생. 위에서 두 단어만 주었기 때문.  \n",
    "BLEU에 사용된 수학은 매우 간단. 그렇기 때문에 직접 계산해보면서 이해하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7BhqvQ6xLTi"
   },
   "source": [
    "# BLEU 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KegHD8FBxTLi"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoNmktwjxieM"
   },
   "source": [
    "## Count : 단순 카운트 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yi7AfUmRxYgZ"
   },
   "outputs": [],
   "source": [
    "# 단순 카운트 함수\n",
    "def simple_count(tokens, n): # 토큰화 된 candidate 문장, n-gram에서의 n 이 두 가지를 인자로 받음.\n",
    "    return Counter(ngrams(tokens, n)) #문장에서 n-gram을 카운트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N87icmStxlB7"
   },
   "source": [
    "단순 카운트 구현 함수 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "KQ0XFwMtxfn4",
    "outputId": "074077bf-7c82-48c0-de5c-aec045a8f476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('the',): 3, ('It',): 1, ('is',): 1, ('a',): 1, ('guide',): 1, ('to',): 1, ('action',): 1, ('which',): 1, ('ensures',): 1, ('that',): 1, ('military',): 1, ('always',): 1, ('obeys',): 1, ('commands',): 1, ('of',): 1, ('party.',): 1})\n"
     ]
    }
   ],
   "source": [
    "candidate = \"It is a guide to action which ensures that the military always obeys the commands of the party.\"\n",
    "tokens = candidate.split() #단어 토큰화\n",
    "result = simple_count(tokens, 1) #토큰화 된 문장, 유니그램의 개수를 구하고자 한다면 n=1\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BDLpSevYxv2R",
    "outputId": "73335042-8ea6-4968-adbb-702caef95f07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('the',): 7})\n"
     ]
    }
   ],
   "source": [
    "candidate = 'the the the the the the the'\n",
    "tokens = candidate.split() #단어 토큰화\n",
    "result = simple_count(tokens, 1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sICROdQx5L3"
   },
   "source": [
    "## Count_clip 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-P4OUAhx6uN"
   },
   "outputs": [],
   "source": [
    "def count_clip(candidate, reference_list, n):\n",
    "    cnt_ca = simple_count(candidate, n)\n",
    "    # Ca 문장에서 n-gram 카운트\n",
    "    temp = dict()\n",
    "\n",
    "    for ref in reference_list: # 다수의 Ref 문장에 대해서 이하 반복\n",
    "        cnt_ref = simple_count(ref, n)\n",
    "        # Ref 문장에서 n-gram 카운트\n",
    "\n",
    "        for n_gram in cnt_ref: # 모든 Ref에 대해서 비교하여 특정 n-gram이 하나의 Ref에 가장 많이 등장한 횟수를 저장\n",
    "            if n_gram in temp:\n",
    "                temp[n_gram] = max(cnt_ref[n_gram], temp[n_gram]) # max_ref_count\n",
    "            else:\n",
    "                temp[n_gram] = cnt_ref[n_gram]\n",
    "\n",
    "    return {\n",
    "        n_gram: min(cnt_ca.get(n_gram, 0), temp.get(n_gram, 0)) for n_gram in cnt_ca\n",
    "        # count_clip=min(count, max_ref_count)\n",
    "        # 위의 get은 찾고자 하는 n-gram이 없으면 0을 반환한다.\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ESVPluYkApk"
   },
   "source": [
    "count_clip 함수는 candidate 문장과 reference 문장들, 그리고 카운트 단위가 되는 n-gram에서의 n의 값 이 세 가지를 인자로 입력받아서 countclip을 수행. 여기서는 유니그램 정밀도를 구현하고 있으므로 역시나 n=1로 하여 함수를 실행하면 됨.  \n",
    "\n",
    "또한 count_clip 함수 내부에는 기존에 구현했던 simple_count 함수가 사용된 것을 확인할 수 있음. Countclip을 구하기 위해서는 Max_Ref_Count값과 비교하기 위해 Count값이 필요하기 때문. Example 2를 통해 함수가 정상 작동되는지 확인해 봄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mNK2gOXTkD2H",
    "outputId": "b9202545-f96c-4209-fb67-db37fee3167b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('the',): 2}\n"
     ]
    }
   ],
   "source": [
    "candidate = 'the the the the the the the'\n",
    "references = [\n",
    "    'the cat is on the mat',\n",
    "    'there is a cat on the mat'\n",
    "]\n",
    "result = count_clip(candidate.split(),list(map(lambda ref: ref.split(), references)),1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDbJMDBGkLM0"
   },
   "source": [
    "동일한 예제 문장에 대해서 위의 simple_count 함수는 the가 7개로 카운트되었던 것과는 달리 이번에는 2개로 카운트 됨. 이제 위의 두 함수를 사용하여 예제 문장에 대해서 보정된 정밀도를 연산하는 함수를 modified_precision란 이름의 함수로 구현해 봄."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bflMCJdYkkWq"
   },
   "source": [
    "## 보정된 유니그램 정밀도 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEPYNm3HkOvi"
   },
   "outputs": [],
   "source": [
    "def modified_precision(candidate, reference_list, n):\n",
    "    clip = count_clip(candidate, reference_list, n) \n",
    "    total_clip = sum(clip.values()) # 분자\n",
    "\n",
    "    ct = simple_count(candidate, n)\n",
    "    total_ct = sum(ct.values()) #분모\n",
    "\n",
    "    if total_ct==0: # n-gram의 n이 커졌을 때 분모가 0이 되는 것을 방지\n",
    "      total_ct=1\n",
    "\n",
    "    return (total_clip / total_ct) # 보정된 정밀도\n",
    "    # count_clip의 합을 분자로 하고 단순 count의 합을 분모로 하면 보정된 정밀도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "8E0E1N4DkSxg",
    "outputId": "2e92ffd1-f01c-44b4-81fb-2223c10fc9e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "result = modified_precision(candidate.split(),list(map(lambda ref: ref.split(), references)),1) # 유니그램이므로 n = 1\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAaanPDnkPtS"
   },
   "source": [
    "소수 값이 나오는데 이는 2/7의 값을 의미함. \n",
    "\n",
    "이제부터 설명에서 언급하는 '정밀도'는 기본적으로 보정된 정밀도(Modified Precision)라고 가정. 정밀도를 보정하므로서 Ca에서 발생하는 단어 중복에 대한 문제점은 해결. 하지만 유니그램 정밀도가 가지는 본질적인 문제점있기에 이제는 유니그램을 넘어 바이그램, 트라이그램 등과 같이 n-gram으로 확장해야 함.  \n",
    "\n",
    "앞서 구현한 함수 simple_count, count_clip, modified_precision은 모두 n-gram의 n을 함수의 인자로 받으므로, n을 1대신 다른 값을 넣어서 실습해보면 바이그램, 트라이그램 등에 대해서도 보정된 정밀도를 구할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgsirDfSkpEF"
   },
   "source": [
    "## 짧은 문장 길이에 대한 패널티(Brevity Penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ9K0Zqzk_xU"
   },
   "source": [
    "Ref가 1개라면 Ca와 Ref의 두 문장의 길이만을 가지고 계산하면 되겠지만 여기서는 Ref가 여러 개일 때를 가정하고 있으므로 r은 '모든 Ref들 중에서 Ca와 가장 길이 차이가 작은 Ref의 길이'로 함. r을 구하는 코드는 아래와 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQUUVtYXkhET"
   },
   "outputs": [],
   "source": [
    "def closest_ref_length(candidate, reference_list): # Ca 길이와 가장 근접한 Ref의 길이를 리턴하는 함수\n",
    "    ca_len = len(candidate) # ca 길이\n",
    "    ref_lens = (len(ref) for ref in reference_list) # Ref들의 길이\n",
    "    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - ca_len), ref_len))\n",
    "    # 길이 차이를 최소화하는 Ref를 찾아서 Ref의 길이를 리턴\n",
    "    return closest_ref_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z3PfH-2lDpS"
   },
   "source": [
    "만약 Ca와 길이가 정확히 동일한 Ref가 있다면 길이 차이가 0인 최고 수준의 매치(best match length)임. 또한 만약 서로 다른 길이의 Ref이지만 Ca와 길이 차이가 동일한 경우에는 더 작은 길이의 Ref를 택함. 예를 들어 Ca가 길이가 10인데, Ref 1, 2가 각각 9와 11이라면 길이 차이는 동일하게 1밖에 나지 않지만 9를 택함. closest_ref_length 함수를 통해 r을 구했다면, 이제 BP를 구하는 함수 brevity_penalty를 구현."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQhnL1IFlFhj"
   },
   "outputs": [],
   "source": [
    "def brevity_penalty(candidate, reference_list):\n",
    "    ca_len = len(candidate)\n",
    "    ref_len = closest_ref_length(candidate, reference_list)\n",
    "\n",
    "    if ca_len > ref_len:\n",
    "        return 1\n",
    "    elif ca_len == 0 :\n",
    "    # candidate가 비어있다면 BP = 0 → BLEU = 0.0\n",
    "        return 0\n",
    "    else:\n",
    "        return np.exp(1 - ref_len/ca_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-3GR1lhlNLN"
   },
   "source": [
    "## BLEU 함수 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57DM-GuqlILF"
   },
   "source": [
    "이제 최종적으로 BLEU 점수를 계산하는 함수 bleu_score를 구현."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9veUzn1lGP6"
   },
   "outputs": [],
   "source": [
    "def bleu_score(candidate, reference_list, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    bp = brevity_penalty(candidate, reference_list) # 브레버티 패널티, BP\n",
    "\n",
    "    p_n = [modified_precision(candidate, reference_list, n=n) for n, _ in enumerate(weights,start=1)] \n",
    "    #p1, p2, p3, ..., pn\n",
    "    score = np.sum([w_i * np.log(p_i) if p_i != 0 else 0 for w_i, p_i in zip(weights, p_n)])\n",
    "    return bp * np.exp(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbUorUGolQ4x"
   },
   "source": [
    "위 함수가 동작하기 위해서는 앞서 구현한 simple_count, count_clip, modified_precision, brevity_penalty 4개의 함수 또한 모두 구현되어져 있어야 함. 지금까지 구현한 BLEU 코드로 계산된 점수와 NLTK 패키지에 이미 구현되어져 있는 BLEU 코드로 계산된 점수를 비교."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3LGHmUPlUWz"
   },
   "source": [
    "## NLTK의 BLEU Vs. 구현한 BLEU 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "h0uOo_H-lXbz",
    "outputId": "6ad2db98-ac10-4462-b3b0-c2a217fc06ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5045666840058485\n",
      "0.5045666840058485\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "\n",
    "candidate = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "references = [\n",
    "    'It is a guide to action that ensures that the military will forever heed Party commands',\n",
    "    'It is the guiding principle which guarantees the military forces always being under the command of the Party',\n",
    "    'It is the practical guide for the army always to heed the directions of the party'\n",
    "]\n",
    "\n",
    "# 이번 챕터에서 구현한 코드로 계산한 BLEU 점수\n",
    "print(bleu_score(candidate.split(),list(map(lambda ref: ref.split(), references))))\n",
    "# NLTK 패키지 구현되어져 있는 코드로 계산한 BLEU 점수\n",
    "print(bleu.sentence_bleu(list(map(lambda ref: ref.split(), references)),candidate.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljq1WBzIkQ_1"
   },
   "source": [
    "# Subword Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLuLUwLLkTF9"
   },
   "source": [
    "## SubwordTextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3mT0R0fnZtT"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdE-AloKloh0"
   },
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBRpvn3Dkmk8"
   },
   "outputs": [],
   "source": [
    "# 디렉토리 안에 모든 파일들을 DataFrame 형태로 읽어오기.\n",
    "# 구체적으로 문장(sentence)과 문장의 감정상태의 확신정도(sentiment=1~10)를 읽어오기.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# 긍정(postive) 예제와 부정(negative) 예제를 하나의 dataframe으로 합치고 \n",
    "# 긍정 혹은 부정을 나타내는 polarity 컬럼을 추가하고 데이터를 랜덤하게 섞음.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# IMDB 영화 리뷰 데이터셋을 다운받고 전처리를 진행.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "  return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwskvxMwmnVl"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "NmhxbDDqkbgA",
    "outputId": "0112ca08-61a1-4bd7-eddb-281757ec03f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie was awful. The ending was absolutel...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Patrick Channing (Jeff Kober) is a disciple of...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first two-thirds of this biopic of fetish ...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fortunately for us Real McCoy fans (most likel...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I wouldn't call it awful, but nothing at all s...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>I loved the Batman tv series and was really lo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>One of the best comedians ever. I've seen this...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>One thing I always liked about Robert Ludlum t...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>Just Cause takes some of the best parts of thr...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>Everything in this film is bad , the story , t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence sentiment  polarity\n",
       "0      This movie was awful. The ending was absolutel...         1         0\n",
       "1      Patrick Channing (Jeff Kober) is a disciple of...         4         0\n",
       "2      The first two-thirds of this biopic of fetish ...         7         1\n",
       "3      Fortunately for us Real McCoy fans (most likel...         8         1\n",
       "4      I wouldn't call it awful, but nothing at all s...         4         0\n",
       "...                                                  ...       ...       ...\n",
       "24995  I loved the Batman tv series and was really lo...         2         0\n",
       "24996  One of the best comedians ever. I've seen this...        10         1\n",
       "24997  One thing I always liked about Robert Ludlum t...         7         1\n",
       "24998  Just Cause takes some of the best parts of thr...         7         1\n",
       "24999  Everything in this film is bad , the story , t...         1         0\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "VRGiDSCFkdHP",
    "outputId": "d69d80a9-5df3-49ec-e53c-d50bd3c0aee7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        This movie was awful. The ending was absolutel...\n",
       "1        Patrick Channing (Jeff Kober) is a disciple of...\n",
       "2        The first two-thirds of this biopic of fetish ...\n",
       "3        Fortunately for us Real McCoy fans (most likel...\n",
       "4        I wouldn't call it awful, but nothing at all s...\n",
       "                               ...                        \n",
       "24995    I loved the Batman tv series and was really lo...\n",
       "24996    One of the best comedians ever. I've seen this...\n",
       "24997    One thing I always liked about Robert Ludlum t...\n",
       "24998    Just Cause takes some of the best parts of thr...\n",
       "24999    Everything in this film is bad , the story , t...\n",
       "Name: sentence, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e5ANb3Gkeof"
   },
   "outputs": [],
   "source": [
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    train_df['sentence'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "i0yJWPKikgky",
    "outputId": "0df6296a-6c72-435e-e0ee-05179481f3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 'as_', 't_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', ' (', 'are_', 'his_', 'have_', 'film_', 'not_', 'ing_', 'be_', 'ed_', 'you_', ' \"', 'it', 'd_', 'an_', 'he_', 'by_', 'at_', 'one_', 'who_', 'y_', 'from_', 'e_', 'or_', 'all_', 'like_', 'they_', '\" ', 'so_', 'just_', 'has_', ') ', 'her_', 'about_', 'out_', 'This_', 'some_', 'ly_', 'movie', 'film', 'very_', 'more_', 'It_', 'would_', 'what_', 'when_', 'which_', 'good_', 'if_', 'up_', 'only_', 'even_', 'their_', 'had_', 'really_', 'my_', 'can_', 'no_', 'were_', 'see_', 'she_', '? ', 'than_', '! ', 'there_', 'get_', 'been_', 'into_', ' - ', 'will_', 'much_', 'story_', 'because_', 'ing', 'time_', 'n_', 'we_', 'ed', 'me_', ': ', 'most_', 'other_', 'don', 'do_', 'm_', 'es_', 'how_', 'also_', 'make_', 'its_', 'could_', 'first_', 'any_', \"' \", 'people_', 'great_', 've_', 'ly', 'er_', 'made_', 'r_', 'But_', 'think_', \" '\", 'i_', 'bad_', 'A_', 'And_', 'It', 'on', '; ', 'him_', 'being_', 'never_', 'way_', 'that', 'many_', 'then_', 'where_', 'two_', 'In_', 'after_', 'too_', 'little_', 'you', '), ', 'well_', 'ng_', 'your_', 'If_', 'l_', '). ', 'does_', 'ever_', 'them_', 'did_', 'watch_', 'know_', 'seen_', 'time', 'er', 'character_', 'over_', 'characters_', 'movies_', 'man_', 'There_', 'love_', 'best_', 'still_', 'off_', 'such_', 'in', 'should_', 'the', 're_', 'He_', 'plot_', 'films_', 'go_', 'these_', 'acting_', 'doesn', 'es', 'show_', 'through_', 'better_', 'al_', 'something_', 'didn', 'back_', 'those_', 'us_', 'less_', '...', 'say_', 'is', 'one', 'makes_', 'can', 'and', 'all', 'ion_', 'find_', 'scene_', 'old_', 'real_', 'few_', 'going_', 'well', 'actually_', 'watching_', 'life_', 'me', '. <', 'o_', 'man', 'there', 'scenes_', 'same_', 'he', 'end_', 'this', '... ', 'k_', 'while_', 'thing_', 'of', 'look_', 'quite_', 'out', 'lot_', 'want_', 'why_', 'seems_', 'every_', 'll_', 'pretty_', 'got_', 'able_', 'nothing_', 'good', 'As_', 'story', ' & ', 'another_', 'take_', 'to', 'years_', 'between_', 'give_', 'am_', 'work_', 'isn', 'part_', 'before_', 'actors_', 'may_', 'gets_', 'young_', 'down_', 'around_', 'ng', 'thought_', 'though_', 'end', 'without_', 'What_', 'They_', 'things_', 'life', 'always_', 'must_', 'cast_', 'almost_', 'h_', '10', 'saw_', 'own_', 'here', 'bit_', 'come_', 'both_', 'might_', 'g_', 'whole_', 'new_', 'director_', 'them', 'horror_', 'ce', 'You_', 'least_', 'bad', 'big_', 'enough_', 'him', 'feel_', 'probably_', 'up', 'here_', 'making_', 'long_', 'her', 'st_', 'kind_', '--', 'original_', 'fact_', 'rather_', 'or', 'far_', 'nt_', 'played_', 'found_', 'last_', 'movies', 'When_', 'so', '\", ', 'comes_', 'action_', 'She_', 've', 'our_', 'anything_', 'funny_', 'ion', 'right_', 'way', 'trying_', 'now_', 'ous_', 'each_', 'done_', 'since_', 'ic_', 'point_', '\". ', 'wasn', 'interesting_', 'c_', 'worst_', 'te_', 'le_', 'ble_', 'ty_', 'looks_', 'show', 'put_', 'looking_', 'especially_', 'believe_', 'en_', 'goes_', 'over', 'ce_', 'p_', 'films', 'hard_', 'main_', 'be', 'having_', 'ry', 'TV_', 'worth_', 'One_', 'do', 'al', 're', 'again', 'series_', 'takes_', 'guy_', 'family_', 'seem_', 'plays_', 'role_', 'away_', 'world_', 'My_', 'character', ', \"', 'performance_', '2_', 'So_', 'watched_', 'John_', 'th_', 'plot', 'script_', 'For_', 'sure_', 'characters', 'set_', 'different_', 'minutes_', 'All_', 'American_', 'anyone_', 'Not_', 'music_', 'ry_', 'shows_', 'too', 'son_', 'en', 'day_', 'use_', 'someone_', 'for', 'woman_', 'yet_', '.\" ', 'during_', 'she', 'ro', '- ', 'times_', 'left_', 'used_', 'le', 'three_', 'play_', 'work', 'ness_', 'We_', 'girl_', 'comedy_', 'ment_', 'an', 'simply_', 'off', 'ies_', 'funny', 'ne', 'acting', 'That_', 'fun_', 'completely_', 'st', 'seeing_', 'us', 'te', 'special_', 'ation_', 'as', 'ive_', 'ful_', 'read_', 'reason_', 'co', 'need_', 'sa', 'true_', 'ted_', 'like', 'ck', 'place_', 'they', '10_', 'However', 'until_', 'rest_', 'sense_', 'ity_', 'everything_', 'people', 'nt', 'ending_', 'again_', 'ers_', 'given_', 'idea_', 'let_', 'nice_', 'help_', 'no', 'truly_', 'beautiful_', 'ter', 'ck_', 'version_', 'try_', 'came_', 'Even_', 'DVD_', 'se', 'mis', 'scene', 'job_', 'ting_', 'Me', 'At_', 'who', 'money_', 'ment', 'ch', 'recommend_', 'was', 'once_', 'getting_', 'tell_', 'de_', 'gives_', 'not', 'Lo', 'we', 'son', 'shot_', 'second_', 'After_', 'To_', 'high_', 'screen_', ' -- ', 'keep_', 'felt_', 'with', 'great', 'everyone_', 'although_', 'poor_', 'el', 'half_', 'playing_', 'couple_', 'now', 'ble', 'excellent_', 'enjoy_', 'couldn', 'x_', 'ne_', ',\" ', 'ie_', 'go', 'become_', 'less', 'himself_', 'supposed_', 'won', 'understand_', 'seen', 'ally_', 'THE_', 'se_', 'actor_', 'ts_', 'small_', 'line_', 'na', 'audience_', 'fan_', 'et', 'world', 'entire_', 'said_', 'at', '3_', 'scenes', 'rs_', 'full_', 'year_', 'men_', 'ke', 'doing_', 'went_', 'director', 'back', 'early_', 'Hollywood_', 'start_', 'liked_', 'against_', 'remember_', 'love', 'He', 'along_', 'ic', 'His_', 'wife_', 'effects_', 'together_', 'ch_', 'Ra', 'ty', 'maybe_', 'age', 'S_', 'While_', 'often_', 'sort_', 'definitely_', 'No', 'script', 'times', 'absolutely_', 'book_', 'day', 'human_', 'There', 'top_', 'ta', 'becomes_', 'piece_', 'waste_', 'seemed_', 'down', '5_', 'later_', 'rs', 'ja', 'certainly_', 'budget_', 'th', 'nce_', '200', '. (', 'age_', 'next_', 'ar', 'several_', 'ling_', 'short_', 'sh', 'fe', 'Of_', 'instead_', 'Man', 'T_', 'right', 'father_', 'actors', 'wanted_', 'cast', 'black_', 'Don', '1_', 'more', 'comedy', 'better', 'camera_', 'wonderful_', 'production_', 'inter', 'course', 'low_', 'else_', 'w_', 'ness', 'course_', 'based_', 'ti', 'Some_', 'know', 'house_', 'say', 'de', 'watch', 'ous', 'pro', 'tries_', 'ra', 'kids_', 'etc', ' \\x96 ', 'loved_', 'est_', 'fun', 'made', 'video_', 'un', 'totally_', 'Michael_', 'ho', 'mind_', 'No_', 'Be', 'ive', 'La', 'Fi', 'du', 'ers', 'Well', 'wants_', 'How_', 'series', 'performances_', 'written_', 'live_', 'New_', 'So', 'Ne', 'Na', 'night_', 'ge', 'gave_', 'home_', 'heart', 'women_', 'nu', 'ss_', 'hope_', 'ci', 'friends_', 'Se', 'years', 'sub', 'head_', 'Y_', 'Du', '. \"', 'turn_', 'red_', 'perfect_', 'already_', 'classic_', 'tri', 'ss', 'person_', 'star_', 'screen', 'style_', 'ur', 'starts_', 'under_', 'Then_', 'ke_', 'ine', 'ies', 'um', 'ie', 'face_', 'ir', 'enjoyed_', 'point', 'lines_', 'Mr', 'turns_', 'what', 'side_', 'sex_', 'Ha', 'final_', ').<', 'With_', 'care_', 'tion_', 'She', 'ation', 'Ar', 'problem_', 'ma', 'lost_', 'are', 'li', '4_', 'fully_', 'oo', 'sha', 'Just_', 'name_', 'ina', 'boy_', 'finally_', 'ol', '!<', 'Bo', 'though', 'about', 'hand', 'ton', 'lead_', 'school_', 'ns', 'ha', 'favorite_', 'stupid_', 'gi', 'original', 'mean_', 'To', 'took_', 'either_', 'ni', 'book', 'episode_', 'om', 'Su', 'D_', 'Mc', 'house', 'cannot_', 'stars_', 'behind_', 'see', 'other', 'Che', 'role', 'art', 'ever', 'Why_', 'father', 'case_', 'tic_', 'moments_', 'Co', 'works_', 'sound_', 'Ta', 'guess_', 'perhaps_', 'Vi', 'thing', 'fine_', 'fact', 'music', 'non', 'ful', 'action', 'ity', 'ct', 'ate_', 'type_', 'lack_', 'death_', 'art_', 'able', 'Ja', 'ge_', 'wouldn', 'am', 'tor', 'extremely_', 'pre', 'self', 'Mor', 'particularly_', 'bo', 'est', 'Ba', 'ya', 'play', 'Pa', 'ther', 'heard_', 'however', 'ver', 'dy_', 'Sa', 'ding_', 'led_', 'late_', 'feeling_', 'per', 'low', 'ably_', 'Un', 'On_', 'known_', 'kill_', 'fight_', 'beginning_', 'cat', 'bit', 'title_', 'vo', 'short', 'old', 'including_', 'Da', 'coming_', 'That', 'place', 'looked_', 'best', 'Lu', 'ent_', 'bla', 'quality_', 'except_', '...<', 'ff', 'decent_', 'much', 'De', 'Bu', 'ter_', 'attempt_', 'Bi', 'taking_', 'ig', 'Ti', 'whose_', 'dialogue_', 'zz', 'war_', 'ill', 'Te', 'war', 'Hu', 'James_', '..', 'under', 'ring_', 'pa', 'ot', 'expect_', 'Ga', 'itself_', 'line', 'lives_', 'let', 'Dr', 'mp', 'che', 'mean', 'called_', 'complete_', 'terrible_', 'boring_', 'others_', '\" (', 'aren', 'star', 'long', 'Li', 'mother_', 'si', 'highly_', 'ab', 'ex', 'os', 'nd', 'ten_', 'ten', 'run_', 'directed_', 'town_', 'friend_', 'David_', 'taken_', 'finds_', 'fans_', 'Mar', 'writing_', 'white_', 'u_', 'obviously_', 'mar', 'Ho', 'year', 'stop_', 'f_', 'leave_', 'king_', 'act_', 'mind', 'entertaining_', 'ish_', 'Ka', 'throughout_', 'viewer_', 'despite_', 'Robert_', 'somewhat_', 'hour_', 'car_', 'evil_', 'Although_', 'wrong_', 'Ro', 'dead_', 'body_', 'awful_', 'home', 'exactly_', 'bi', 'family', 'ts', 'usually_', 'told_', 'z_', 'oc', 'minutes', 'tra', 'some', 'actor', 'den', 'but', 'Sha', 'tu', 'strong_', 'Jo', 'real', 'la', 'gin', 'ul', 'amazing_', 'save_', 'wrong', 'dis', 'obvious_', 'close_', 'sometimes_', 'shown_', 'head', 'land', 'Go', 'mer', 'ending', 'else', 'audience', 'su', 'parts_', 'ga', 'before', 'cinema', 'opening_', 'laugh_', 'Ca', 'sh_', 'guys_', 'ds_', 'number_', 'Ma', 'soon_', 'ob', 'po', 'wonder_', 'group_', 'men', 'Mac', 'thinking_', 'fan', 'across_', 'turned_', 'ant', 'tells_', 'em', 'night', 'ton_', 'picture_', 'past_', 'Hi', 'girl', 'ght', 'woman', 'started_', 'ba', 'Ru', 'da', 'wi', 'running_', 'part', 'wish_', 'ner', 'ap', 'rn', 'ant_', 'mon', 'ast', 'awful', 'Yes', 'The', 'ard', 'nce', 'era', 'today', 'ad', 'Now_', '.) ', 'local_', 'killer_', 'huge_', 'flick', 'ends_', 'light', 'ons_', 'Al', 'knew_', 'due_', 'direction_', 'close', 'Gra', 'od', 'giving_', 'Le', 'op', 'Pe', 'ey_', 'wa', 'sta', 'worse_', 'single_', 'cut_', 'light_', 'ia', 'happens_', 'supporting_', 'room_', 'girls_', 'female_', 'E_', 'falls_', 'nd_', 'ish', 'mostly_', 'tan', 'major_', 'bring_', 'killed_', 'ele', 'el_', 'dark_', 'myself_', 'Pro', 'ent', 'ated_', 'British_', 'va', '....', 'talking_', 'con', 'tion', 'children_', 'by', 'voice_', 'sense', 'Car', '.. ', 'ain', 'For', 'Con', '8_', 'performance', 'au', 'stories_', 'ine_', 'Or', 'order_', 'first', 'ac', 'involved_', 'interesting', 'drama_', 'Dan', 'away', 'From_', 'ping_', 'boy', 'air', 'sing_', 'lle', 'You', 'lo', 'ian', 'ingly_', 'ia_', 'haven', 'using_', 'fo', 'dy', 'modern_', 'ST', 'wife', 'unt', 'game_', 'together', 'pp', 'clearly_', 'First_', 'sad', 'ris', 'ven', 'col', 'Maybe_', 'val', 'sexual_', 'serious_', 'relationship_', 'musical_', 'boring', 'But', 'hit_', 'brilliant_', 'easily_', 'living_', 'ca', 'police_', 'ip', ' , ', 'feels_', 'effects', 'sex', 'ist_', 'die', 'para', 'ort', 'humor_', 'Cor', 'ist', 'et_', 'Richard_', 'call_', 'example', 'appears_', 'actress_', 'rit', 'matter_', 'ar_', 'ns_', 'needs_', 'important_', 'fli', 'ec', 'stupid', 'ee', 'change_', 'bur', ' . ', 'comic_', 'DVD', 'We', '?<', 'Paul_', 'child_', 'ag', 'enjoy', 'cha', 'actual_', 'says_', 'nearly_', 'heart_', 'did', 'similar_', 'side', 'ru', 'ped_', 'und', 'super', 'name', 'clear_', \"', \", 'cu', 'child', 'moment_', 'ions_', 'fall_', 'done', 'chance_', 'then', 'ian_', 'George_', 'exc', 'enough', 'Jack_', 'win', 'Di', 'ying_', 'said', '80', 'ze', 'example_', 'themselves_', 'named_', 'ger', 'near_', 'guy', 'car', 'horrible_', 'bri', '!! ', 'ori', 'his', 'ded_', 'An_', 'released_', 'laugh', 'kept_', 'beyond_', 'b_', 'Sch', 'An', 'Lan', 'In', 'gar', 'genre', 'cho', 'Har', 'title', 'romantic_', 'mother', 'English_', 'mention_', 'interest_', 'Its_', 'money', 'face', 'brought_', 'ut', 'after', 'Win', 'working_', 'ny', 'knows_', 'happened_', 'certain_', '6_', 'within_', 'usual_', 'upon_', 'il', 'Her_', 'from', 'drama', 'Si', 'Mo', 'God', 'five_', 'whether_', 'tried_', 'ial_', 'history_', 'far', 'Re', 'novel', 'chi', 'inc', 'ure_', 'ied_', 'anti', 'Mad', 'lly_', 'Is_', '7_', 'ess', 'bunch_', 'vin', 'slow_', 'style', 'hi', 'eyes_', 'cinema_', 'showing_', 'gen', 'ra_', 'among_', 'unc', 'Po', 'Peter_', 'kid_', 'ght_', 'ny_', 'gh', 'tro', 'four_', 'ue', 'ley_', 'stuff_', 'strange_', 'sit_', 'sch', 'anyway', '199', 'hours_', 'These_', 'Most_', 'own', 'ned_', 'ban', 'Fa', 'decided_', 'xi', 'top', 'll', 'get', 'events_', 'Also_', 'typical_', 'shots_', 'look', 'happy_', 'um_', 'simple_', 'either', 'comment', 'ssi', 'ps', 'Bar', 'Per', 'saying_', 'none_', 'surprised_', 'sse', 'ka', 'ily_', 'horror', 'dig', 'tt', 'ric', 'post', 'TV', '198', '* ', 'half', 'gn', 'ste', 'ls', 'hero_', 'Pi', 'Like_', 'sad_', 'hear_', 'begins_', 'rent_', 'ure', 'rie', 'greatest_', 'Je', 'van', 'sci', 'kid', 'himself', 'Also', 'view_', 'score_', 'dge', 'became_', 'Cra', '197', 'ones_', 'cal', '9_', 'hor', 'hand_', 'days_', 'yourself_', 'tle', 'gan', 'ea', 'ago', 'WA', 'pen', 'ls_', 'learn_', 'Sta', 'By_', 'middle_', 'job', 'uc', 'ko', 'bar', 'lots_', 'cheap_', 'fi', 'stay_', 'stand_', 'pri', 'za', 'im', 'ight', 'happen_', 'Ab', 'Gar', 'ore', 'lan', 'classic', 'writer_', 'ster', 'picture', 'hate_', 'der', 'grand', 'disc', 'Mi', 'ud', 'é', 'murder_', 'basically_', 'jokes_', 'famous_', 'eg', 'easy_', 'rm', 'der_', 'R_', 'Mat', 'two', 'daughter', 'Spi', 'camera', 'AN', 'glo', 'talk_', 'daughter_', 'Fre', 'ri', 'perfect', 'experience_', 'buy_', 'zo', 'bu', 'Pu', 'Col', 'uni', 'later', 'children', 'sets_', 'annoying_', 'Tom_', 'uses_', 'jo', 'dead', 'psycho', 'mid', 'room', 'ki', 'hope', 'dialogue', 'attention_', 'cc', 'above_', 'possibly_', 'mo', 'difficult_', 'Mon', 'Japanese_', '!\" ', 'death', 'class_', ': \"', 'tic', 'ler', 'bus', 'genre_', 'stre', 'keeps_', 'cre', 'una', 'tly_', 'leaves_', 'RE', 'yes', 'realize_', 'nor_', 'figure_', 'Chan', 'rec', 'minute_', 'leading_', 'high', 'gui', 'ug', 'sequence_', 'na_', 'help', 'ani', 'Who_', 'exist', 'documentary_', 'sal', 'pe', 'key_', 'Bra', 'murder', 'leg', 'songs_', 'production', 'dle', 'cla', 'arm', 'US', \"'. \", 'reason', 'moving_', 'alone_', 'Ko', 'Bel', 'fu', 'elements_', 'Ste', 'prof', 'ning_', 'ey', 'dark', 'tur', 'les_', 'Ni', 'NOT_', 'ps_', 'bor', 'ary_', ' />\"', 'tter', 'level_', 'ys', 'apparently_', 'poorly_', 'meets_', 'killing_', 'id', 'ging_', 'ep', 'emotional_', 'brings_', 'means_', 'fla', 'episodes_', 'doubt_', 'camp', 'ME', 'Ad', 'sen', 'opinion', 'nch', 'ell', 'Ri', 'writer', 'something', 'Fe', 'flick_', 'flaw', 'ath', 'net', 'lines', 'cinematography_', 'straight_', 'slow', 'lu', 'ber', 'shi', 'husband_', 'forward_', 'form_', 'cra', 'ay', 'Fo', 'Another_', 'wo', 'whom_', 'reality_', 'hold_', 'Chi', 'Bro', 'roles_', 'move_', 'fire', 'brother_', 'Gi', 'Ben', 'review', 'que', 'cri', 'television_', 'overall_', 'French_', 'violence_', 'lla', 'enti', 'ass', 'previous_', 'forced_', 'cop', 'Oscar_', 'DE', 'possible_', 'hat', 'ear', 'budget', 'Tu', 'Ber', 'start', 'nti', 'hard', 'yn', 'school', 'deal_', 'rest', 'problems_', 'lie', 'ite', 'cool_', 'add_', 'towards_', 'reading_', 'LO', 'Gold', 'regard', 'itself', 'OK', 'leads_', 'id_', 'ved_', 'moments', 'dia', 'aw', '!) ', ' $', 'write_', 'theme_', 'Wo', 'filmed_', 'use', 'talent_', 'silly_', 'personal_', 'performances', 'needed_', 'mit', 'meant_', 'cli', 'Sho', 'tain', 'Pri', 'whi', 'comments_', 'city_', 'various_', 'sing', 'rate_', 'create_', 'respect', 'port', 'act', '194', 'message_', 'ted', 'dance_', 'case', 'ves_', 'song_', 'somehow_', 'incredibly_', 'points_', 'manages_', 'career_', 'begin_', 'Tra', 'RI', '20_', 'lai', 'interested_', 'terrible', 'hell_', 'har', 'Ku', 'Ger', 'video', 'ren', 'ky_', 'Ap', 'review_', 'ds', 'blood', 'worse', 'new', 'des', 'ways_', 'read', 'herself_', 'fre', ' *', 'set', 'rated_', 'friends', 'feature_', 'eventually_', 'blood_', 'Sea', 'ving_', 'enjoyable_', 'appear_', 'Stan', 'SE', 'thought', 'suit', 'qui', 'political_', 'person', 'les', 'gla', 'around', 'think', 'len', 'hit', 'direction', 'tale_', 'mess', 'dramatic_', 'ual_', 'gore_', 'Can', 'Am', 'ver_', 'others', 'ju', 'fairly_', 'dan', 'power_', 'dro', 'count', 'Her', 'une', 'third_', 'rop', 'crap', 'ai', 'ade', 'Joe_', 'town', 'ridiculous_', 'gone_', 'William_', 'particular_', 'older_', 'male_', 'humor', 'ard_', 'where', 'run', 'ld', 'bb', 'C_', 'ther_', 'sp', 'plenty_', 'ling', 'future_', 'stars', 'sin', 'pi', 'meet_', 'lt', 'da_', 'check_', 'En', '?\" ', 'ball', 'animation_', 'ta_', 'King_', 'hardly_', 'cul', '60', 'rt', 'Is', 'rai', 'land_', 'clu', 'wise', 'fast_', 'class', 'bra', 'worked_', 'question', 'per_', 'ok', 'expecting_', 'front_', 'come', 'Cu', 'scary_', 'past', 'hero', 'Mel', 'gri', 'average_', 'writers_', 'nk', 'fashion', 'dream', 'bear', 'attempts_', 'stand', 'total_', 'through', 'sm', 'ms', 'ice', 'gs_', 'eye', 'effort_', 'ale', 'warm', 'note', 'ger_', 'follow_', 'cro', 'vis', 'subject_', 'reviews_', 'mm', 'ect', 'Wa', 'Rob', 'imagine_', 'however_', 'decides_', 'brother', 'achieve', 'things', 'stage_', 'sound', 'rating_', 'ously_', 'ier', 'features_', 'ase', 'Vo', 'really', 'pay', 'pal', 'filled_', 'Disney_', 'telling_', 'join', 'coa', 'Lee_', 'team_', 'ov', 'emp', 'days', 'bin', 'ann', 'ally', 'women', 'social_', 'friend', 'vic', 'novel_', 'gle', 'ance_', 'weak_', 'viewers_', 'sy', 'fort', 'idea', 'Mu', 'MA', 'thriller', 'medi', 'forget_', 'York_', 'Au', 'stuff', 'ons', 'hilarious_', 'career', 'Ke', 'Christ', 'ors_', 'mentioned_', 'mark', 'def', 'watching', 'version', 'lor', 'flo', 'country_', 'G_', 'Bat', 'plain_', 'Sam', 'Anyway', 'lic', 'expected_', 'Tru', 'Great_', 'Ser', 'N_', 'And', '?) ', 'san', 'hr', 'Ham', 'pay_', 'lea', 'hol', 'Unfortunately', 'Luc', 'uti', 'row', 'history', 'bea', 'What', 'Or_', 'unless_', 'ica', 'episode', 'stra', 'sounds_', 'ability_', 'Cha', 'sco', 'represent', 'portrayed_', 'outs', 'dri', 'crap_', 'Oh', 'word_', 'open_', 'fantastic_', 'II', 'power', 'ical_', 'badly_', 'Well_', 'IN', 'Angel', 'waiting_', 'sees_', 'mor', 'ari', 'tom', 'sli', 'nation', 'mi', 'ma_', 'inf', 'Mil', 'viewing_', 'rt_', 'premise_', 'fit_', 'wl', 'unique_', 'talent', 'stay', 'fails_', 'breath', 'thi', 'ert', 'Sco', 'talk', 'slightly_', 'je', 'ah', 'NE', 'Fin', 'ridiculous', 'la_', 'Ki', 'vir', 'hea', 'ely_', 'beautiful', 'admit_', 'pu', 'crime_', 'comment_', '0_', 'shot', 'free_', 'entertaining', 'deserves_', 'mas', 'dialog_', 'hip', 'ff_', 'talented_', 'runs_', 'ini', 'ew', 'ded', 'Gri', 'roles', 'realistic_', 'clo', 'ana', 'Rat', 'Oh_', 'Man_', 'Den', 'spent_', 'rse', 'die_', 'Spe', 'Dra', 'ord', 'mal', 'ism', 'del', 'War', 'Cro', 'nn', 'min', 'fighting_', 'excellent', 'ct_', 'ask_', 'abo', 'parents_', 'ou', 'flash', 'Ver', 'Star', 'ym', 'score', 'nature_', 'den_', 'cou', 'body', 'aff', 'Ze', 'Pat', 'Mal', 'lab', 'wing_', 'theater_', 'sho', 'ow', 'mini', 'biggest_', 'Best_', 'wrote_', 'perfectly_', 'pack', 'ile', 'bly_', 'agree_', 'Perhaps_', '-- ', 'sign', 'di', 'cer', 'caught_', 'Good_', 'visual_', 'roll', 'my', 'memorable_', 'kids', 'ise_', 'hin', 'bre', 'beat', 'ring', 'reveal', 'res', 'pit', 'fa', '70', 'words_', 'wn', 'wait_', 'storyline_', 'make', 'ended_', 'ship_', 'ose', 'hot_', 'add', 'DO', 'ib', 'eri', 'directors_', 'amount_', 'Sure', 'ua', 'tin', 'mu', 'hilarious', 'eti', 'deep_', 'battle_', 'bas', 'Pre', 'Ali', 'tre', 'tie', 'thriller_', 'spirit', 'sister', 'ship', 'ser', 'rl', 'rich_', 'outside_', 'ato', 'ad_', 'Do', 'weren', 'sla', 'ro_', 'large_', 'craft', 'Shi', 'ye', 'true', 'spend_', 'rd', 'entirely_', 'Do_', 'wit', 'quickly_', 'powerful_', 'ary', 'Jane_', '193', 'sti', 'ph', 'mel', 'list', 'interest', 'footage_', 'comm', 'Tri', 'vers', 'spe', 'sna', 'sequences_', 'present', 'casting_', 'Star_', 'M_', ').', 'shoot', 'result_', 'gre', 'fore', 'ete', 'break', 'soundtrack_', 'sion_', 'poor', 'lay', 'eas', 'black', 'temp', 'nda', 'king', 'compared_', 'chu', 'break_', 'Ben_', 'ute', 'recent_', 'pure_', 'oi', 'lie_', 'burn', 'uns', 'rip', 'ner_', 'late', 'husband', 'former_', 'dull_', 'argu', 'Hollywood', 'nc', 'ming_', 'lin', 'atmosphere_', 'wood', 'why', 'amazing', 'ron', 'rat', 'gra', 'sed_', 'period_', 'game', 'Sto', 'win_', 'ult', 'scar', 'pun', 'hei', ' `', 'release_', 'present_', 'pin', 'ks_', 'appreciate_', '00', 'jump', 'bomb', 'HA', 'showed_', 'nan', 'kills_', 'decade', 'NO', 'Boy', 'ting', 'rating', 'editing_', 'actress', 'Wal', 'Ea', '\", \"', 'weird_', 'inside_', 'hair', 'eli', 'disappointed_', 'Wor', 'ski', 'ings_', 'fast', 'drag', 'adapt', 'TO', 'NG_', 'sequel_', 'fle', 'Sand', 'RO', 'whatever_', 'sleep', 'sca', 'ret', 'ney_', 'creepy_', 'cal_', '\") ', 'sor', 'popular_', 'nne', 'kick', 'ht', 'display', 'another', 'ves', 'please_', 'moves_', 'care', 'bet', 'bat', 'War_', 'CO', 'program', 'predictable_', 'positive_', 'hing_', 'copy_', 'bia', 'anything', 'affect', 'thrill', 'rk', 'mark_', 'ism_', 'edit', 'Bri', 'rate', 'missing_', 'ila', 'ial', 'guess', 'ft', 'entr', 'decide_', '30', 'sun', 'filmmakers_', 'box_', 'ating_', 'Cla', 'CA', '18', 'nie', 'material_', 'married_', 'hu', 'fin', 'blo', 'Wood', 'Tom', 'vi', 'oni', 'ena', 'BA', 'path', 'os_', 'human', 'mag', 'ins', 'earlier_', 'TI', 'LA', 'Far', 'portrayal_', 'orc', 'lame_', 'ks', 'form', 'call', 'acted_', 'Christmas_', 'violence', 'superb_', 'idiot', 'follow', 'blow', 'SO', 'Les', 'Bill_', '30_', 'sorry_', 'created_', 'common_', 'cheesy_', 'Lea', 'Carl', '!!! ', 'question_', 'pt', 'pick', 'med_', 'leaving_', 'box', 'Ci', 'Bla', 'AR', '\".<', 'ze_', 'makers_', 'draw', 'ala', 'Day', 'B_', 'succeed', 'pat', 'ones', 'gay_', 'cy', 'barely_', 'ara', 'air_', 'San', 'Director_', 'xt', 'screenplay_', 'pan', 'miss_', 'does', 'consider_', 'com', 'ER', 'ub', 'ple', 'mystery_', 'mine', 'involving_', 'familiar_', 'Mari', 'German_', 'nat', 'eye_', 'dly_', 'disa', 'country', 'att', 'app', 'tho', 'press', 'mat', 'llo', 'fi_', 'connect', 'called', 'ane', 'May', 'LE', 'K_', 'Italian_', 'Every_', 'sure', 'ster_', 'starring_', 'horse', 'further_', 'entertainment_', 'ense', 'dog', 'disappointed', 'cher', 'af', 'won_', 'secret', 'likes_', 'indi', 'follows_', 'ball_', 'God_', 'Cur', '196', 'wasted_', 'ideas_', 'cur', 'Bal', 'lly', 'ire', 'gu', 'general_', 'believable_', 'aus', 'Stu', 'Despite_', 'understand', 'lit', 'last', 'cy_', 'bought_', 'ago_', 'Very_', 'Only_', 'Han', 'wear', 'thu', 'themselves', 'recently_', 'ms_', 'intention', 'focus_', 'ations_', 'ali', 'yp', 'yet', 'ici', 'gy', 'exten', 'Min', 'Lin', 'Ed', 'Dar', 'tis', 'credits_', 'Now', '50', 'sister_', 'setting_', 'odd_', 'missed_', 'mea', 'lot', 'ight_', 'gg', 'fantasy_', 'ash', 'US_', 'Overall', 'young', 'suddenly_', 'nge', 'members_', 'dra', 'cover_', 'artist', 'Watch_', 'moment', 'background_', '.....', 'seriously_', 'mic', 'considered_', 'Ric', 'Pres', '! <', ' (\"', 'opinion_', 'ise', 'gun', 'different', 'Sou', 'utterly_', 'asse', 'alt', 'Though_', 'LY_', 'Big_', 'situation_', 'rio', 'il_', 'ef', 'ding', 'Still', 'Cre', 'younger_', 'special', 'raise', 'El', '90', 'walk_', 'tone_', 'tes_', 'sitting_', 'glad_', 'base', 'Let', 'Boo', 'vent', 'lead', 'considering_', 'animated_', 'witness', 'torture', 'throw', 'sea', 'load', 'lim', 'hot', 'following_', 'ess_', 'center', 'Scott_', 'NG', 'BO', '15_', 'word', 'rid', 'pop', 'ions', 'ges', 'enter', 'Sal', 'Gre', 'ties_', 'spl', 'hy', 'ery_', 'disappointment', 'avoid_', 'Jud', 'Ce', 'need', 'hel', 'hands_', 'develop', 'cause_', 'Steve_', 'zombie_', 'voice', 'successful_', 'eo', 'Mary_', 'EN', 'Because_', 'stage', 'rv', 'master', 'crazy_', 'Mer', 'rent', 'hes', 'OF_', 'yl', 'tive_', 'remake_', 'passion', 'managed_', 'fra', 'fans', 'drive', 'CH', 'Blo', 'Art', 'surprise_', 'suggest', 'list_', 'imme', 'crew_', 'continu', 'Sci', 'solid_', 'ora', 'eu', 'Men', 'Cal', 'sus', 'shar', 'omi', 'ita', 'istic_', 'Pl', 'Jack', 'Davi', 'wonder', 'slasher_', 'produced_', 'frame', 'cle', 'Em', 'subs', 'state', 'seek', 'ona', 'mention', 'laughing_', 'iti', 'hide', 'date', 'Some', 'touch', 'soft', 'shop', 'interview', 'dumb_', 'clean', 'bored_', 'bill', 'bed_', 'beauty_', 'basic_', 'Cou', 'zi', 'ultimately_', 'thinks_', 'sto', 'odd', 'masterpiece', 'kind', 'cool', 'Ac', 'tto', 'sit', 'nci', 'ized_', 'gore', 'dee', 'boo', 'Va', 'Come', 'ning', 'escape', 'eng', 'RA', 'America', 'worthy_', 'unre', 'tche', 'shame_', 'nothing', 'explo', 'Sl', 'Bus', 'BE', '13', 'pra', 'least', 'effect_', 'deliver', 'boys_', 'Wi', 'Stra', 'Fr', 'Cap', '**', '\".', 'space_', 'potential_', 'oli', 'lon', 'ind', 'gor', 'gon', 'generally_', 'ext', 'chees', 'beginning', 'Tony_', 'wait', 'meaning', 'ley', 'fire_', 'des_', 'cop_', 'ati', 'Ram', 'Ex', '195', 'were', 'survive', 'ral_', 'push', 'mut', 'killer', 'dist', 'charm', 'ang', 'Frank', 'writing', 'worth', 'wor', 'stop', 'stick_', 'ler_', 'chemistry_', 'cap', 'ae', 'Ya', 'second', 'ost', 'machine', 'lessly_', 'individual', 'experience', 'ead', 'dancing_', 'Sy', 'Del', 'Bor', '!!', 'would', 'suspense_', 'project', 'intelligent_', 'cover', 'asi', 'Brit', 'speak_', 'season_', 'oth', 'ida', 'factor', 'amo', 'World_', 'Once_', 'Hard', ' ... ', 'tol', 'live', 'changed_', 'brain', 'uri', 'seriously', 'release', 'likely_', 'gne', 'explain_', 'ance', 'added_', 'Here_', 'AL', '% ', 'wre', 'spar', 'gree', 'eyes', 'detail', 'Night', 'Mag', 'term', 'tape', 'public_', 'pleas', 'lives', 'ker', 'ile_', 'had', 'dre', 'directing_', 'dialog', 'convincing_', 'chance', 'big', 'beat_', 'appl', 'truth_', 'spa', 'rica', 'monster_', 'market', 'imm', 'have', 'fine', 'clue', 'card', 'blu', 'adult_', 'Who', 'Jim_', 'Bea', '.)', 'value', 'twist_', 'thrown_', 'phe', 'model', 'entertainment', 'Where_', 'LI', 'Ju', 'Black_', 'ura', 'nic', 'han', 'failed_', 'cinematic_', 'bizarre_', 'ben', 'Gu', 'rare_', 'mbo', 'historical_', 'everyone', 'epi', 'ate', 'ada', 'Cli', 'wind', 'sou', 'nder', 'mb', 'held_', 'formula', 'flu', 'effect', 'clever_', 'catch_', 'W_', 'pick_', 'business_', 'attempt', 'Show', 'Paul', 'segment', 'romance_', 'ram', 'nom', 'how', 'ged_', 'flow', 'equally_', 'computer_', 'commercial', 'Val', 'IMDb_', 'trans', 'sent_', 'pet', 'lk', 'ider', 'corn', 'channel', 'Ge', 'Christopher_', 'ways', 'tat', 'subject', 'shooting_', 'return_', 'neither_', 'neighbor', 'lady_', 'impossible_', 'Spa', 'BI', '***', ' -', 'yr', 'violent_', 'syn', 'suffer', 'fur', 'cru', 'Charl', 'secret_', 'rp', 'ros', 'pie', 'ious_', 'hoping_', 'ence_', 'Ye', 'Son', 'trick', 'nia', 'effective_', 'desp', 'costume', 'check', 'board_', 'ami', 'aire', 'ado', 'Whi', 'Two_', 'Rose', 'Green', 'surround', 'promise', 'mad', 'lesson', 'imagination', 'hum', 'excuse_', 'escape_', 'aspect_', 'ak', 'Thu', 'Pal', 'Kr', 'Bur', 'vil', 'travel', 'reso', 'protagonist', 'object', 'nes', 'longer_', 'lia', 'key', 'incredible_', 'hoo', 'fool', 'expression', 'bot', 'bel', 'Ree', 'Oscar', 'Fu', 'safe', 'remains_', 'note_', 'natural_', 'just', 'hm', 'grace', 'credit_', 'constantly_', 'Sam_', 'Ren', 'OK_', 'view', 'unlike_', 'surprise', 'success_', 'ssion', 'song', 'player', 'match_', 'ela', 'din', 'critic', 'accident', '20', 'otherwise_', 'material', 'knowing_', 'ings', 'ffe', 'depth_', 'cula', 'Whe', 'Ph', 'Ai', 'respect_', 'puts_', 'pher', 'kin', 'concept_', 'zed_', 'unfortunate', 'que_', 'predictable', 'order', 'onto_', 'meta', 'ev', 'dress', 'dog_', 'cell', 'Thi', 'Frank_', 'spin', 'rot', 'military_', 'hall', 'cut', 'choice_', 'chick', 'bs', 'Za', 'Many_', 'witch', 'weak', 'swa', 'rti', 'producers_', 'inn', 'gold', 'fault', 'ez', 'cute_', 'cult_', 'WO', 'SH', 'drink', ', (', 'wall', 'theme', 'taste', 'sion', 'iz', 'gun_', 'ek', 'drawn_', 'anyone', 'antic', 'tension_', 'team', 'sweet_', 'ree', 'perform', 'partner', 'horrible', 'contains_', 'Es', 'De_', 'Chris_', 'AT', 'vote', 'tch_', 'singing_', 'shine', 'hasn', 'happen', 'gal', 'demon', 'dar', 'Jer', 'GE', 'ske', 'indeed_', 'guys', 'emotion', 'apart_', 'See', 'Roger', 'Pol', 'trouble_', 'seat', 'planet', 'exciting_', 'err', 'dream_', 'cus', 'arrive', 'HO', '!!!!', 'trip_', 'today_', 'sle', 'setting', 'rr', 'plus_', 'og', 'faci', 'disp', 'crack', 'cen', 'Gun', 'words', 'will', 'prefer', 'pect', 'noi', 'leader', 'dit', 'deal', 'creep', 'Zo', 'Sid', 'East', 'record', 'poo', 'normal_', 'message', 'ffi', 'fer', 'correct', 'colle', 'ator', 'Ros', 'Other_', 'zen', 'usi', 'pil', 'mental_', 'ji', 'immediately_', 'ible_', 'capt', 'bab', 'Chu', 'tar', 'stands_', 'progress', 'making', 'lc', 'fic', 'exp', 'encounter', 'circ', 'change', 'annoying', 'Mur', 'Lor', 'Little_', 'tl', 'rain', 'fail', 'died_', 'Time', 'Blood', 'tell', 'reflect', 'ked_', 'judge', 'ide', 'development_', 'control_', 'clima', 'bed', 'alr', 'Tre', 'trouble', 'thr', 'spot', 'ress', 'red', 'pol', 'hill', 'eb', 'TH', 'Ken', '\\x85 ', 'surprisingly_', 'rep', 'freak', 'dep', 'college_', 'brilliant', 'blin', 'bath', 'People_', 'Nat', 'Charles_', 'walking_', 'ref', 'reco', 'pace_', 'nde', 'mil', 'mainly_', 'literally_', 'fia', 'dull', 'Sn', 'Ever', 'Dam', 'Bre', 'Brad', 'Both_', 'ward', 'trash', 'tough_', 'serve', 'reasons_', 'ngs', 'llen', 'ines', 'honest', 'focus', 'carrie', 'aim', 'Us', 'Prince', 'Nothing_', 'truth', 'supp', 'sma', 'musical', 'inco', 'fight', 'enc', 'bother', 'arch', 'Jon', 'Japan', 'Er', 'Des', '!!!', 'unw', 'unfortunately_', 'til', 'rese', 'marri', 'ior', 'ene', 'ain_', 'Aust', 'ular', 'tru', 'tch', 'tale', 'prop', 'phan', 'orat', 'nit', 'matter', 'host', 'hood', '\\\\&undsc', 'Not', 'Film_', 'Ama', 'yle', 'var', 'standards', 'pers', 'nice', 'meaning_', 'laughs_', 'joke_', 'iss', 'happi', 'era_', 'WH', 'Lil', 'Girl', 'ES', ' />-', 'watche', 'tant', 'qua', 'presented_', 'minor_', 'gro', 'fie', 'door', 'corp', 'catch', 'cally_', 'bert', 'Indian_', 'Gen', 'questions_', 'lacks_', 'forever', 'establish', 'esc', 'cheap', 'Sol', 'while', 'twist', 'society_', 'pass_', 'overa', 'merely_', 'highlight', 'flat_', 'fill', 'color', 'cartoon_', 'Will_', 'NT', 'IT', 'Harry_', 'Fan', 'youth', 'possible', 'orm', 'free', 'eight', 'destroy', 'creati', 'cing_', 'ces_', 'Carr', 'unl', 'suggest_', 'slo', 'owner', 'kh', 'instead', 'influence', 'experiment', 'convey', 'appeal_', 'Ol', 'Night_', '---', 'vy', 'terms_', 'sick_', 'par', 'once', 'law', 'ize_', 'infe', 'Spo', 'House_', '\\x85', 'studio_', 'simple', 'rre', 'guard', 'girlfriend_', 'fear', 'dam', 'concern', 'amusing_', 'adaptation_', 'Ms', 'King', 'water', 'ory_', 'officer', 'litera', 'knock', 'grat', 'falling_', 'ered_', 'cow', 'cond', 'alo', 'Kar', 'Der', 'Cri', 'text', 'skin', 'sequel', 'level', 'impression_', 'ice_', 'force_', 'fake_', 'deri', 'contain', 'band_', 'appa', 'South_', 'HE', 'Conn', 'wise_', 'ur_', 'ual', 'sy_', 'luck', 'lack', 'impressi', 'disaster', 'business', 'being', 'beg', 'Burt', ' <', 'villain_', 'type', 'shoot_', 'shame', 'sb', 'pt_', 'proves_', 'manner', 'lame', 'impressive_', 'ern', 'disappear', 'alone', 'LL', 'Having_', 'Brook', 'Arm', '!\"', 'works', 'state_', 'shock', 'rev', 'mus', 'int', 'ino', 'images_', 'brid', 'berg', 'alis', 'Clo', 'singer', 'shr', 'rock_', 'provides_', 'page', 'instance', 'drug_', 'crime', 'beautifully_', 'acts_', 'UN', 'Tal', 'Bruce_', 'self_', 'reality', 'mans', 'lived_', 'innocent_', 'ically_', 'fall', 'dict', 'Henry_', 'Fox', 'Bac', 'sold', 'says', 'period', 'ome', 'melodrama', 'include_', 'evil', 'Ins', 'stati', 'silent_', 'ria', 'mom', 'met_', 'guns', 'ground', 'gate', 'fell_', 'cle_', 'cari', 'birth', 'Look', 'Hill', '1950', 'water_', 'reminded_', 'express', 'delight', 'als_', 'Wes', 'Mis', 'Louis', 'Grant', 'xe', 'written', 'touch_', 'ters_', 'squa', 'moral', 'ffer', 'aut', 'appearance_', 'Sim', 'Nor', 'Mont', 'IS_', 'Cath', 'take', 'shel', 'protect', 'gut', 'ans', 'Too_', 'Scar', 'Death', 'American', 'AND_', 'throw_', 'suck', 'standard_', 'sil', 'should', 'share_', 'scary', 'loves_', 'indu', 'foot', 'ew_', 'answer', 'Wit', 'Van_', 'Terr', 'Str', 'subtle_', 'stories', 'store_', 'must', 'ments_', 'mbi', 'gs', 'ft_', 'fellow_', 'erat', 'eni', 'crash', 'ches', 'becoming_', 'appeared_', 'TE', 'Fal', '., ', 'visit', 'viewer', 'tag', 'surely_', 'sur', 'stri', 'putting_', 'pull_', 'process', 'pointless_', 'nta', 'mass', 'hur', 'hell', 'gue', 'girls', 'Rev', 'Pan', 'Billy_', 'villain', 'suppose_', 'sick', 'prom', 'narrat', 'mer_', 'followed_', 'decision', 'auto', 'adult', 'Movie_', 'Ban', 'tone', 'thoroughly_', 'sympath', 'sts_', 'sk', 'pot', 'piece', 'offers_', 'nte', 'most', 'helps_', 'det', 'cti', 'brief_', 'block', 'adds_', 'Street', 'Red_', 'Qui', 'Love', 'BL', 'support_', 'ses_', 'rta', 'recognize', 'mission', 'ignore', 'hon', 'broad', 'bid', 'ano', 'Swe', 'Shakespeare', 'Ron', 'Mart', 'Charlie_', 'thanks_', 'tage_', 'serial_', 'revenge_', 'ors', 'office_', 'nst', 'feature', 'drugs', 'disturb', 'anymore', 'Bl', \", '\", 'univers', 'touching_', 'strange', 'improve', 'iff', 'heavy_', 'fare', 'central_', 'buff', 'Inter', 'EA', 'worr', 'turning_', 'tired_', 'than', 'seemingly_', 'motion_', 'ku', 'has', 'goe', 'evi', 'duc', 'dem', 'cinematography', 'aspects_', 'any', 'High', 'Cho', 'tick', 'surviv', 'suicide', 'return', 'remember', 'ppy_', 'noti', 'mess_', 'mes', 'inve', 'grow', 'enge', 'dom', 'Tar', 'Since_', 'Roy', '19', ' ( ', 'track_', 'racis', 'narrative_', 'nal', 'mysterious_', 'moral_', 'imp', 'desert', 'compl', 'along', 'Sw', 'Super', 'HI', 'Dor', 'America_', 'vert', 'superb', 'stu', 'shouldn', 'science_', 'rough', 'ray', 'ova', 'dumb', 'deb', 'court', 'control', 'complex_', 'butt', 'Joe', 'Ir', 'Direct', 'throughout', 'tende', 'stic_', 'somewhere_', 'sel', 'pti', 'picked_', 'parts', 'mob', 'fear_', 'developed_', 'couple', 'cas', 'attitude', 'apo', 'Sun', 'MO', 'L_', 'Ei', 'teen_', 'pull', 'ough', 'hunt', 'favor', 'dos', 'delivers_', 'chill', 'ately', 'Van', 'vat', 'tz', 'trip', 'stuck_', 'rela', 'mood_', 'finish', 'essen', 'ering_', 'disappoint', 'could', 'commit', 'TA', 'Lam', 'Harris', 'whole', 'value_', 'ural', 'sim', 'season', 'redeeming_', 'poli', 'please', 'happened', 'geo', 'force', 'ero', 'core_', 'cand', 'blue', 'bell', 'assi', 'asp', 'adventure_', 'Sin', 'McC', 'whatsoever', 'sky', 'shows', 'pse', 'language_', 'insight', 'ier_', 'finding_', 'everything', 'cker', 'challenge', 'books_', 'Out', 'Ji', 'Glo', 'tune', 'terri', 'prem', 'oe', 'nish', 'movement', 'ities_', 'effort', 'absolute_', 'Brian_', 'Alan_', 'unin', 'unde', 'ude', 'tear', 'oh_', 'ize', 'ilia', 'hint', 'credib', 'craz', 'choice', 'charming_', 'audiences_', 'apart', 'York', 'Marc', 'wonderful', 'willing_', 'wild', 'repeated', 'refer', 'ready_', 'radi', 'punch', 'prison', 'painful_', 'pain', 'paid_', 'pace', 'nni', 'mate_', 'hole', 'future', 'disturbing_', 'cia', 'buck', 'ache', 'Taylor', 'Lind', 'Hol', 'vel', 'tor_', 'terrific_', 'suspense', 'sf', 'research', 'remark', 'problem', 'plu', 'pathetic_', 'negative_', 'lovely_', 'lift', 'hype', 'gl', 'earn', 'ave', 'Their_', 'SS', 'Cass', 'slowly_', 'rented_', 'opportunity_', 'fat', 'every', 'este', 'dub', 'cons', 'bull', 'Sav', 'P_', 'My', 'wondering_', 'unbe', 'twe', 'statu', 'shin', 'rock', 'party_', 'inform', 'heroine', 'hate', 'girlfriend', 'fate', 'ette', 'dies_', 'comparison', 'alb', 'ak_', 'Lis', 'Christian_', 'Act', 'yon', 'storyline', 'soul', 'rece', 'rea', 'product', 'nut', 'lets_', 'funniest_', 'field_', 'city', 'Stephen_', 'GH', 'Ann', 'wee', 'weapon', 'viewing', 'tte', 'sty', 'spi', 'quality', 'price', 'possess', 'ntly', 'dd', 'compa', 'buy', 'agree', 'Hal', 'Comp', 'twists_', 'shak', 'nudity_', 'mati', 'giant_', 'company_', 'baby_', 'admit', 'Finally', 'wn_', 'whe', 'romance', 'presence_', 'myself', 'jokes', 'ident', 'friendship', 'fift', 'explore', 'episodes', 'element_', 'edi', 'eat', 'conve', 'Ira', 'However_', 'DI', 'winning_', 'sexy_', 'rescue', 'physical_', 'pe_', 'oid', 'nobody_', 'nis', 'mad_', 'lin_', 'ket', 'hom', 'generation', 'dance', 'attack', 'appropriate', 'allowed_', 'Ve', 'RS', 'Mr_', 'Kid', 'Instead_', 'Hell', 'Everything_', 'Before_', 'Arthur_', 'waste', 'themes_', 'stunt', 'rap', 'million_', 'hi_', 'games', 'fair_', 'distract', 'cross', 'boat', 'available_', 'abilit', 'Hitler', 'Fl', 'Cas', 'wearing_', 'spirit_', 'rede', 'rb', 'perspective', 'ocr', 'mac', 'kle', 'gang_', 'floor', 'fab', 'Pen', 'ON', 'Kur', 'Jerry_', 'Here', 'Andrew', '??', 'window', 'uss', 'mp_', 'intens', 'expert', 'ei', 'changes_', 'carry_', 'born_', 'bee', 'award', 'Sor', 'Jos', 'Home', 'Cat', '1980', 'zing_', 'victim', 'tight', 'space', 'slu', 'pli', 'neat', 'mistake', 'ky', 'joke', 'includes_', 'hear', 'emb', 'dev', 'damn_', 'confusi', 'church', 'NI', 'Clark', 'theatre', 'sso', 'lock', 'laughed_', 'fran', 'drive_', 'danger', 'alle', 'Which_', 'Western', 'Roman', 'Rit', 'Pie', 'Law', 'France', 'Did_', '14', 'vor', 'usual', 'turn', 'supposedly_', 'sm_', 'satisf', 'realistic', 'pieces_', 'nse', 'near', 'image_', 'flat', 'development', 'design', 'contrast', 'colla', 'board', 'arti', 'anywhere', 'Unfortunately_', 'Rock', 'Ford', 'Doc', 'white', 'small', 'replace', 'prison_', 'owe', 'minat', 'may', 'inspired_', 'helped_', 'expect', 'doll', 'dish', 'chase', 'awa', 'Those_', 'Second', 'OR', 'Nazi', 'Ell', 'watchable', 'via', 'test', 'stick', 'step_', 'speech', 'relationship', 'pass', 'ote', 'nel', 'mild', 'gue_', 'embarrass', 'describe_', 'bound', 'bother_', 'aging', 'Julie', '70s', 'via_', 'street_', 'squ', 'scream', 'pos', 'overs', 'mix_', 'martial_', 'magic_', 'jud', 'gener', 'eh', 'concept', 'alien', 'FO', 'which', 'values_', 'success', 'soldiers_', 'pla', 'lous', 'lose_', 'io', 'ike', 'fish', 'eth', 'ddy', 'crowd', 'creative_', 'conc', 'beh', 'bbi', 'Matth', 'Europe', '1970', 'ulat', 'track', 'target', 'swea', 'stal', 'refuse', 'phon', 'pho', 'hang', 'gea', 'doubt', 'compr', 'cloth', 'cliché', 'bland', 'behavior', 'aci', 'Simp', 'Leon', 'England', 'Edi', 'Cons', ')<', ' .', 'wy', 'worker', 'volu', 'vehicle', 'tour', 'random_', 'phone_', 'ong', 'moved_', 'grave', 'folk', 'filming_', 'feelings_', 'build_', 'basi', 'Tor', 'TR', 'Sk', 'New', 'Miss_', 'Kl', 'Kat', 'Boll', 'zil', 'ust', 'robot', 'result', 'reac', 'ped', 'pea', 'ow_', 'mmi', 'laughs', 'issues_', 'intended_', 'impressed_', 'favorite', 'dw', 'documentary', 'doctor_', 'debut', 'account', 'North', 'Im', 'GO', 'weird', 'transform', 'train', 'swi', 'sum', 'soci', 'same', 'reh', 'ld_', 'ffic', 'conversation', 'comedic_', 'artistic_', 'adi', 'accept', 'Stone', 'Jew', 'CR', 'threaten', 'stea', 'scra', 'sake', 'potential', 'listen', 'het', 'cted_', 'cod', 'chase_', 'berg_', 'appear', 'Ton', 'Queen', 'Mark_', 'Hall', 'FI', 'wer', 'thes', 'sons', 'provide_', 'nger', 'ney', 'mot', 'mask', 'flesh', 'exe', 'dozen', 'disgu', 'conclusion', 'accent', 'Victoria', 'SP', 'Jr', 'Char', 'Albert', 'try', 'tal_', 'round_', 'mix', 'ison', 'hundred', 'holds_', 'gger', 'approach_', 'Space', 'Okay', 'MI', 'Love_', 'Elvi', 'Doo', 'tragic_', 'sweet', 'stud', 'sible', 'remain', 'pur', 'nts_', 'ken', 'got', 'fam', 'edge_', 'Hea', 'Film', 'Cast', 'teenage_', 'technical_', 'skip', 'rend', 'our', 'illus', 'ham', 'favourite_', 'ensi', 'consist', 'cold_', 'cent', 'cate', 'MAN', 'F_', 'Die', 'Cub', 'Chinese_', 'yourself', 'ugh', 'stretch', 'society', 'rth', 'root', 'reminds_', 'reg', 'rd_', 'put', 'purpose', 'ition_', 'humanity', 'gotten_', 'fest', 'feel', 'fascinat', 'failure', 'culture_', 'cont', 'allow_', 'pursu', 'preci', 'if', 'belong', 'VE', 'Sar', 'O_', 'Nic', 'Dead', 'AC', ' ****', 'western_', 'uct', 'thro', 'tes', 'struggle_', 'straight', 'stic', 'similar', 'repe', 'pid', 'nes_', 'mou', 'irre', 'hic', 'explained', 'deeply_', 'cs_', 'confront', 'clichés', 'attack_', 'asks_', 'Yet_', 'Was_', 'Tro', 'Stre', 'Rei', 'Kelly_', 'Julia', 'Bas', '? <', 'ties', 'technique', 'stunning_', 'slight', 'skill', 'sat_', 'outstanding_', 'lies_', 'journey_', 'hap', 'expla', 'definit', 'critics_', 'continue_', 'compelling_', 'charge', 'Thing', 'PE', 'Marie', 'Lynch', 'Jason_', 'Hen', 'Av', '.... ', '\\x97', 'wanting_', 'wanna', 'transp', 'thats_', 'smok', 'respons', 'professional_', 'print', 'physic', 'names_', 'inge', 'infa', 'grip', 'green', 'ggi', 'buster', 'bum', 'belief', 'accept_', 'abuse', 'Rain', 'Pos', 'Lee', 'Hoo', 'All', 'threa', 'soundtrack', 'realized_', 'ration', 'purpose_', 'notice_', 'member_', 'lovers', 'log', 'kni', 'inse', 'inde', 'impl', 'government_', 'door_', 'community', 'also', 'Zombie', 'WI', 'Sur', 'Stewart_', 'Roo', 'NA', 'Comm', 'Anna', 'wonderfully_', 'vac', 'tit', 'thus_', 'shadow', 'rg', 'resol', 'religious_', 'problems', 'nonsense', 'naked_', 'marvel', 'fantastic', 'em_', 'earth_', 'demand', 'cost', 'bes', 'band', 'background', 'Mas', 'Bon', 'African', ':<', 'thousand', 'realism', 'race_', 'ption', 'pred', 'neg', 'met', 'little', 'kn', 'flying_', 'ement', 'editing', 'abandon', 'Take', 'On', 'Mich', 'Gin', 'Fer', 'wide', 'victim_', 'spell', 'search_', 'rush', 'road_', 'rank', 'pping_', 'mpl', 'kil', 'incomp', 'humour_', 'group', 'ghost', 'ens', 'electr', 'edg', 'dru', 'culture', 'cars', 'Wil', 'UR', 'Haw', 'Give', 'Fat', 'Dou', 'Ant', 'AD', 'vs', 'tia', 'rei', 'regret', 'necessar', 'master_', 'mani', 'honestly_', 'hey', 'hadn', 'gant', 'fresh_', 'exce', 'document', 'direct_', 'dated_', 'afraid_', 'OU', 'Mid', 'Len', 'Good', 'Beat', 'yer', 'walk', 'ture_', 'train_', 'theor', 'stink', 'spit', 'rarely_', 'proper', 'intelligen', 'hed_', 'hair_', 'forgot', 'fascinating_', 'ere', 'deliver_', 'believable', 'awesome_', 'attend', 'actresses_', 'Up', 'Par', 'Bad_', 'zombie', 'ys_', 'wards', 'trash_', 'strip', 'spectacular', 'six_', 'silly', 'shed_', 'praise', 'loud_', 'inspir', 'insi', 'god', 'four', 'devi', 'Sir', 'Plan', 'PL', 'Everyone_', 'Dol', 'thinking', 'store', 'spo', 'rou', 'pou', 'opposite', 'dud', 'difference_', 'deli', 'compare_', 'cable', 'VER', 'Tim_', 'Ob', 'Jane', 'Jam', 'Don_', 'CI', 'yo', 'want', 'villains', 'toward_', 'taste_', 'support', 'stone', 'sted_', 'spect', 'satire', 'row_', 'rag', 'observ', 'nel_', 'motiv', 'moro', 'lust', 'lect', 'ively_', 'gli', 'gie', 'fet', 'eld', 'div', 'creating_', 'brain_', 'bird', 'attention', 'ates_', 'ald', 'Sher', 'Russ', 'Rea', 'Joan_', 'Gab', 'Coo', 'Bond', '40', 'trade', 'sive_', 'routine', 'plane_', 'photograph', 'ound', 'om_', 'nk_', 'mountain', 'mate', 'listen_', 'isa', 'imagina', 'gia', 'embarrassing', 'convince', 'building_', 'avoid', 'Wow', 'SA', 'Al_', 'vy_', 'unsu', 'tty_', 'situations_', 'sensi', 'results', 'recogni', 'quick', 'plan_', 'mod', 'masterpiece_', 'limit', 'lar', 'gorgeous_', 'fil', 'ensu', 'edly_', 'cor', 'context', 'bul', 'bottom_', 'began_', 'animation', 'anc', 'acc', 'Ty', 'Sc', 'London_', 'Lewis', '.\"<', 'weight', 'rubbish', 'rab', 'project_', 'powers', 'personalit', 'offer_', 'noir_', 'killed', 'justif', 'jun', 'information_', 'gem', 'ative_', 'PO', 'Jeff_', 'Gui', 'voca', 'tab', 'spot_', 'remind', 'proceed', 'kick_', 'ious', 'grab', 'enem', 'educat', 'claim', 'cks', 'charisma', 'bal', 'Scott', 'Over', 'Mus', 'Laure', 'Kan', 'Hunt', 'Dead_', 'Acti', '90_', '50_', ' ! ! ! ! ! ! ! ! ! !', 'ws_', 'vul', 'village', 'speed', 'skills', 'public', 'outl', 'naive', 'mos', 'latter_', 'ki_', 'iat', 'honest_', 'ga_', 'emotions_', 'detective_', 'citi', 'bits_', 'answer_', 'accomplish', 'Washington', 'Sm', 'Dal', 'CE', 'Bett', 'Af', '40_', 'sell', 'pret', 'pper', 'opera', 'notabl', 'involved', 'important', 'humorous', 'finale', 'dise', 'date_', 'contribut', 'complain', 'comedies_', 'battle', 'balance', 'Go_', 'Fla', 'Alon', '); ', 'wis', 'ups', 'spoke', 'pulled_', 'points', 'mediocre_', 'ker_', 'introduced_', 'independent_', 'hil', 'fits_', 'eating_', 'confused_', 'concerned', 'cing', 'ca_', 'bran', 'borat', 'bing_', 'ay_', 'abr', 'Russian_', 'Kevin_', 'H_', 'Fred_', 'Exce', 'English', 'Danny_', 'Dani', 'Coll', 'Alt', '100_', 'used', 'translat', 'shape', 'odi', 'manage_', 'loy', 'lik', 'ibi', 'eat_', 'behav', 'apparent_', 'admi', 'acr', 'ach', 'Young_', 'Run', 'Martin_', 'Mak', 'Hart', 'Asi', '25', '& ', 'trag', 'terror', 'tea', 'shallow', 'rob', 'rape', 'pond', 'ole', 'neck', 'nature', 'loving_', 'jerk', 'hours', 'hidden_', 'gar_', 'field', 'fel', 'existence', 'erotic', 'constant_', 'cau', 'bar_', 'VI', 'Univers', 'Sen', 'CK', '100', 'wealth', 'wave', 'understanding_', 'sole', 'ral', 'none', 'nasty_', 'mari', 'likable_', 'ith', 'intense_', 'hou', 'gh_', 'ely', 'dic', 'dea', 'clip', 'bow', 'UL', 'Nu', 'Moon', 'Ital', 'Ed_', 'Cle', '.......', 'yeah', 'tree', 'successful', 'ril', 'ract', 'philosoph', 'parents', 'marriage_', 'lte', 'ject', 'ite_', 'hun', 'fantas', 'fame', 'extra_', 'dreadful', 'details_', 'dad_', 'capture_', 'annoy', 'Other', '?!', 'tions', 'stalk', 'speak', 'revolution', 'redu', 'pretend', 'politic', 'places_', 'parody', 'park', 'onic', 'nowhere_', 'mono', 'mile', 'manipulat', 'loses_', 'lli', 'into', 'hid', 'ghost_', 'gha', 'engage', 'assum', 'ador', 'admire', 'X_', 'See_', 'Full', 'Eye', 'zy', 'ware', 'ven_', 'uncle', 'treated_', 'television', 'surreal', 'student_', 'rival', 'ride_', 'recall', 'nudity', 'locations', 'ility', 'hamm', 'gags', 'fill_', 'dealing_', 'co_', 'climax_', 'bon', 'atmosphere', 'aged_', 'Rock_', 'Kim', 'Had', 'Brid', 'Anton', 'zombies_', 'unfunny', 'techn', 'source', 'section', 'pris', 'priest', 'police', 'olo', 'nine', 'maker', 'limited_', 'ik', 'genius_', 'enjoyable', 'distan', 'desperate_', 'believe', 'asked_', 'appearance', 'Ring', 'Pete', 'Master', 'Kin', 'Harr', 'Earth', 'Dog', 'Brown', 'Bren', 'Add', 'web', 'tee', 'sucks', 'structure', 'regi', 'porn_', 'osi', 'llian', 'lett', 'length_', 'ior_', 'hal', 'faith', 'enta', 'deserve_', 'cartoon', 'bs_', 'ahead_', 'Got', 'Eu', 'Americans_', 'Alex', 'speaking_', 'smil', 'photographe', 'ope', 'mpe', 'minim', 'million', 'mental', 'magnificent', 'lur', 'lov', 'keeping_', 'iting', 'homo', 'haunt', 'fiction_', 'fee', 'exploit', 'entertain', 'dding', 'attracti', 'advice', 'Park', 'Fur', 'Cage', 'suc', 'songs', 'smart_', 'shock_', 'rif', 'repl', 'ranc', 'ran', 'photography_', 'patient', 'ladies', 'hated_', 'growing_', 'cheer', 'attractive_', 'ass_', 'approach', 'ants_', 'Mrs', 'Hay', 'Hank', 'Eli', 'EVER', 'Batman_', 'week', 'sword', 'rac', 'promot', 'portray', 'pictures_', 'lt_', 'ito', 'interna', 'forgive', 'device', 'corrupt', 'choreograph', 'chop', 'blame_', 'atch', 'VE_', 'KE', 'Johnny_', 'vity', 'ville', 'vas', 'uit', 'tional_', 'quote', 'quick_', 'producer_', 'personally_', 'parti', 'oa', 'nity', 'loo', 'ives', 'increas', 'ical', 'heads_', 'graphic', 'going', 'featuring_', 'defin', 'cute', 'criminal', 'cheat', 'cash', 'cann', 'bol', 'bec', 'Welles', 'SPOILERS', 'Power', 'Kell', 'Georg', 'Gene_', 'Blai', 'Again', '11', 'yell', 'vious', 'unusual_', 'tradition', 'summar', 'stunn', 'revealed', 'remo', 'psychi', 'provi', 'prepare', 'offer', 'insane', 'happens', 'efforts', 'delic', 'current_', 'construct', 'bil', 'aries', 'animals_', 'advance', 'Kong', 'Jan', 'Howard', 'Daw', 'Cru', ' !', 'terribly_', 'teache', 'tas', 'sudden', 'sleaz', 'sharp', 'ress_', 'rape_', 'ppi', 'numbers_', 'mouth', 'lower', 'ime', 'ifie', 'ideal', 'exception_', 'ema', 'charm_', 'breaking_', 'addition_', 'Walke', 'Lat', 'Jean_', 'Eddie_', 'City_', '.\"', 'warning', 'versions', 'tack', 'reli', 'ration_', 'prove_', 'plo', 'pile', 'performer', 'monk', 'intellectual', 'handle', 'ets', 'essor', 'ature', 'atri', 'ans_', 'Int', 'Fel', 'European_', 'Cus', 'As', 'wr', 'worst', 'witty', 'wild_', 'wedding', 'students_', 'sadly_', 'princip', 'paint', 'mmy', 'mixed_', 'kinda_', 'frequent', 'discover_', 'dal', 'command', 'colour', 'bou', 'bored', 'Wild', 'Ul', 'Really', 'Mitch', 'Cinema', 'Andy_', '16', 'visuals', 'varie', 'ut_', 'unfold', 'suspect', 'semi', 'responsible_', 'religion', 'rapi', 'py_', 'otic', 'numerous_', 'news', 'nces', 'kl', 'junk', 'joy', 'insult', 'festival', 'drop_', 'costumes_', 'been', 'bag', 'aware_', 'aver', 'Mir', 'Last_', 'Hon', 'Frie', 'Cent', 'wishe', 'vie', 'toy', 'repeat', 'pter', 'oppo', 'open', 'noticed_', 'murders_', 'ka_', 'harm', 'finish_', 'extreme_', 'eno', 'dying_', 'doo', 'ddle', 'clear', 'cat_', 'bru', 'addict', 'Smith', 'Rod', 'Rem', 'zzle', 'tory', 'starting_', 'specific', 'screaming', 'scenery_', 'psychological_', 'occur', 'obli', 'mn', 'lica', 'laughter', 'inso', 'grad', 'goof', 'gas', 'element', 'dom_', 'dism', 'deals_', 'ctor', 'camp_', 'audi', 'ator_', 'ack', 'Smith_', 'Sh', 'Kenne', 'Holl', 'Dean', 'xious', 'uncom', 'situation', 'shots', 'seem', 'rin', 'pain_', 'originally_', 'number', 'nightmare', 'mystery', 'ml', 'kiss', 'imag', 'iful', 'grew_', 'grade_', 'gge', 'event', 'eate', 'dramati', 'dad', 'condition', 'conce', 'comfort', 'chair', 'aur', 'YOU', 'Red', 'REAL', 'Norma', 'Kir', 'wash', 'upt', 'titi', 'returns_', 'retr', 'restr', 'require', 'relief', 'realise', 'rch', 'rang', 'ple_', 'lus', 'lip', 'intrigue', 'incident', 'iler', 'ha_', 'ground_', 'fores', 'exh', 'dancer', 'anger', 'Wr', 'They', 'Sinatra', 'SI', 'Op', 'Long', 'GI', 'Dem', 'yd', 'week_', 'treatment', 'treat', 'stan', 'slic', 'separate', 'screenplay', 'remarkable_', 'pped_', 'persona', 'mble', 'invi', 'innocen', 'hack', 'gru', 'gma', 'glass', 'forgotten_', 'fem', 'confi', 'clever', 'bone', 'amateur', 'Richard', 'Ray_', 'Please_', 'Kris', 'IM', 'Gordon', 'ED', 'Black', 'wen', 'very', 'ured', 'theater', 'stab', 'redi', 'perce', 'peace', 'passe', 'ops', 'oon', 'morning', 'llow', 'legend', 'irritating', 'hopes_', 'gross', 'genuinely_', 'ech', 'crus', 'bitter', 'acti', 'accura', 'Yu', 'Rome', 'Parker', 'Dia', 'studio', 'still', 'stereotypes', 'serv', 'sequences', 'sequence', 'pres', 'portray_', 'poet', 'opti', 'only', 'ins_', 'impact_', 'emotion_', 'ek_', 'earth', 'dou', 'dislike', 'Sti', 'Reg', 'Philip', 'Bil', 'Att', 'Ash', 'Adam_', 'viol', 'v_', 'uma', 'ultimate_', 'ught', 'trailer_', 'superior_', 'sucked', 'sno', 'service', 'ride', 'por', 'plan', 'mum', 'mme', 'merc', 'lonel', 'guide', 'fici', 'facts', 'evidence', 'doctor', 'discover', 'depend', 'degree', 'cruel', 'counter', 'color_', 'cess', 'cause', 'bro', 'ambitio', 'amaze', 'alternat', 'Wom', 'White_', 'John', 'Bud', 'wound', 'wander', 'typi', 'technology', 'swe', 'standing_', 'reuni', 'organi', 'ngly_', 'minu', 'leas', 'gift', 'executed', 'environment', 'diss', 'demonstrat', 'compani', 'allows_', 'Wayne', 'Kno', 'Instead', 'DA', 'Cart', 'Anthony_', 'unable_', 'uf', 'twin', 'tely', 'sympathetic', 'spoof', 'sis', 'saying', 'rh', 'repr', 'rave', 'promising', 'nch_', 'moo', 'ming', 'liz', 'lighting_', 'lesbian', 'large', 'izing_', 'impos', 'dor', 'disco', 'corny', 'arts_', 'Wars', 'Trac', 'Seve', 'Poli', 'PA', 'Moore', 'LL_', 'Jimmy_', 'Gary_', '?\"', 'zero', 'underw', 'tou', 'spen', 'sheer_', 'scared_', 'rever', 'relationships_', 'proved_', 'predict', 'pia', 'obsc', 'lum', 'learn', 'herself', 'gras', 'finished_', 'continues_', 'brave', 'aris', 'api', 'THIS_', 'Mille', 'Leg', 'First', 'Dis', 'Allen_', 'traditional_', 'statement', 'spir', 'soon', 'rence', 'ran_', 'pros', 'opi', 'mistake_', 'lawyer', 'discovers_', 'deepe', 'ction_', 'cares', 'brutal_', 'brutal', 'breaks_', 'antly', 'accent_', 'Killer', 'Can_', 'Broadway', 'unintentional', 'unbelievable_', 'tte_', 'suspect_', 'strike', 'sens', 'screw', 'rtu', 'pant', 'opens_', 'obsessi', 'mates', 'los', 'logic', 'kit', 'joy_', 'inte', 'iness_', 'han_', 'exact', 'entertained', 'ego', 'dreams_', 'convention', 'collecti', 'chest', 'bling_', 'authentic', 'Then', 'Much_', 'Mot', 'Bette', 'viewers', 'vampire_', 'teach', 'stylis', 'someone', 'sne', 'saved_', 'rule', 'regular_', 'practic', 'ppe', 'pion', 'notice', 'native', 'monsters', 'lo_', 'learned_', 'incon', 'hour', 'hood_', 'feeling', 'embe', 'driving_', 'convincing', 'cav', 'ber_', 'angle', 'absurd', 'Trek', 'Sat', 'Paris_', 'Mol', 'Max', 'Kh', 'Emma', 'Edward', 'Anyone_', '?? ', '17', ' \" ', 'wrap', 'unrealistic', 'tam', 'subtitle', 'spoilers', 'since', 'sexual', 'render', 'remake', 'rely', 'pop_', 'oge', 'oft', 'nett', 'monst', 'law_', 'ional', 'inclu', 'ich', 'ians_', 'hotel_', 'graphic_', 'gonna_', 'gent', 'flashbacks', 'families', 'erin', 'dropp', 'dir', 'bond', 'affair_', 'Scre', 'Dun', 'wide_', 'ttl', 'topic', 'symboli', 'switch', 'solve', 'send', 'rud', 'rem', 'reasons', 'reasonabl', 'pee', 'nar', 'location_', 'ining_', 'gam', 'disappointing_', 'desire_', 'criminal_', 'considera', 'century_', 'celebrat', 'brow', 'area', 'Thin', 'Rec', \"' (\", 'ward_', 'vision_', 'treme', 'surprising_', 'super_', 'risk', 'receive', 'qual', 'pic', 'mee', 'levels', 'kins', 'jack', 'ire_', 'introduc', 'hits_', 'happening_', 'handsome', 'gradua', 'giv', 'garbage', 'forces_', 'finest_', 'easi', 'depressing', 'credits', 'asto', 'Sadly', 'Ple', 'Inc', 'Dick_', 'Alexand', 'wooden_', 'wood_', 'stro', 'steal_', 'soul_', 'reference', 'race', 'quis', 'pir', 'perv', 'obvious', 'majority_', 'lean', 'kes_', 'insti', 'identity', 'everybody_', 'double_', 'dies', 'credit', 'const', 'confe', 'compar', 'centur', 'bloody_', 'Under', 'Twi', 'Sean_', 'Lio', 'Halloween', 'Gal', 'Clu', 'Came', 'Barbara_', '?)', '11_', 'ws', 'ulous', 'subtle', 'substance', 'string', 'shocking_', 'scientist_', 'rian', 'nou', 'multi', 'lf', 'inal', 'harsh', 'handed', 'fir', 'expectations_', 'excited', 'exceptional', 'eva', 'complete', 'comic', 'childhood_', 'ched_', 'adults_', 'Timo', 'Soo', 'Mos', 'Kath', 'Karl', 'Cinderella', 'Christian', 'Age', 'Adam', '!). ', 'zar', 'zan', 'trap', 'trai', 'thin_', 'site_', 'site', 'rich', 'resi', 'reach_', 'quirk', 'patr', 'ony', 'nerv', 'matche', 'inept', 'imagine', 'horri', 'front', 'ford_', 'epic_', 'dat', 'cynic', 'ckin', 'cie', 'caused_', 'brothers_', 'belo', 'appealing', 'West_', 'UK', 'TC', 'Suc', 'Rand', 'Grad', 'Domin', 'Disney', '12_', 'warr', 'vision', 'spoo', 'seeing', 'scenario', 'scale', 'rad', 'ola', 'next', 'necessary_', 'indicat', 'exploitation', 'ened_', 'directing', 'depict', 'curio', 'ciati', 'bullet', 'appre', 'amateurish', 'Yo', 'Watching_', 'Sky', 'Shar', 'Part_', 'Nichol', 'Mars', 'Are_', 'wel', 'visit_', 'unne', 'underrated', 'tedious', 'seconds_', 'rig', 'report', 'reme', 'rar', 'mond_', 'media_', 'lying_', 'las', 'language', 'ised_', 'instant', 'inspiration', 'creates_', 'conflict', 'compose', 'chan', 'cab', 'ava', 'always', 'Water', 'Steven_', 'Pas', 'Nick_', 'Let_', 'Down', 'yth', 'victims_', 'theaters', 'seasons', 'sai', 'rising', 'recr', 'plann', 'pent', 'painfully_', 'ot_', 'occu', 'nob', 'moti', 'lem', 'lati', 'gua', 'fights_', 'event_', 'elev', 'discovered_', 'cs', 'cliché_', 'cance', 'bik', 'bigger_', 'backs', 'atic', 'Shan', 'Sab', 'Poi', 'Hitchcock', 'GR', 'Francis', 'Det', 'Care', 'Anderson', 'veteran', 'ution_', 'theless', 'sports', 'slave', 'ses', 'revi', 'refreshing', 'quar', 'provok', 'premise', 'paper', 'nty', 'norm', 'mood', 'menac', 'loud', 'loose', 'letter', 'investigati', 'introduce', 'holes_', 'gan_', 'fund', 'ents_', 'drunk', 'disgusting', 'dio', 'confusing_', 'cky', 'baby', 'THE', 'Nancy', 'Kate_', 'Gia', 'Carol', 'Cand', \"'.\", 'western', 'unf', 'struc', 'strong', 'search', 'sav', 'ries_', 'resemble', 'rental', 'raci', 'producer', 'nic_', 'news_', 'memor', 'many', 'magical', 'format', 'equal', 'decl', 'curs', 'ction', 'convict', 'contrived', 'capable_', 'bringing_', 'boyfriend_', 'bli', 'anybody_', 'animal_', 'advertis', 'Music', 'Jun', 'Jones', 'Greg', 'Fra', 'Donald_', 'Dark', '1930', 'é_', 'yc', 'urne', 'tire', 'step', 'scr', 'reporter', 'position', 'okay', 'nted_', 'misse', 'logical', 'ient', 'identif', 'feet', 'fail_', 'creat', 'content_', 'contemp', 'concei', 'border', 'ask', 'actual', 'Way', 'Plus', 'Mill', 'Foo', 'Dy', 'Bec', ' ,', 'utter_', 'urban', 'struggle', 'sign_', 'sher', 'seduc', 'scientist', 'saw', 'released', 'received_', 'lity_', 'jump_', 'island_', 'ignor', 'ick', 'horrifi', 'hange', 'handled', 'endea', 'dil', 'ative', 'angry_', 'ages_', 'accus', 'Writ', 'Without_', 'Wall', 'Thank', 'Sla', 'Qua', 'Page', 'ND', 'Lost', 'Fish', 'Eric_', 'Does', 'Clau', 'Cel', 'Camp', 'Australian', 'Arn', 'Ann_', 'Ala', 'Actually', \".' \", \",' \", 'wall_', 'thoughts', 'somebody_', 'round', 'proud', 'oy', 'overly_', 'opera_', 'offensive', 'myth', 'murderer', 'mpt', 'ivi', 'ir_', 'iga', 'iar', 'holi', 'hearted_', 'gath', 'fictional', 'expectation', 'etta', 'enco', 'ence', 'deserved_', 'depiction', 'dece', 'comedian', 'bles', 'aside_', 'ambi', 'ake', 'Wonder', 'Why', 'Through', 'Overall_', 'Off', 'OI', 'More_', 'Jennifer_', 'Gill', 'Germany', 'Douglas_', 'Cy', 'CGI_', '\").', 'walks_', 'ury', 'three', 'thank_', 'surp', 'soph', 'sed', 'satisfying', 'rebel', 'pure', 'practically_', 'minds', 'manage', 'lp', 'learns_', 'isl', 'involves_', 'impro', 'impa', 'icon', 'hyp', 'fortune', 'erm', 'cuts_', 'copi', 'conclusion_', 'ced_', 'captured_', 'bble', 'arro', 'Wei', 'Sis', 'Pin', 'Marg', 'Life', 'Laur', 'Later', 'Hop', 'Eva', 'Blue', 'Barry', 'Baby', 'whilst_', 'unfa', 'twi', 'test_', 'ters', 'stric', 'streets', 'stom', 'spoil', 'relative', 'relate_', 'recommend', 'ology', 'middle', 'laughable', 'jea', 'genuine_', 'gat', 'frustrati', 'forth', 'excitement', 'costs', 'cord', 'compo', 'bright_', 'bank', 'aka', 'WE', 'Ten', 'THAT', 'Pur', 'Pitt', 'Mike_', 'Hum', 'Being_', 'veri', 'turi', 'tun', 'tel', 'task', 'sting', 'six', 'sentimental', 'quit', 'pleasure_', 'pity', 'personality_', 'motivation', 'moder', 'miserabl', 'mirror', 'manner_', 'logi', 'ein', 'eful', 'dubbed', 'discussi', 'ders', 'defeat', 'dangerous_', 'cry_', 'clos', 'cial_', 'chor', 'Wat', 'Wan', 'Spanish_', 'Have', 'Guy', 'Game', '. . ', 'winner', 'welcome', 'unexp', 'ture', 'tall', 'tal', 'stoo', 'smo', 'serious', 'rc', 'phi', 'outrage', 'oh', 'national_', 'mber_', 'mba', 'loser', 'lee', 'largely_', 'involve', 'ico', 'garbage_', 'found', 'even', 'distinct', 'design_', 'cure', 'consu', 'circumstances', 'calls_', 'blown_', 'attract', 'anime', 'Zi', 'Vietnam', 'Ryan', 'ON_', 'NY', 'Lady_', 'La_', 'Flor', 'Bern', 'AI', ' )', 'unk', 'unh', 'ugly_', 'tine', 'spre', 'simpli', 'significant', 'sequels', 'remembered_', 'reache', 'plat', 'obsessed_', 'ncy_', 'mysteri', 'mous', 'mbs', 'lover_', 'lights', 'lad', 'industr', 'ible', 'grown_', 'general', 'fru', 'explosion', 'exception', 'ese', 'endur', 'domina', 'dera', 'cies', 'built_', 'barr', 'Tod', 'Ran', 'Maria', 'Grand', 'Dee', 'Aw', ' />**', 'xo', 'voices', 'visually', 'ui', 'twice_', 'tend_', 'spor', 'solut', 'slap', 'scien', 'robbe', 'redibl', 'prot', 'prevent', 'ood', 'kee', 'issue_', 'ironic', 'iron', 'investigat', 'intr', 'hl', 'gus', 'food_', 'enl', 'dl', 'described_', 'complaint', 'careful', 'apartment_', 'alcohol', 'aid', 'acy', 'Year', 'Vis', 'Vir', 'Tow', 'Fly', 'Dream', 'Award', '*****', 'vague', 'strat', 'reviewers_', 'offend', 'locat', 'iu', 'ital', 'iev', 'hospital_', 'fou', 'financ', 'filmmaker_', 'farm', 'evening', 'essentially_', 'energy_', 'ef_', 'complex', 'competi', 'ching', 'bal_', 'ax', 'ances', 'acted', 'ace_', 'Story', 'LD', 'Inde', 'Hope', 'Duk', 'Dian', 'Bob', 'Back', 'Any_', 'About_', ' ...', 'yard', 'whenever_', 'wake', 'ures_', 'unse', 'trust_', 'treat_', 'teenager', 'stock_', 'rri', 'rise_', 'rant', 'pupp', 'pte', 'pes', 'overd', 'operati', 'occasional', 'nicely_', 'nical', 'liners', 'impo', 'holding_', 'engaging_', 'diver', 'distribut', 'dim', 'delightful_', 'crappy_', 'cook', 'connection_', 'cohe', 'bore', 'Vincen', 'Susan', 'Rep', 'Powell', 'Oliver', 'Neil', 'Murphy', 'Mic', 'Indi', 'Ele', 'Bru', 'Beaut', '. *', ' />*', 'zation', 'urge', 'urag', 'teenagers', 'seven_', 'river', 'prep', 'nail', 'mble_', 'matters', 'loose_', 'iva', 'issue', 'intriguing_', 'ili', 'god_', 'glimpse', 'ently', 'els_', 'een_', 'develop_', 'desire', 'cops_', 'contra', 'buil', 'broke', 'ater', 'asleep', 'adventur', 'Williams_', 'Wend', 'None_', 'Mod', 'House', 'Horror_', 'Anim', '192', 'ughter', 'trial', 'soap_', 'severe', 'road', 'poster', 'portraying_', 'phr', 'pathetic', 'overlook', 'moving', 'month', 'lau', 'lacking_', 'knowledge_', 'kidnapp', 'interpretation', 'industry_', 'hurt', 'heavi', 'genius', 'false', 'existent', 'execution', 'drop', 'difference', 'determine', 'detail_', 'dent', 'cutting', 'combin', 'comb', 'cket', 'chron', 'capital', 'bodies', 'bic', 'believes_', 'area_', 'angles', 'Ted', 'Sop', 'End', 'Dre', 'Dick', 'Ak', 'Africa', ' ? ', 'vol', 'system', 'steps', 'situations', 'sexuality', 'sets', 'ripp', 'revel', 'rel', 'realiz', 'private', 'paper_', 'notch', 'nge_', 'mistr', 'merit', 'mbl', 'match', 'losing_', 'lme', 'interacti', 'indeed', 'ifica', 'henc', 'heaven', 'fro', 'fon', 'femin', 'faces_', 'enh', 'driven_', 'dressed_', 'dne', 'decen', 'ctic', 'coming', 'club_', 'castle', 'captures_', 'building', 'atic_', 'athe', 'assassin', 'army_', 'alien_', 'abso', 'Tho', 'Scr', 'Prob', 'Para', 'Gor', 'Eg', 'Com', 'City', 'At', 'Apparently', ' / ', 'ule', 'ue_', 'tograph', 'thirt', 'thank', 'suit_', 'suffering_', 'sight_', 'sey', 'screenwriter', 'rell', 'ppet', 'passed_', 'pacing_', 'normally_', 'mill', 'lyn', 'ition', 'gers', 'football', 'faithful', 'expose', 'expos', 'emerge', 'ell_', 'depicted', 'crude', 'criticism', 'combination_', 'claim_', 'carr', 'bt', 'brilliantly_', 'boss', 'analy', 'ame', 'Ray', 'Pic', 'Lord_', 'Kill', 'Fea', 'Evil', 'Bos', 'BS', 'AB', '\" - ', ' :', 'tta', 'trailer', 'soli', 'rum', 'revolve', 'ressi', 'quiet_', 'portrays_', 'populat', 'plant', 'oin', 'occasionally_', 'nost', 'nau', 'mun', 'lb', 'ipat', 'hysteri', 'grow_', 'gag', 'fus', 'foot_', 'finger', 'figur', 'esp', 'equi', 'ener', 'dec', 'chain', 'broken_', 'agent', 'actions_', 'aa', 'Russell', 'Indian', 'Heav', 'Daniel_', 'Ast', ' /> ', 'zard', 'unlikely', 'ump', 'tele', 'teacher_', 'subplot', 'rub', 'rte', 'rly_', 'radio_', 'quir', 'pair_', 'ordinary_', 'oppos', 'nsi', 'mouth_', 'maintain', 'lve', 'loc', 'inventi', 'inexp', 'imitat', 'generate', 'gal_', 'frightening', 'frig', 'foreign_', 'filmmaker', 'excess', 'elle', 'creator', 'count_', 'controvers', 'cliche', 'casti', 'bet_', 'aking_', 'acqu', 'Three', 'Texas', 'Tarzan_', 'Earth_', 'Dan_', 'Besides', 'yw', 'woods_', 'wan', 'vest', 'uous', 'unit', 'therefore_', 'tears_', 'surface', 'steals_', 'sni', 'shut', 'roman', 'roll_', 'rele', 'reaction', 'qualities', 'proper_', 'profession', 'photo', 'months_', 'mem', 'makeup', 'longe', 'lam', 'ix', 'insist', 'inher', 'fying_', 'forgettable', 'faced', 'expens', 'enthusias', 'describ', 'cry', 'commentary_', 'collection_', 'civili', 'category', 'cam', 'believed', 'ancient_', 'Walter_', 'Sum', 'Sometimes', 'Sel', 'Lou', 'Kn', 'Joseph_', 'Gro', 'Fon', 'Columbo', 'system_', 'student', 'shocked', 'sell_', 'ridi', 'prior', 'primar', 'mon_', 'mmer', 'lish', 'higher_', 'fatal', 'employe', 'dirty', 'cris', 'conf', 'ckle', 'blend', 'bility_', 'baseball', 'awake', 'arr', 'ape', 'alive_', 'Wid', 'Santa_', 'Kei', 'Dep', 'Burn', 'Bob_', '´', 'warn', 'unknown_', 'twenty_', 'touches', 'supernatural', 'sitcom', 'saving_', 'rupt', 'relatively_', 'possibilit', 'nose', 'mes_', 'massive', 'male', 'ied', 'honor', 'heroes_', 'gig', 'gangs', 'divi', 'diat', 'consequen', 'classics', 'cases', 'bug', 'brief', 'bott', 'assume_', 'associate', 'assistan', 'arra', 'aria', 'absen', 'VHS_', 'Steve', 'Port', 'Paris', 'Old_', 'Morgan_', 'Horr', 'High_', 'General', 'Din', 'Dark_', 'Colo', 'Avoid_', 'zel', 'unnecessary_', 'unexpected_', 'tragedy_', 'tim', 'stle', 'stereo', 'stai', 'send_', 'recommended_', 'produce', 'pregnan', 'noon', 'move', 'ludicrous', 'lude', 'length', 'ident_', 'ide_', 'grue', 'focused', 'extraordinar', 'desperate', 'depress', 'dai', 'creature_', 'covered_', 'chief', 'boss_', 'asking_', 'Yeah', 'WW', 'Rid', 'Island', 'FA', 'Denn', 'Ch', 'Basically', 'Ang', 'Ami', '?! ', '): ', 'virtually_', 'underg', 'truck', 'training', 'tif', 'surf', 'rmin', 'reject', 'rante', 'plots_', 'placed_', 'ni_', 'mature', 'lousy_', 'justice_', 'io_', 'glori', 'gentle', 'fly_', 'explanation_', 'execut', 'exaggerat', 'events', 'elie', 'destructi', 'choose_', 'characteriz', 'char', 'cent_', 'books', 'bby', 'appreciated', 'allo', 'Neve', 'Nee', 'Jackson_', 'Irish', 'IN_', 'During_', 'Devil', 'Count', 'yes_', 'user', 'unpr', 'tual', 'treasure', 'stronge', 'sorr', 'ruined_', 'reputation', 'rently', 'related', 'quel', 'produce_', 'presum', 'politics', 'plans', 'painting', 'killers', 'initial_', 'impli', 'ify', 'hooke', 'funnie', 'fad', 'empty_', 'driver', 'di_', 'detect', 'designed', 'deserve', 'believ', 'awesome', 'accents', 'Your', 'Thank_', 'RE_', 'Pacino', 'Movies', 'Jay', 'IMDb', 'Hugh', 'Festival', 'Enter', 'Donn', 'Christi', 'Alm', 'Academy_', '000_', 'ycl', 'vivi', 'upset', 'ups_', 'unp', 'tiny', 'surprises', 'study_', 'strongly_', 'speaks', 'size', 'riv', 'relation', 'quee', 'py', 'never', 'mainstream', 'libera', 'latest', 'ising', 'insu', 'icia', 'hurt_', 'freedom', 'estl', 'emotionally_', 'dust', 'desc', 'convinced_', 'compell', 'cock', 'clothes_', 'cameo_', 'blind_', 'besides', 'attacke', 'Victor_', 'Return', 'Poo', 'Never_', 'Nel', 'Hey', 'Caine', 'Brando', 'ually_', 'tive', 'silen', 'rew', 'quate', 'preach', 'ological', 'nude', 'multiple', 'link', 'lge', 'ledge', 'laz', 'integr', 'hn', 'hie', 'folks_', 'experiences', 'emphasi', 'earlier', 'delivered_', 'deco', 'deaths', 'continuity', 'complicate', 'burne', 'boyfriend', 'awkward_', 'atrocious', 'amuse', 'ack_', 'Wilson', 'Turn', 'Robin_', 'Pr', 'Om', 'Mun', 'Meanwhile', 'Jessi', 'Jess', 'Jenn', 'Gand', 'Et', 'Canadian_', 'Brothers', 'Bake', 'Ah', '1990', 'wreck', 'unif', 'toi', 'teens', 'smart', 'shir', 'serves_', 'sati', 'rix', 'remain_', 'pub', 'propaganda', 'players_', 'plas', 'ping', 'overcom', 'orious', 'minde', 'meeting_', 'lph', 'loyal', 'lm', 'llin', 'lake', 'kar', 'istic', 'instru', 'included_', 'hire', 'graph', 'gory_', 'favour', 'elde', 'dum', 'destroy_', 'destin', 'denti', 'consistent', 'cameo', 'betr', 'arrest', 'appea', 'animal', 'amen', 'accidentally', 'acce', 'Silv', 'Saturday_', 'ST_', 'Res', 'MGM', 'Korea', 'Fam', 'Asian_', 'Alle', 'zu', 'weeks', 'ticke', 'terrifi', 'table_', 'storytell', 'stopped_', 'steal', 'slash', 'shoe', 'select', 'rocke', 'roa', 'record_', 'previously', 'participa', 'okay_', 'ogr', 'official', 'nke', 'mistakes', 'misca', 'memorabl', 'logue', 'itat', 'ists_', 'intelligence_', 'ien', 'greate', 'ggy', 'gangster_', 'critical', 'closer', 'cartoons', 'boot', 'accepta', 'abu', 'TER', 'States', 'Roberts', 'LER', 'Jones_', 'Hat', 'Eri', 'Eliza', 'Coop', 'wes', 'uninteresting', 'tense', 'teet', 'suffers_', 'stranger', 'station_', 'scu', 'resid', 'rand', 'popula', 'ours', 'opene', 'occurr', 'non_', 'nominated_', 'mol', 'missi', 'memory_', 'memories_', 'maid', 'intri', 'inju', 'inevitabl', 'humans_', 'hanging_', 'gratuitous_', 'gas_', 'forme', 'direct', 'difficult', 'department', 'damag', 'creatures', 'cif', 'Warner', 'Titan', 'Matt_', 'Larr', 'KI', 'Hor', 'Holm', 'Fair', 'Drew', 'Andr', '1960', 'wri', 'vely', 'uls', 'travel_', 'trat', 'transf', 'timi', 'suspen', 'struggling', 'spoil_', 'slaps', 'sink', 'reti', 'reaction_', 'quest_', 'pilot_', 'narration', 'invite', 'hearing_', 'gm', 'gai', 'full', 'frankly', 'fairy', 'expe', 'dimension', 'dent_', 'deme', 'contest', 'conscious', 'cked', 'below_', 'ations', 'angel', 'alive', 'absurd_', 'Wer', 'Tha', 'Stewar', 'Play', 'Picture', 'Part', 'Martin', 'Franc', 'Fir', 'Fas', 'Ev', 'Cos', 'Carre', 'Bog', 'BU', 'Anne_', 'yan', 'writ', 'vit', 'vai', 'summ', 'ston', 'stin', 'stif', 'sensitive', 'rules', 'provided_', 'prostitut', 'pretentious_', 'poignan', 'pai', 'paced_', 'offi', 'nds_', 'mig', 'laughable_', 'instal', 'inati', 'forget', 'eit', 'defend', 'conse', 'beaut', 'Spr', 'Rol', 'Our_', 'NOT', 'Lugosi', 'Luci', 'Las', 'Imp', 'Ic', 'Earl', 'Davis_', 'Cod', '!)', 'twiste', 'sincer', 'sacrifice', 'references_', 'range_', 'purchase', 'orn', 'noise', 'neo', 'mecha', 'lun', 'insult_', 'fully', 'flicks_', 'fair', 'endless_', 'eeri', 'devot', 'curious_', 'comical', 'beth_', 'begin', 'aura', 'ase_', 'ach_', 'Sullivan', 'St', 'Sarah', 'London', 'Liv', 'Kee', 'Jackie_', 'Hong', 'Emil', 'Clair', 'China', 'California', 'Atlant', 'Alice', '\"?', '!!!!!!', 'xico', 'wick', 'visi', 'viewed_', 'uish', 'tribu', 'theatrical_', 'talks_', 'smile_', 'seven', 'reminisce', 'relie', 'rci', 'rah', 'pleasant_', 'plague', 'picio', 'ounce', 'murdered_', 'mul', 'mous_', 'mock', 'mira', 'mete', 'loss_', 'initia', 'iest_', 'health', 'harde', 'gran', 'goal', 'ghe', 'fy', 'fix', 'experienced', 'edy', 'deci', 'conflict_', 'compe', 'committed', 'cele', 'brick', 'bour', 'bers', 'berate', 'artist_', 'anth', 'Woody_', 'WWI', 'V_', 'TT', 'Sunday', 'Story_', 'Rob_', 'Rachel', 'Nin', 'Gree', 'Friday', 'Dev', 'Bros', 'Brana', ' : ', 'wha', 'vig', 'views', 'unconvincing', 'smi', 'sibl', 'quen', 'pointless', 'perp', 'particular', 'overwhelm', 'offered', 'nominat', 'naturally', 'locke', 'left', 'lady', 'ilt', 'iel', 'ication', 'historic', 'haunting', 'gem_', 'figures', 'figured_', 'evol', 'ery', 'eco', 'dynami', 'duct', 'doi', 'description', 'cultural', 'contrac', 'confide', 'combined', 'coin', 'cke', 'chosen_', 'amed', 'agon', 'Thomas_', 'THI', 'Nation', 'MOVIE', 'Lev', 'Jeff', 'Hoffman', 'Glen', 'Even', '1st_', ' ! ', 'yu', 'trappe', 'thir', 'tension', 'tail', 'table', 'split', 'sides', 'settle', 'schem', 'save', 'ruc', 'prime', 'posit', 'painte', 'ndi', 'marry_', 'kun', 'killing', 'isol', 'iot', 'intend', 'impres', 'horribly_', 'hing', 'heroi', 'gle_', 'fri', 'fitt', 'fighter', 'estin', 'ee_', 'drunk_', 'directly', 'dinos', 'chose_', 'changing', 'blonde_', 'benefi', 'award_', 'av', 'aki', 'ages', 'acter', 'VERY_', 'Ur', 'Tel', 'Superman_', 'Real', 'Phi', 'Palm', 'Nicol', 'Johnson', 'Jesus_', 'J_', 'Hes', 'Helen', 'Fun', 'Fle', 'Dir', 'Chap', 'vag', 'uncon', 'ues', 'types_', 'tical', 'sprin', 'sorts', 'securi', 'previ', 'porno', 'party', 'pare', 'method', 'medica', 'mber', 'landscape', 'jor', 'jail', 'imper', 'hunter', 'happening', 'gritty', 'gain_', 'flaws_', 'fak', 'extra', 'edited_', 'ecc', 'dragg', 'chie', 'cant_', 'breast', 'authorit', 'ated', 'ality', 'advise', 'advan', 'according_', 'Wors', 'Unlike', 'United_', 'Simon_', 'Riv', 'Pea', 'Michell', 'Exp', 'Child', 'Cham', 'Bourne', 'Basi', 'widow', 'walked_', 'upp', 'unforg', 'uld_', 'tting', 'till_', 'thy_', 'talents_', 'suspenseful', 'summer_', 'storm', 'screening', 'scare_', 'realizes_', 'rce', 'raw', 'qu', 'ngl', 'magic', 'lac', 'jobs', 'ister_', 'inti', 'inha', 'ill_', 'hands', 'grin', 'forward', 'examin', 'equent', 'emi', 'contact', 'concentrat', 'compu', 'competen', 'biograph', 'attach', 'amus', 'alik', 'activi', 'William', 'Myst', 'Luke_', 'Live', 'Life_', '15', 'zes', 'werewolf', 'warne', 'uring_', 'trilogy', 'swim', 'stumble', 'spite', 'spends_', 'sleep_', 'sist', 'sentence', 'rma', 'reward', 'reviewer_', 'pul', 'preten', 'performed', 'passing', 'par_', 'oph', 'livi', 'kinds_', 'journal', 'isticat', 'inva', 'idi', 'ham_', 'fte', 'few', 'featured', 'ern_', 'eag', 'dollars', 'disb', 'depth', 'cryin', 'cross_', 'content', 'contemporary_', 'colors', 'chee', 'because', 'asy', 'agent_', 'Willi', 'Warr', 'Ven', 'Vamp', 'Roch', 'ONE', 'Movie', 'Mau', 'Mass', 'MST', 'Hin', 'Hear', 'Gue', 'Gl', 'Freddy_', 'Definite', 'Captain_', 'BBC', '??? ', '80s_', '\"), ', 'wol', 'weekend', 'vampires', 'underst', 'tial_', 'terrorist', 'strength_', 'starre', 'soldier_', 'snow', 'sity', 'ruin_', 'retar', 'resu', 'required', 'recommended', 'ques', 'propo', 'presents_', 'perm', 'overt', 'olds', 'occas', 'nn_', 'nen', 'nei', 'mail', 'lost', 'lion', 'libr', 'inner_', 'headed', 'happy', 'guest', 'govern', 'friendly', 'explains', 'ens_', 'effectively', 'draw_', 'downright', 'dete', 'dde', 'dare', 'cring', 'courag', 'conspi', 'comedie', 'claims_', 'cide', 'chas', 'captivat', 'bite', 'bare', 'author_', 'addition', 'Vid', 'Rh', 'Oliv', 'Nata', 'Mexican', 'Keaton_', 'Iron', 'Barb', 'ALL_', '12', '!), ', 'worthwhile', 'weake', 'ung', 'understood_', 'unbelievable', 'superf', 'stolen', 'stereotypic', 'spoiler', 'sight', 'scares', 'rut', 'remove', 'remotely_', 'releva', 'prese', 'poke', 'ndou', 'mbla', 'lucky_', 'lling_', 'legendary', 'imagery', 'humou', 'hug', 'hired', 'heck', 'guilty', 'extras', 'expected', 'everywhere', 'dry_', 'drea', 'directed', 'dimensional_', 'ddi', 'dden', 'communica', 'cham', 'buddy', 'bank_', 'azi', 'algi', 'adventures', 'accurate_', 'accompan', 'Thom', 'Still_', 'Someone', 'Serious', 'SU', 'Phill', 'Perso', 'Patrick_', 'Lei', 'Jus', 'Gho', 'Get_', 'Freeman', 'Especially_', '?).', '...\"']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "DEFJaqyrkkKB",
    "outputId": "aa6f0a5b-2b7a-4735-90ae-af3fe2846b25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's just say it in simple words so that even the makers of this film might have a chance to understand: This is a very dumb film with an even dumber script, lame animation, and a story that's about as original as thumbtacks. Don't bother -- unless you need to find some way to entertain a group of mentally retarded adults or extremely slow children. They might laugh, especially if they're off their meds. There's a special kind of insult in a film this ridiculous -- not only do the filmmakers apparently think that children are brainless idiots who can be entertained with claptrap that cost approximately zero effort, but they don't even bother to break a sweat inserting a gag here and there that an adult might find amusing. This film, frankly, ticked me off royally. Shame on you for stooping so low.\n"
     ]
    }
   ],
   "source": [
    "print(train_df['sentence'][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "BIoJC3XgkjbT",
    "outputId": "9337867f-34f0-4db1-b0f2-1f19640b1b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [2601, 7968, 8, 56, 202, 15, 11, 1392, 2152, 55, 13, 78, 1, 2428, 6, 14, 32, 290, 31, 4, 1258, 7, 2511, 106, 62, 9, 4, 67, 2709, 32, 22, 41, 78, 3754, 125, 602, 2, 2395, 4753, 2, 5, 4, 98, 142, 7968, 8, 60, 20, 314, 20, 2521, 2915, 5248, 8044, 3, 644, 7968, 21, 3289, 522, 1995, 37, 459, 7, 210, 63, 141, 7, 5105, 7961, 4, 1038, 6, 5094, 64, 7812, 1283, 5894, 50, 841, 1347, 1515, 3, 275, 290, 1286, 2, 358, 75, 466, 7968, 182, 177, 79, 220, 1761, 3, 608, 7968, 8, 4, 451, 312, 6, 7432, 11, 4, 32, 14, 2049, 522, 33, 77, 110, 1, 2366, 1605, 129, 13, 1133, 29, 2835, 200, 2403, 8, 46, 83, 35, 5691, 7961, 22, 1578, 2418, 4032, 7961, 13, 4534, 7961, 2465, 423, 1381, 4719, 64, 5622, 7961, 3850, 2, 26, 53, 109, 7968, 21, 78, 4172, 7, 2243, 4, 4214, 21, 4503, 3090, 154, 4, 6766, 7961, 307, 5, 91, 13, 41, 2881, 290, 210, 7725, 100, 3, 62, 66, 2, 7351, 2, 7239, 40, 105, 177, 423, 849, 2503, 3, 998, 105, 25, 37, 23, 6356, 1157, 55, 863, 7975]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sample question: {}'.format(tokenizer.encode(train_df['sentence'][20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "DyiHNbMDknRw",
    "outputId": "14359ba3-b4ce-400d-8acd-f975ba132134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [135, 7968, 8, 965, 7974, 2405, 34, 7, 105, 13, 14, 32, 18, 78, 677, 7975]\n",
      "기존 문장: It's mind-blowing to me that this film was even made.\n"
     ]
    }
   ],
   "source": [
    "# train_df에 존재하는 문장 중 일부를 발췌\n",
    "sample_string = \"It's mind-blowing to me that this film was even made.\"\n",
    "\n",
    "# encode\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# encoding한 문장을 다시 decode\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "B-nT9h7cko5O",
    "outputId": "a9204066-5b8a-4145-f400-d0746205193f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기(Vocab size) : 8185\n"
     ]
    }
   ],
   "source": [
    "print('단어 집합의 크기(Vocab size) :', tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "phY_ElwhkqBu",
    "outputId": "5defb3f4-5954-46d3-b9e1-864f91776f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 ----> It\n",
      "7968 ----> '\n",
      "8 ----> s \n",
      "965 ----> mind\n",
      "7974 ----> -\n",
      "2405 ----> blow\n",
      "34 ----> ing \n",
      "7 ----> to \n",
      "105 ----> me \n",
      "13 ----> that \n",
      "14 ----> this \n",
      "32 ----> film \n",
      "18 ----> was \n",
      "78 ----> even \n",
      "677 ----> made\n",
      "7975 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "UReHANuokrWE",
    "outputId": "79417752-8ec7-401b-d2da-e9747fbed874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [135, 7968, 8, 965, 7974, 2405, 34, 7, 105, 13, 14, 32, 18, 6373, 8049, 8050, 990, 677, 7975]\n",
      "기존 문장: It's mind-blowing to me that this film was evenxyz made.\n"
     ]
    }
   ],
   "source": [
    "# 앞서 실습한 문장에 even 뒤에 임의로 xyz 추가\n",
    "sample_string = \"It's mind-blowing to me that this film was evenxyz made.\"\n",
    "\n",
    "# encode\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# encoding한 문장을 다시 decode\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "0Aed7I00kxco",
    "outputId": "aa9f296d-442b-4cf3-d927-5ce802d31255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 ----> It\n",
      "7968 ----> '\n",
      "8 ----> s \n",
      "965 ----> mind\n",
      "7974 ----> -\n",
      "2405 ----> blow\n",
      "34 ----> ing \n",
      "7 ----> to \n",
      "105 ----> me \n",
      "13 ----> that \n",
      "14 ----> this \n",
      "32 ----> film \n",
      "18 ----> was \n",
      "6373 ----> even\n",
      "8049 ----> x\n",
      "8050 ----> y\n",
      "990 ----> z \n",
      "677 ----> made\n",
      "7975 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sFMLf6llrmQ"
   },
   "source": [
    "### 네이버 영화 리뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "iyWnKsxukyjr",
    "outputId": "ebb8808c-23d9-4ad6-bb2b-c4fb30399519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings_test.txt', <http.client.HTTPMessage at 0x7fe80cc83390>)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMlQZPMRk0aA"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "YiXtApn1k2HP",
    "outputId": "5237f20d-cf54-43d4-be21-6417e87ae743"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5] # 상위 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Q-cn29-Xk3KX",
    "outputId": "e524c815-43d0-448c-b96e-8290d9cbb6b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "bMCyREqHk4KQ",
    "outputId": "ba68e597-173a-4f54-ce8d-d578df945434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "document    5\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "WxYZRp4ek5L7",
    "outputId": "25402f5c-4c04-45d3-d4dc-a90c47273bda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPEVl90ak6Hv"
   },
   "outputs": [],
   "source": [
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    train_data['document'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "OuASxRDwk7K5",
    "outputId": "294cbf0a-c254-4b7c-ebdf-02902db19a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['. ', '..', '영화', '이_', '...', '의_', '는_', '도_', '다', ', ', '을_', '고_', '은_', '가_', '에_', '.. ', '한_', '너무_', '정말_', '를_', '고', '게_', '영화_', '지', '... ', '진짜_', '이', '다_', '요', '만_', '? ', '과_', '나', '가', '서_', '지_', '로_', '으로_', '아', '어', '....', '음', '한', '수_', '와_', '도', '네', '그냥_', '나_', '더_', '왜_', '이런_', '면_', '기', '하고_', '보고_', '하는_', '서', '좀_', '리', '자', '스', '안', '! ', '에서_', '영화를_', '미', 'ㅋㅋ', '네요', '시', '주', '라', '는', '오', '없는_', '에', '해', '사', '!!', '영화는_', '마', '잘_', '수', '영화가_', '만', '본_', '로', '그_', '지만_', '대', '은', '비', '의', '일', '개', '있는_', '없다', '함', '구', '하']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rNjGuG1jk8F6",
    "outputId": "7c335bbe-a0e4-43f8-a014-532c588c62cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [669, 4700, 17, 1749, 8, 96, 131, 1, 48, 2239, 4, 7466, 32, 1274, 2655, 7, 80, 749, 1254]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "CaEnC4YHk9NW",
    "outputId": "d3c08d86-76ef-4721-d663-dd8c1f2a4a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [570, 892, 36, 584, 159, 7091, 201]\n",
      "기존 문장: 보면서 웃지 않는 건 불가능하다\n"
     ]
    }
   ],
   "source": [
    "sample_string = train_data['document'][21]\n",
    "\n",
    "# encode\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# encoding한 문장을 다시 decode\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "FCJwUmp_k-Qi",
    "outputId": "20978f8e-e7c3-42a8-b04a-4f7256cd7ac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570 ----> 보면서 \n",
      "892 ----> 웃\n",
      "36 ----> 지 \n",
      "584 ----> 않는 \n",
      "159 ----> 건 \n",
      "7091 ----> 불가능\n",
      "201 ----> 하다\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "6EY_tvWJk_hL",
    "outputId": "ac5005af-23c0-461b-ae80-9f36bdeab9a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [4, 23, 1364, 2157, 8235, 8128, 8130, 8235, 8147, 8169, 8235, 8147, 8169, 393]\n",
      "기존 문장: 이 영화 굉장히 재밌다 킄핫핫ㅎ\n"
     ]
    }
   ],
   "source": [
    "sample_string = '이 영화 굉장히 재밌다 킄핫핫ㅎ'\n",
    "\n",
    "# encode\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# encoding한 문장을 다시 decode\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "cS_6IhT_lA12",
    "outputId": "d230aeec-8913-416f-dd25-46dfbed50b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ----> 이 \n",
      "23 ----> 영화 \n",
      "1364 ----> 굉장히 \n",
      "2157 ----> 재밌다 \n",
      "8235 ----> �\n",
      "8128 ----> �\n",
      "8130 ----> �\n",
      "8235 ----> �\n",
      "8147 ----> �\n",
      "8169 ----> �\n",
      "8235 ----> �\n",
      "8147 ----> �\n",
      "8169 ----> �\n",
      "393 ----> ㅎ\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAqD_jqMh3dv"
   },
   "source": [
    "## Sentencepiece (구글의 BPE 구현체)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "WLeHwrl9h53R",
    "outputId": "985b5f63-6ec3-4d9c-c05b-d372b7c7baa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 3.5MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.91\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd-IUT3Pfw7_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2AmE_wfsuVF"
   },
   "source": [
    "Sentencepiece의 학습 데이터로는 빈 칸이 포함되지 않은 문서 집합이어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVb9nZWV0f3i"
   },
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkXpbkaQ0hKp"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get('https://github.com/euphoris/datasets/raw/master/imdb.zip')\n",
    "with open('imdb.zip', 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9F_I3csz0sdi"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('imdb.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmhCRd330vWX"
   },
   "outputs": [],
   "source": [
    "with open('review.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(df['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nH9CSKad0xQr"
   },
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train('--input=review.txt --model_prefix=imdb --vocab_size=1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFL2xlUb08gl"
   },
   "source": [
    "SentencePieceTrainer를 이용해 토큰 학습\n",
    "\n",
    "input : 학습시킬 파일  \n",
    "model_prefix : 만들어질 모델 이름  \n",
    "vocab_size : 단어 집합의 크기  \n",
    "model_type : 사용할 모델 (unigram(default), bpe, char, word)  \n",
    "max_sentence_length: 문장의 최대 길이  \n",
    "pad_id, pad_piece: pad token id, 값  \n",
    "unk_id, unk_piece: unknown token id, 값  \n",
    "bos_id, bos_piece: begin of sentence token id, 값  \n",
    "eos_id, eos_piece: end of sequence token id, 값  \n",
    "user_defined_symbols: 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjM5coBM0zGQ"
   },
   "outputs": [],
   "source": [
    "tokens = pd.read_csv('imdb.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "BUALlx_C0-nN",
    "outputId": "4eca36f5-b832-4abb-f969-983fb1c6345e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s</td>\n",
       "      <td>-3.23395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.35727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>▁</td>\n",
       "      <td>-3.45567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>▁the</td>\n",
       "      <td>-3.64478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.71767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>▁a</td>\n",
       "      <td>-3.87010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t</td>\n",
       "      <td>-4.02045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1\n",
       "0  <unk>  0.00000\n",
       "1    <s>  0.00000\n",
       "2   </s>  0.00000\n",
       "3      s -3.23395\n",
       "4      . -3.35727\n",
       "5      ▁ -3.45567\n",
       "6   ▁the -3.64478\n",
       "7      , -3.71767\n",
       "8     ▁a -3.87010\n",
       "9      t -4.02045"
      ]
     },
     "execution_count": 140,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "EttG7Gc01BpA",
    "outputId": "ce8f85ee-03fa-423c-981b-2c702fa14eb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "vocab_file = \"imdb.model\"\n",
    "sp.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jlUUkSY21H5R",
    "outputId": "a3699cfb-ce5d-4005-c88d-84885e2c2c7a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'A very, very, very slow-moving, aimless movie about a distressed, drifting young man.'"
      ]
     },
     "execution_count": 142,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.loc[0, 'review']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "4xeVtZKJ1Kkl",
    "outputId": "db975667-daff-4a70-baab-1be7e9663fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁A', '▁very', ',', '▁very', ',', '▁very', '▁slow', '-', 'moving', ',', '▁a', 'im', 'less', '▁movie', '▁about', '▁a', '▁dist', 're', 's', 's', 'ed', ',', '▁dri', 'ft', 'ing', '▁you', 'ng', '▁man', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4YvO4Hn0dwQ"
   },
   "source": [
    "### 네이버 영화 리뷰 - 필수 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "id": "Q0HDAKAMkI2y",
    "outputId": "bf680842-328b-4d2e-bf1d-dc58ad53a26b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-20 02:07:37--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19515078 (19M) [text/plain]\n",
      "Saving to: ‘ratings.txt.1’\n",
      "\n",
      "ratings.txt.1       100%[===================>]  18.61M  35.6MB/s    in 0.5s    \n",
      "\n",
      "2020-07-20 02:07:38 (35.6 MB/s) - ‘ratings.txt.1’ saved [19515078/19515078]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-hYjFStzz3T"
   },
   "outputs": [],
   "source": [
    "naver_df = pd.read_table('ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "Nmde3FHFz45g",
    "outputId": "521ed573-e8dd-4dfe-8c22-65a0405aba9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "UhidmzTpz8Vu",
    "outputId": "5238008d-61e8-49a6-eca1-0cbfed0530d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 개수 : 200000\n"
     ]
    }
   ],
   "source": [
    "print('리뷰 개수 :',len(naver_df)) # 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ot5g1s4Y0rfH",
    "outputId": "1529e70e-c02c-4179-d0fe-b81d7d1bc46a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(naver_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ZLf13bo40yLW",
    "outputId": "ba3c377f-01b7-4432-b952-f9cbba7eb53d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "naver_df = naver_df.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(naver_df.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "tRDphUw701qy",
    "outputId": "5f40c09b-f290-4b62-ce90-be85067bd0d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 개수 : 199992\n"
     ]
    }
   ],
   "source": [
    "print('리뷰 개수 :',len(naver_df)) # 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7LxEwdc1HZp"
   },
   "outputs": [],
   "source": [
    "with open('naver_review.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(naver_df['document']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHqJUBGN5gMU"
   },
   "source": [
    "입력이 반드시 txt 파일이어야 하므로 txt 파일에 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV8WLRKx1Shq"
   },
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=naver --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lt8o18I740P0"
   },
   "source": [
    "SentencePieceTrainer를 이용해 토큰 학습\n",
    "\n",
    "input : 학습시킬 파일  \n",
    "model_prefix : 만들어질 모델 이름  \n",
    "vocab_size : 단어 집합의 크기  \n",
    "model_type : 사용할 모델 (unigram(default), bpe, char, word)  \n",
    "max_sentence_length: 문장의 최대 길이  \n",
    "pad_id, pad_piece: pad token id, 값  \n",
    "unk_id, unk_piece: unknown token id, 값  \n",
    "bos_id, bos_piece: begin of sentence token id, 값  \n",
    "eos_id, eos_piece: end of sequence token id, 값  \n",
    "user_defined_symbols: 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_3JudJV6GPc"
   },
   "source": [
    "vocab 생성이 완료되면 naver.model, naver.vocab 파일 두개가 생성 됩니다.  \n",
    ".vocab 에서 학습된 subwords 를 확인할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaKBT6xj3wF1"
   },
   "outputs": [],
   "source": [
    "vocab_list = pd.read_csv('naver.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "TPbT6TYLAwt0",
    "outputId": "283d91d0-6693-429d-e0f5-4b747042e9b8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>영화</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>▁영화</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>▁이</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>▁아</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>▁그</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0  <unk>  0\n",
       "1    <s>  0\n",
       "2   </s>  0\n",
       "3     ..  0\n",
       "4     영화 -1\n",
       "5    ▁영화 -2\n",
       "6     ▁이 -3\n",
       "7     ▁아 -4\n",
       "8    ... -5\n",
       "9     ▁그 -6"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "TSWbzlis31gs",
    "outputId": "b872d72d-6ccf-4400-96f6-7bd9a92e8084"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>P</td>\n",
       "      <td>-4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>▁이연걸</td>\n",
       "      <td>-2896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>립니다</td>\n",
       "      <td>-1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4940</th>\n",
       "      <td>쉼</td>\n",
       "      <td>-4937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>▁만들고</td>\n",
       "      <td>-2939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4069</th>\n",
       "      <td>균</td>\n",
       "      <td>-4066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>▁믿</td>\n",
       "      <td>-605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>자기</td>\n",
       "      <td>-1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>았어요</td>\n",
       "      <td>-2585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>무서</td>\n",
       "      <td>-2533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1\n",
       "4131     P -4128\n",
       "2899  ▁이연걸 -2896\n",
       "1670   립니다 -1667\n",
       "4940     쉼 -4937\n",
       "2942  ▁만들고 -2939\n",
       "4069     균 -4066\n",
       "608     ▁믿  -605\n",
       "1113    자기 -1110\n",
       "2588   았어요 -2585\n",
       "2536    무서 -2533"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHokapf863o4"
   },
   "source": [
    "Vocabulary 에는 unknown, 문장의 시작, 문장의 끝을 의미하는 special token이 0, 1, 2에 사용됨.\n",
    "\n",
    "> 들여쓴 블록\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "25W1ZJ_k6tMd",
    "outputId": "caf0df7a-38d6-4081-81e5-bbaff6820f9c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-31fc2ebfc51f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p-OyAa766OQ"
   },
   "source": [
    "설정한대로 5000 개의 subwords 가 학습됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3Nt7KwMO4BKZ",
    "outputId": "5649e288-3bd3-4625-9d60-5e437f2835e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "vocab_file = \"naver.model\"\n",
    "sp.load(vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnfbelxV6edl"
   },
   "source": [
    "GetPieceSize() : 단어 집합의 크기를 확인.  \n",
    "encode_as_pieces : 문장을 입력하면 서브 워드 시퀀스로 변환.  \n",
    "encode_as_ids : 문장을 입력하면 정수 시퀀스로 변환.  \n",
    "idToPiece : 정수로부터 맵핑되는 서브 워드로 변환.  \n",
    "PieceToId : 서브워드로부터 맵핑되는 정수로 변환.  \n",
    "DecodeIds : 정수 시퀀스로부터 문장으로 변환.  \n",
    "DecodePieces : 서브워드 시퀀스로부터 문장으로 변환.  \n",
    "encode : 문장으로부터 인자값에 따라서 정수 시퀀스 또는 서브워드 시퀀스로 변환 가능.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "WkvF18vl4QCY",
    "outputId": "e7a589b9-f541-4584-9faf-aa409c5446b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뭐 이딴 것도 영화냐.\n",
      "['▁뭐', '▁이딴', '▁것도', '▁영화냐', '.']\n",
      "[132, 966, 1296, 2590, 3276]\n",
      "\n",
      "진짜 최고의 영화입니다 ㅋㅋ\n",
      "['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n",
      "[54, 200, 821, 85]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "  \"뭐 이딴 것도 영화냐.\",\n",
    "  \"진짜 최고의 영화입니다 ㅋㅋ\",\n",
    "]\n",
    "for line in lines:\n",
    "  print(line)\n",
    "  print(sp.encode_as_pieces(line))\n",
    "  print(sp.encode_as_ids(line))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "fgP2obCw7so5",
    "outputId": "f06f06bd-8399-4b07-ee59-f5a7179b7cfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.GetPieceSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HOTD6wns7vEn",
    "outputId": "04f61297-3424-4769-fc15-9a50ae80b746"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'영화'"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.IdToPiece(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "iFBrLnWr7ybI",
    "outputId": "bc90f20a-eb22-4f70-b2f8-b5decdb1de32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.PieceToId('영화')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "wMn9E0zv7iqh",
    "outputId": "05dd4027-2186-4d90-dc92-ac02bdf53099"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'진짜 최고의 영화입니다 ᄏᄏ'"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodeIds([54, 200, 821, 85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Ov1nA7cY7mwn",
    "outputId": "28273e65-bfe3-4afc-d7c8-8183aa20c360"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'진짜 최고의 영화입니다 ᄏᄏ'"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodePieces(['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpj3i9wv7-f4"
   },
   "source": [
    "enable_sampling=True 일 때 Drop-out이 적용되며 alpha=0.1은 10% 확률로 dropout 한다는 의미."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "zDN6Ar4d75aW",
    "outputId": "c9e3c5dd-3a4b-4a1d-e0e6-fe348e915f71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁진짜', '▁최고의', '▁영화입니다', '▁', 'ᄏᄏ']\n",
      "['▁진짜', '▁최', '고', '의', '▁영화입니다', '▁ᄏᄏ']\n",
      "['▁', '진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n",
      "['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n",
      "['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "epz73OH08CV9",
    "outputId": "56ca02a8-7ddb-43aa-cff1-6d227e4a10ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n",
      "[54, 200, 821, 85]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=str))\n",
    "print(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02bcxHxJ1QIv"
   },
   "source": [
    "# 글자 레벨 기계 번역기 (함수형 API) - 필수 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz5CJRhZwJXb"
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1inVOFJzwI8j"
   },
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
    "filename = 'fra-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
    "    shutil.copyfileobj(r, out_file)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "seyA5LGZv_uF",
    "outputId": "10397183-5f94-4cd0-c0e7-da785202dc56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177210"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lines= pd.read_csv('fra.txt', names=['src', 'tar', 'CC'], sep='\\t')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "hvXdi_d6wg8U",
    "outputId": "9e65e521-215a-4048-8f48-4c6fa3e9b693"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29632</th>\n",
       "      <td>I heard the message.</td>\n",
       "      <td>J'entendis le message.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59927</th>\n",
       "      <td>Are you out of your mind?</td>\n",
       "      <td>Êtes-vous fou ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36122</th>\n",
       "      <td>I was really unlucky.</td>\n",
       "      <td>J'étais très malchanceux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24920</th>\n",
       "      <td>I'm laying you off.</td>\n",
       "      <td>Je te licencie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40808</th>\n",
       "      <td>Have yourself a drink.</td>\n",
       "      <td>Servez-vous un verre !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33446</th>\n",
       "      <td>You're kind of cute.</td>\n",
       "      <td>Vous êtes mignons, dans votre genre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49715</th>\n",
       "      <td>It'll be too late then.</td>\n",
       "      <td>Ce sera alors trop tard.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52234</th>\n",
       "      <td>Where are your parents?</td>\n",
       "      <td>Où sont tes parents ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>I am better.</td>\n",
       "      <td>Je vais mieux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42063</th>\n",
       "      <td>I like French cooking.</td>\n",
       "      <td>J'apprécie la cuisine française.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             src                                   tar\n",
       "29632       I heard the message.                J'entendis le message.\n",
       "59927  Are you out of your mind?                       Êtes-vous fou ?\n",
       "36122      I was really unlucky.             J'étais très malchanceux.\n",
       "24920        I'm laying you off.                       Je te licencie.\n",
       "40808     Have yourself a drink.                Servez-vous un verre !\n",
       "33446       You're kind of cute.  Vous êtes mignons, dans votre genre.\n",
       "49715    It'll be too late then.              Ce sera alors trop tard.\n",
       "52234    Where are your parents?                 Où sont tes parents ?\n",
       "2138                I am better.                        Je vais mieux.\n",
       "42063     I like French cooking.      J'apprécie la cuisine française."
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:60000] # 6만개만 저장\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "Zsi21Xu3wlGy",
    "outputId": "b91783a2-c786-4f15-f849-7b952565d933"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35048</th>\n",
       "      <td>I bet I can prove it.</td>\n",
       "      <td>\\t Je parie que je peux le prouver. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Am I late?</td>\n",
       "      <td>\\t Suis-je en retard ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34079</th>\n",
       "      <td>Do you need anything?</td>\n",
       "      <td>\\t As-tu besoin de quoi que ce soit ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10447</th>\n",
       "      <td>Go to your room.</td>\n",
       "      <td>\\t Va dans ta chambre ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33064</th>\n",
       "      <td>Why are we laughing?</td>\n",
       "      <td>\\t Pourquoi est-ce qu'on rigole ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9551</th>\n",
       "      <td>We're not done.</td>\n",
       "      <td>\\t Nous n'en avons pas terminé. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25037</th>\n",
       "      <td>I've found the key.</td>\n",
       "      <td>\\t J'ai trouvé la clé. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31154</th>\n",
       "      <td>Not everyone agreed.</td>\n",
       "      <td>\\t Tout le monde n'était pas d'accord. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16270</th>\n",
       "      <td>Tell Tom to wait.</td>\n",
       "      <td>\\t Dites à Tom d'attendre. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47251</th>\n",
       "      <td>Have you been drinking?</td>\n",
       "      <td>\\t Avez-vous bu ? \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                                        tar\n",
       "35048    I bet I can prove it.     \\t Je parie que je peux le prouver. \\n\n",
       "494                 Am I late?                  \\t Suis-je en retard ? \\n\n",
       "34079    Do you need anything?   \\t As-tu besoin de quoi que ce soit ? \\n\n",
       "10447         Go to your room.                 \\t Va dans ta chambre ! \\n\n",
       "33064     Why are we laughing?       \\t Pourquoi est-ce qu'on rigole ? \\n\n",
       "9551           We're not done.         \\t Nous n'en avons pas terminé. \\n\n",
       "25037      I've found the key.                  \\t J'ai trouvé la clé. \\n\n",
       "31154     Not everyone agreed.  \\t Tout le monde n'était pas d'accord. \\n\n",
       "16270        Tell Tom to wait.              \\t Dites à Tom d'attendre. \\n\n",
       "47251  Have you been drinking?                       \\t Avez-vous bu ? \\n"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHrky952xM6T"
   },
   "outputs": [],
   "source": [
    "# 글자 집합 구축\n",
    "src_vocab=set()\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 글자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab=set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "DBrUwEU0xORz",
    "outputId": "f725e489-d09f-467c-ee69-3c4de316884b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "RjO5UE_VxP9b",
    "outputId": "4cf6fed9-f252-4d20-eca9-891e2736d2d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "-ZyVbd5KxRbE",
    "outputId": "a46c986c-1746-4155-8de6-3770ec96590c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, ',': 11, '-': 12, '.': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, '\\xa0': 78, '«': 79, '»': 80, 'À': 81, 'Ç': 82, 'É': 83, 'Ê': 84, 'Ô': 85, 'à': 86, 'â': 87, 'ç': 88, 'è': 89, 'é': 90, 'ê': 91, 'ë': 92, 'î': 93, 'ï': 94, 'ô': 95, 'ù': 96, 'û': 97, 'œ': 98, 'С': 99, '\\u2009': 100, '\\u200b': 101, '‘': 102, '’': 103, '\\u202f': 104}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2psnowDVYo5P"
   },
   "source": [
    "기본적인 전처리를 거친 후의 데이터. 데이터는 기본적으로 세 종류가 있어야 함.  \n",
    "encoder_input은 인코더의 입력을 위한 데이터.  \n",
    "decoder_input은 디코더의 입력을 위한 데이터. 그렇기 때문에 시작을 의미하는 \\t 토큰이 필요.  \n",
    "decoder_target은 디코더의 레이블을 위한 데이터. 그렇기 때문에 종료를 의미하는 \\n 토큰이 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jLr50UH1xSLG",
    "outputId": "654d452d-2c29-4d8c-d662-b087ebaeee45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [31, 58, 10], [31, 58, 10], [41, 70, 63, 2], [41, 70, 63, 2]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "      temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_3DeJJ8IxTzK",
    "outputId": "d3117a29-d471-4f78-da04-3c82ff9433b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 47, 52, 3, 4, 3, 2], [1, 3, 44, 52, 63, 72, 71, 3, 4, 3, 2], [1, 3, 44, 52, 63, 72, 71, 13, 3, 2], [1, 3, 28, 66, 72, 69, 70, 104, 4, 3, 2], [1, 3, 28, 66, 72, 69, 56, 77, 104, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "V68v4hj3xYLc",
    "outputId": "37cfc686-9f09-40ef-bdd0-b9456537a430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 47, 52, 3, 4, 3, 2], [3, 44, 52, 63, 72, 71, 3, 4, 3, 2], [3, 44, 52, 63, 72, 71, 13, 3, 2], [3, 28, 66, 72, 69, 70, 104, 4, 3, 2], [3, 28, 66, 72, 69, 56, 77, 104, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      if t>0:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "      t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "MxKqHu6GxZs1",
    "outputId": "221dee76-9e7b-4e17-d46f-23de6ba87eee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAiLC3W7xbTj"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWKdRFOtxcyU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYgMNHUWZBEg"
   },
   "source": [
    "인코더와 디코더 모델을 설계.  \n",
    "\n",
    "인코더 모델은 LSTM을 사용. return_state=True이므로  \n",
    "마지막 시점의 은닉 상태인 state_h와 state_c를 리턴.  \n",
    "\n",
    "현재 return_sequences는 값을 지정해주지 않았는데, 디폴트값이 False이므로  \n",
    "이 경우 encoder_outputs은 마지막 시점의 은닉상태.  \n",
    "\n",
    "state_h와 state_c를 encoder_state에 저장해둠. 이를 컨텍스트 벡터로 사용하기 위함.  \n",
    "\n",
    "참고 : https://wikidocs.net/106473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cv1RK5MTxeFC"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-LB2nsWZoq-"
   },
   "source": [
    "디코더 모델 또한 LSTM을 사용. 인코더와는 달리 return_sequences가 True인데,  \n",
    "이러면 모든 시점의 은닉 상태를 리턴. 이는 decoder_outputs에 저장됨.  \n",
    "이를 출력층으로 통과시켜서 예측."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_KtCjeFx9tW"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I6UYSlgRx_z0",
    "outputId": "9f8c92c4-9bfb-4793-bc42-a8d254505855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "750/750 [==============================] - 13s 17ms/step - loss: 0.7774 - val_loss: 0.6827\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.4753 - val_loss: 0.5522\n",
      "Epoch 3/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.3965 - val_loss: 0.4856\n",
      "Epoch 4/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.3530 - val_loss: 0.4454\n",
      "Epoch 5/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.3241 - val_loss: 0.4198\n",
      "Epoch 6/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.3024 - val_loss: 0.4048\n",
      "Epoch 7/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.2857 - val_loss: 0.3944\n",
      "Epoch 8/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.2723 - val_loss: 0.3816\n",
      "Epoch 9/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.2612 - val_loss: 0.3740\n",
      "Epoch 10/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.2515 - val_loss: 0.3703\n",
      "Epoch 11/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.2432 - val_loss: 0.3646\n",
      "Epoch 12/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.2357 - val_loss: 0.3613\n",
      "Epoch 13/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.2290 - val_loss: 0.3587\n",
      "Epoch 14/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.2227 - val_loss: 0.3585\n",
      "Epoch 15/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.2171 - val_loss: 0.3574\n",
      "Epoch 16/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.2118 - val_loss: 0.3569\n",
      "Epoch 17/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.2069 - val_loss: 0.3563\n",
      "Epoch 18/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.2024 - val_loss: 0.3573\n",
      "Epoch 19/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1980 - val_loss: 0.3582\n",
      "Epoch 20/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1939 - val_loss: 0.3588\n",
      "Epoch 21/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1901 - val_loss: 0.3600\n",
      "Epoch 22/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1865 - val_loss: 0.3617\n",
      "Epoch 23/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1832 - val_loss: 0.3629\n",
      "Epoch 24/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1798 - val_loss: 0.3651\n",
      "Epoch 25/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1768 - val_loss: 0.3645\n",
      "Epoch 26/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1738 - val_loss: 0.3673\n",
      "Epoch 27/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1709 - val_loss: 0.3680\n",
      "Epoch 28/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1682 - val_loss: 0.3720\n",
      "Epoch 29/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1657 - val_loss: 0.3752\n",
      "Epoch 30/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1632 - val_loss: 0.3809\n",
      "Epoch 31/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1607 - val_loss: 0.3807\n",
      "Epoch 32/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1586 - val_loss: 0.3820\n",
      "Epoch 33/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1563 - val_loss: 0.3852\n",
      "Epoch 34/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1543 - val_loss: 0.3857\n",
      "Epoch 35/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1521 - val_loss: 0.3883\n",
      "Epoch 36/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1502 - val_loss: 0.3899\n",
      "Epoch 37/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1482 - val_loss: 0.3937\n",
      "Epoch 38/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1466 - val_loss: 0.3973\n",
      "Epoch 39/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1447 - val_loss: 0.3993\n",
      "Epoch 40/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1431 - val_loss: 0.4015\n",
      "Epoch 41/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1416 - val_loss: 0.4031\n",
      "Epoch 42/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1397 - val_loss: 0.4071\n",
      "Epoch 43/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1383 - val_loss: 0.4103\n",
      "Epoch 44/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1368 - val_loss: 0.4129\n",
      "Epoch 45/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1354 - val_loss: 0.4164\n",
      "Epoch 46/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1340 - val_loss: 0.4164\n",
      "Epoch 47/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1327 - val_loss: 0.4217\n",
      "Epoch 48/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1313 - val_loss: 0.4236\n",
      "Epoch 49/50\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1300 - val_loss: 0.4253\n",
      "Epoch 50/50\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.1289 - val_loss: 0.4282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa06716a0b8>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp_AxqGbZ3at"
   },
   "source": [
    "학습을 다했다면 이를 테스트 과정에 사용할 차례.  \n",
    "\n",
    "인코더는 달라진 것이 없으므로 앞서 사용한 인코더를 그대로 사용.  \n",
    "하지만 디코더는 학습 단계(티처 포싱 사용)과 테스트 단계(이전 시점의 예측을 현재 시점의 입력으로 사용)의 동작 방식이 다르므로  \n",
    "\n",
    "디코더의 구조를 변경해줄 필요가 있음.  \n",
    "\n",
    "학습 과정과는 달리 state_h와 state_c를 버리지 않고있음을 주목.  \n",
    "테스트 단계에서의 디코더의 동작 자체는 decoder_sequence()라는 함수에서 컨트롤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9EhO43ayAz6"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cR_4bThIyCRk"
   },
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "decoder_states = [state_h, state_c]\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy6gLQIAyDuz"
   },
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNtoNyknab1z"
   },
   "source": [
    "decode_sequence는 번역을 원하는 문장을 입력받으면  \n",
    "번역 문장을 출력하는 테스트 단계를 위한 함수.  \n",
    "\n",
    "While문은 디코더의 각 시점을 컨트롤하는데 사용.  \n",
    "각 루프가 디코더의 현재 시점이 됨.  \n",
    "\n",
    "최종적으로 리턴할 decoded_sentence라는 문자열에  \n",
    "지속적으로 현재 시점에 예측한 단어를 추가해나감.  \n",
    "ex) I -> I like -> I like apples  \n",
    "\n",
    "target_seq는 이전 시점의 예측 결과이면서 현재 시점의 입력인 단어에 해당되는 원-핫 벡터."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMnOKzgYyFsn"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow-eKfrFzoIK"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "7ob9YTTgyGRg",
    "outputId": "c6ba0f85-7929-4688-a8c5-3452cd753767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  Soit bien ! \n",
      "-----------------------------------\n",
      "입력 문장: I lied.\n",
      "정답 문장:  J'ai menti. \n",
      "번역기가 번역한 문장:  J'ai apprécié. \n",
      "-----------------------------------\n",
      "입력 문장: Come in.\n",
      "정답 문장:  Entre. \n",
      "번역기가 번역한 문장:  Entrez ! \n",
      "-----------------------------------\n",
      "입력 문장: Hurry up.\n",
      "정답 문장:  Magnez-vous ! \n",
      "번역기가 번역한 문장:  Magnez-vous ! \n",
      "-----------------------------------\n",
      "입력 문장: We walked.\n",
      "정답 문장:  Nous sommes allées à pied. \n",
      "번역기가 번역한 문장:  Nous l'avons tout. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbC3si3rrEEN"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "smooth_fn = SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 994
    },
    "id": "ZbbUbHkjzpuk",
    "outputId": "c9cb2914-7a56-4de1-8190-087220ac40c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "['Run!']\n",
      "정답 문장:  Cours ! \n",
      "['Cours', '!']\n",
      "번역기가 번역한 문장:  Soit bien ! \n",
      "['Soit', 'bien', '!']\n",
      "BLEU-1: 0.333333\n",
      "BLEU-2: 0.129099\n",
      "BLEU-3: 0.146742\n",
      "BLEU-4: 0.113622\n",
      "-----------------------------------\n",
      "입력 문장: I lied.\n",
      "['I', 'lied.']\n",
      "정답 문장:  J'ai menti. \n",
      "[\"J'ai\", 'menti.']\n",
      "번역기가 번역한 문장:  J'ai apprécié. \n",
      "[\"J'ai\", 'apprécié.']\n",
      "BLEU-1: 0.400000\n",
      "BLEU-2: 0.115470\n",
      "BLEU-3: 0.111474\n",
      "BLEU-4: 0.075984\n",
      "-----------------------------------\n",
      "입력 문장: Come in.\n",
      "['Come', 'in.']\n",
      "정답 문장:  Entre. \n",
      "['Entre.']\n",
      "번역기가 번역한 문장:  Entrez ! \n",
      "['Entrez', '!']\n",
      "BLEU-1: 0.285714\n",
      "BLEU-2: 0.084515\n",
      "BLEU-3: 0.081851\n",
      "BLEU-4: 0.053077\n",
      "-----------------------------------\n",
      "입력 문장: Hurry up.\n",
      "['Hurry', 'up.']\n",
      "정답 문장:  Magnez-vous ! \n",
      "['Magnez-vous', '!']\n",
      "번역기가 번역한 문장:  Magnez-vous ! \n",
      "['Magnez-vous', '!']\n",
      "BLEU-1: 0.444444\n",
      "BLEU-2: 0.298142\n",
      "BLEU-3: 0.159969\n",
      "BLEU-4: 0.086334\n",
      "-----------------------------------\n",
      "입력 문장: We walked.\n",
      "['We', 'walked.']\n",
      "정답 문장:  Nous sommes allées à pied. \n",
      "['Nous', 'sommes', 'allées', 'à', 'pied.']\n",
      "번역기가 번역한 문장:  Nous l'avons tout. \n",
      "['Nous', \"l'avons\", 'tout.']\n",
      "BLEU-1: 0.416667\n",
      "BLEU-2: 0.243975\n",
      "BLEU-3: 0.132653\n",
      "BLEU-4: 0.069853\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    actual.append([lines.tar[seq_index][1:len(lines.tar[seq_index])-1].split()])\n",
    "    predicted.append(decoded_sentence[:len(decoded_sentence)-1].split())\n",
    "                  \n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print(lines.src[seq_index].split())\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print(lines.tar[seq_index][1:len(lines.tar[seq_index])-1].split())\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력\n",
    "    print(decoded_sentence[:len(decoded_sentence)-1].split())\n",
    "    \n",
    "    #print(actual)\n",
    "    #print(predicted)\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0), smoothing_function = smooth_fn.method1))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0), smoothing_function = smooth_fn.method1))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0), smoothing_function = smooth_fn.method1))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function = smooth_fn.method1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgyWaYYUZMAW"
   },
   "source": [
    "# 단어 레벨 챗봇 (함수형 API) - 선택적 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-OZb2-8Zb99"
   },
   "source": [
    "이 챗봇은 데이터를 매우 적게 학습. 더 많은 데이터를 학습할수록 일반화 능력이 높아짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "3_13acNUX7Jv",
    "outputId": "6ba4ac80-31c8-4e0f-8521-c1b9d0ad0817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
      "Collecting JPype1>=0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/49/725710351d78d26c65337b1e3b322d7b27b34b704535ab56afc0d9ab0ffd/JPype1-1.0.1-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 47.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
      "Collecting tweepy>=3.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
      "Collecting beautifulsoup4==4.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 10.6MB/s \n",
      "\u001b[?25hCollecting colorama\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Installing collected packages: JPype1, tweepy, beautifulsoup4, colorama, konlpy\n",
      "  Found existing installation: tweepy 3.6.0\n",
      "    Uninstalling tweepy-3.6.0:\n",
      "      Successfully uninstalled tweepy-3.6.0\n",
      "  Found existing installation: beautifulsoup4 4.6.3\n",
      "    Uninstalling beautifulsoup4-4.6.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.6.3\n",
      "Successfully installed JPype1-1.0.1 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CT-_Zm6lXyku"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "id": "lMaSKuFiYADi",
    "outputId": "f7ddce0b-9d9b-4520-8190-e5dec4eb578d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-19 00:01:32--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 889842 (869K) [text/plain]\n",
      "Saving to: ‘ChatbotData .csv’\n",
      "\n",
      "\r",
      "ChatbotData .csv      0%[                    ]       0  --.-KB/s               \r",
      "ChatbotData .csv    100%[===================>] 868.99K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2020-07-19 00:01:32 (9.53 MB/s) - ‘ChatbotData .csv’ saved [889842/889842]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "69dqvTJMYGSA",
    "outputId": "aeb73f85-1ee2-422b-d311-7692cf75c95d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ChatbotData .csv'   sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_iLXSDtX3Ny"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 태그 단어\n",
    "PAD = \"<PADDING>\"   # 패딩\n",
    "STA = \"<START>\"     # 시작\n",
    "END = \"<END>\"       # 끝\n",
    "OOV = \"<OOV>\"       # 없는 단어(Out of Vocabulary)\n",
    "\n",
    "# 태그 인덱스\n",
    "PAD_INDEX = 0\n",
    "STA_INDEX = 1\n",
    "END_INDEX = 2\n",
    "OOV_INDEX = 3\n",
    "\n",
    "# 데이터 타입\n",
    "ENCODER_INPUT  = 0\n",
    "DECODER_INPUT  = 1\n",
    "DECODER_TARGET = 2\n",
    "\n",
    "# 한 문장에서 단어 시퀀스의 최대 개수\n",
    "max_sequences = 30\n",
    "\n",
    "# 임베딩 벡터 차원\n",
    "embedding_dim = 100\n",
    "\n",
    "# LSTM 히든레이어 차원\n",
    "lstm_hidden_dim = 128\n",
    "\n",
    "# 정규 표현식 필터\n",
    "RE_FILTER = re.compile(\"[.,!?\\\"':;~()]\")\n",
    "\n",
    "# 챗봇 데이터 로드\n",
    "chatbot_data = pd.read_csv('ChatbotData .csv', encoding='utf-8')\n",
    "question, answer = list(chatbot_data['Q']), list(chatbot_data['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "20kVEhbvYKHP",
    "outputId": "56303f6c-01b5-4d29-ba88-de80f4a863f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 데이터 개수\n",
    "len(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "l1bWONM3YMHd",
    "outputId": "030a1016-ddd4-44c2-ee57-0dbe64aae5f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : 12시 땡!\n",
      "A : 하루가 또 가네요.\n",
      "\n",
      "Q : 1지망 학교 떨어졌어\n",
      "A : 위로해 드립니다.\n",
      "\n",
      "Q : 3박4일 놀러가고 싶다\n",
      "A : 여행은 언제나 좋죠.\n",
      "\n",
      "Q : 3박4일 정도 놀러가고 싶다\n",
      "A : 여행은 언제나 좋죠.\n",
      "\n",
      "Q : PPL 심하네\n",
      "A : 눈살이 찌푸려지죠.\n",
      "\n",
      "Q : SD카드 망가졌어\n",
      "A : 다시 새로 사는 게 마음 편해요.\n",
      "\n",
      "Q : SD카드 안돼\n",
      "A : 다시 새로 사는 게 마음 편해요.\n",
      "\n",
      "Q : SNS 맞팔 왜 안하지ㅠㅠ\n",
      "A : 잘 모르고 있을 수도 있어요.\n",
      "\n",
      "Q : SNS 시간낭비인 거 아는데 매일 하는 중\n",
      "A : 시간을 정하고 해보세요.\n",
      "\n",
      "Q : SNS 시간낭비인데 자꾸 보게됨\n",
      "A : 시간을 정하고 해보세요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 일부만 학습에 사용\n",
    "question = question[:100]\n",
    "answer = answer[:100]\n",
    "\n",
    "# 챗봇 데이터 출력\n",
    "for i in range(10):\n",
    "    print('Q : ' + question[i])\n",
    "    print('A : ' + answer[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPiHbiqPYNx8"
   },
   "outputs": [],
   "source": [
    "# 형태소분석 함수\n",
    "def pos_tag(sentences):\n",
    "    \n",
    "    # KoNLPy 형태소분석기 설정\n",
    "    tagger = Okt()\n",
    "    \n",
    "    # 문장 품사 변수 초기화\n",
    "    sentences_pos = []\n",
    "    \n",
    "    # 모든 문장 반복\n",
    "    for sentence in sentences:\n",
    "        # 특수기호 제거\n",
    "        sentence = re.sub(RE_FILTER, \"\", sentence)\n",
    "        \n",
    "        # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
    "        sentence = \" \".join(tagger.morphs(sentence))\n",
    "        sentences_pos.append(sentence)\n",
    "        \n",
    "    return sentences_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "vLuihRTrYQUm",
    "outputId": "4dc028b2-7d48-4cbb-de65-5e6b339ad86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : 12시 땡\n",
      "A : 하루 가 또 가네요\n",
      "\n",
      "Q : 1 지망 학교 떨어졌어\n",
      "A : 위로 해 드립니다\n",
      "\n",
      "Q : 3 박 4일 놀러 가고 싶다\n",
      "A : 여행 은 언제나 좋죠\n",
      "\n",
      "Q : 3 박 4일 정도 놀러 가고 싶다\n",
      "A : 여행 은 언제나 좋죠\n",
      "\n",
      "Q : PPL 심하네\n",
      "A : 눈살 이 찌푸려지죠\n",
      "\n",
      "Q : SD 카드 망가졌어\n",
      "A : 다시 새로 사는 게 마음 편해요\n",
      "\n",
      "Q : SD 카드 안 돼\n",
      "A : 다시 새로 사는 게 마음 편해요\n",
      "\n",
      "Q : SNS 맞팔 왜 안 하지 ㅠㅠ\n",
      "A : 잘 모르고 있을 수도 있어요\n",
      "\n",
      "Q : SNS 시간 낭비 인 거 아는데 매일 하는 중\n",
      "A : 시간 을 정 하고 해보세요\n",
      "\n",
      "Q : SNS 시간 낭비 인데 자꾸 보게 됨\n",
      "A : 시간 을 정 하고 해보세요\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 형태소분석 수행\n",
    "question = pos_tag(question)\n",
    "answer = pos_tag(answer)\n",
    "\n",
    "# 형태소분석으로 변환된 챗봇 데이터 출력\n",
    "for i in range(10):\n",
    "    print('Q : ' + question[i])\n",
    "    print('A : ' + answer[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1iJrUckYSK2"
   },
   "outputs": [],
   "source": [
    "# 질문과 대답 문장들을 하나로 합침\n",
    "sentences = []\n",
    "sentences.extend(question)\n",
    "sentences.extend(answer)\n",
    "\n",
    "words = []\n",
    "\n",
    "# 단어들의 배열 생성\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        words.append(word)\n",
    "\n",
    "# 길이가 0인 단어는 삭제\n",
    "words = [word for word in words if len(word) > 0]\n",
    "\n",
    "# 중복된 단어 삭제\n",
    "words = list(set(words))\n",
    "\n",
    "# 제일 앞에 태그 단어 삽입\n",
    "words[:0] = [PAD, STA, END, OOV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "k15_zuuBYTv7",
    "outputId": "5c50d4d2-c829-44c5-fa50-579060343e02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 개수\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "1Y3vbVu9YU_L",
    "outputId": "2cf5ed8e-1f48-492e-d0b7-c0a0120769f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PADDING>',\n",
       " '<START>',\n",
       " '<END>',\n",
       " '<OOV>',\n",
       " '입어볼까',\n",
       " '이야',\n",
       " '결정',\n",
       " '막',\n",
       " '잠깐',\n",
       " '쇼핑',\n",
       " '정도',\n",
       " '있는',\n",
       " '됨',\n",
       " '연인',\n",
       " '그게',\n",
       " '약',\n",
       " '떨리는',\n",
       " '옷',\n",
       " '1',\n",
       " '맛있게']"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 단어 출력\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFQJ4ZwcYWbY"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 단어와 인덱스의 딕셔너리 생성\n",
    "word_to_index = {word: index for index, word in enumerate(words)}\n",
    "index_to_word = {index: word for index, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "kkitFQZxYZHI",
    "outputId": "c09035de-2f43-4c5e-aab5-06207048181f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 18,\n",
       " '<END>': 2,\n",
       " '<OOV>': 3,\n",
       " '<PADDING>': 0,\n",
       " '<START>': 1,\n",
       " '결정': 6,\n",
       " '그게': 14,\n",
       " '됨': 12,\n",
       " '떨리는': 16,\n",
       " '막': 7,\n",
       " '맛있게': 19,\n",
       " '쇼핑': 9,\n",
       " '약': 15,\n",
       " '연인': 13,\n",
       " '옷': 17,\n",
       " '이야': 5,\n",
       " '입어볼까': 4,\n",
       " '있는': 11,\n",
       " '잠깐': 8,\n",
       " '정도': 10}"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 -> 인덱스\n",
    "# 문장을 인덱스로 변환하여 모델 입력으로 사용\n",
    "dict(list(word_to_index.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "jhVa1ADUYbGV",
    "outputId": "f267d903-0b69-4b38-e816-9970f136a8c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PADDING>',\n",
       " 1: '<START>',\n",
       " 2: '<END>',\n",
       " 3: '<OOV>',\n",
       " 4: '입어볼까',\n",
       " 5: '이야',\n",
       " 6: '결정',\n",
       " 7: '막',\n",
       " 8: '잠깐',\n",
       " 9: '쇼핑',\n",
       " 10: '정도',\n",
       " 11: '있는',\n",
       " 12: '됨',\n",
       " 13: '연인',\n",
       " 14: '그게',\n",
       " 15: '약',\n",
       " 16: '떨리는',\n",
       " 17: '옷',\n",
       " 18: '1',\n",
       " 19: '맛있게'}"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델의 예측 결과인 인덱스를 문장으로 변환시 사용\n",
    "dict(list(index_to_word.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88QFC-JXYdaq"
   },
   "outputs": [],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "def convert_text_to_index(sentences, vocabulary, type): \n",
    "    \n",
    "    sentences_index = []\n",
    "    \n",
    "    # 모든 문장에 대해서 반복\n",
    "    for sentence in sentences:\n",
    "        sentence_index = []\n",
    "        \n",
    "        # 디코더 입력일 경우 맨 앞에 START 태그 추가\n",
    "        if type == DECODER_INPUT:\n",
    "            sentence_index.extend([vocabulary[STA]])\n",
    "        \n",
    "        # 문장의 단어들을 띄어쓰기로 분리\n",
    "        for word in sentence.split():\n",
    "            if vocabulary.get(word) is not None:\n",
    "                # 사전에 있는 단어면 해당 인덱스를 추가\n",
    "                sentence_index.extend([vocabulary[word]])\n",
    "            else:\n",
    "                # 사전에 없는 단어면 OOV 인덱스를 추가\n",
    "                sentence_index.extend([vocabulary[OOV]])\n",
    "\n",
    "        # 최대 길이 검사\n",
    "        if type == DECODER_TARGET:\n",
    "            # 디코더 목표일 경우 맨 뒤에 END 태그 추가\n",
    "            if len(sentence_index) >= max_sequences:\n",
    "                sentence_index = sentence_index[:max_sequences-1] + [vocabulary[END]]\n",
    "            else:\n",
    "                sentence_index += [vocabulary[END]]\n",
    "        else:\n",
    "            if len(sentence_index) > max_sequences:\n",
    "                sentence_index = sentence_index[:max_sequences]\n",
    "            \n",
    "        # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
    "        sentence_index += (max_sequences - len(sentence_index)) * [vocabulary[PAD]]\n",
    "        \n",
    "        # 문장의 인덱스 배열을 추가\n",
    "        sentences_index.append(sentence_index)\n",
    "\n",
    "    return np.asarray(sentences_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "YPmuWDxOYfRi",
    "outputId": "fe13c520-635d-4953-f64f-e609bb438ab4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([354, 431,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코더 입력 인덱스 변환\n",
    "x_encoder = convert_text_to_index(question, word_to_index, ENCODER_INPUT)\n",
    "\n",
    "# 첫 번째 인코더 입력 출력 (12시 땡)\n",
    "x_encoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "AAoOMv89YgqV",
    "outputId": "ef4e6a0e-4b0b-4e0f-b06a-ba4895477715"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1, 246, 262,  26, 349,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코더 입력 인덱스 변환\n",
    "x_decoder = convert_text_to_index(answer, word_to_index, DECODER_INPUT)\n",
    "\n",
    "# 첫 번째 디코더 입력 출력 (START 하루 가 또 가네요)\n",
    "x_decoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "aHso80jXYhlc",
    "outputId": "be3a6965-6be5-4682-e7d6-78d8ac6e3f25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([246, 262,  26, 349,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코더 목표 인덱스 변환\n",
    "y_decoder = convert_text_to_index(answer, word_to_index, DECODER_TARGET)\n",
    "\n",
    "# 첫 번째 디코더 목표 출력 (하루 가 또 가네요 END)\n",
    "y_decoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "Ahr_oCPYYkBJ",
    "outputId": "00bc2fbe-1562-496e-c51d-f56014b01793"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원핫인코딩 초기화\n",
    "one_hot_data = np.zeros((len(y_decoder), max_sequences, len(words)))\n",
    "\n",
    "# 디코더 목표를 원핫인코딩으로 변환\n",
    "# 학습시 입력은 인덱스이지만, 출력은 원핫인코딩 형식임\n",
    "for i, sequence in enumerate(y_decoder):\n",
    "    for j, index in enumerate(sequence):\n",
    "        one_hot_data[i, j, index] = 1\n",
    "\n",
    "# 디코더 목표 설정\n",
    "y_decoder = one_hot_data\n",
    "\n",
    "# 첫 번째 디코더 목표 출력\n",
    "y_decoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "o3gCejxdYnZZ",
    "outputId": "167a6182-a136-46e4-f5d7-11dbfba83c50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#--------------------------------------------\n",
    "# 훈련 모델 인코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 입력 문장의 인덱스 시퀀스를 입력으로 받음\n",
    "encoder_inputs = layers.Input(shape=(None,))\n",
    "\n",
    "# 임베딩 레이어\n",
    "encoder_outputs = layers.Embedding(len(words), embedding_dim)(encoder_inputs)\n",
    "\n",
    "# return_state가 True면 상태값 리턴\n",
    "# LSTM은 state_h(hidden state)와 state_c(cell state) 2개의 상태 존재\n",
    "encoder_outputs, state_h, state_c = layers.LSTM(lstm_hidden_dim,\n",
    "                                                dropout=0.1,\n",
    "                                                recurrent_dropout=0.5,\n",
    "                                                return_state=True)(encoder_outputs)\n",
    "\n",
    "# 히든 상태와 셀 상태를 하나로 묶음\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# 훈련 모델 디코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 목표 문장의 인덱스 시퀀스를 입력으로 받음\n",
    "decoder_inputs = layers.Input(shape=(None,))\n",
    "\n",
    "# 임베딩 레이어\n",
    "decoder_embedding = layers.Embedding(len(words), embedding_dim)\n",
    "decoder_outputs = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# 인코더와 달리 return_sequences를 True로 설정하여 모든 타임 스텝 출력값 리턴\n",
    "# 모든 타임 스텝의 출력값들을 다음 레이어의 Dense()로 처리하기 위함\n",
    "decoder_lstm = layers.LSTM(lstm_hidden_dim,\n",
    "                           dropout=0.1,\n",
    "                           recurrent_dropout=0.5,\n",
    "                           return_state=True,\n",
    "                           return_sequences=True)\n",
    "\n",
    "# initial_state를 인코더의 상태로 초기화\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_outputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 단어의 개수만큼 노드의 개수를 설정하여 원핫 형식으로 각 단어 인덱스를 출력\n",
    "decoder_dense = layers.Dense(len(words), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# 훈련 모델 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 입력과 출력으로 함수형 API 모델 생성\n",
    "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 학습 방법 설정\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDAb3Ed1YoLG"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#  예측 모델 인코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 훈련 모델의 인코더 상태를 사용하여 예측 모델 인코더 설정\n",
    "encoder_model = models.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# 예측 모델 디코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 예측시에는 훈련시와 달리 타임 스텝을 한 단계씩 수행\n",
    "# 매번 이전 디코더 상태를 입력으로 받아서 새로 설정\n",
    "decoder_state_input_h = layers.Input(shape=(lstm_hidden_dim,))\n",
    "decoder_state_input_c = layers.Input(shape=(lstm_hidden_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]    \n",
    "\n",
    "# 임베딩 레이어\n",
    "decoder_outputs = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# LSTM 레이어\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_outputs,\n",
    "                                                 initial_state=decoder_states_inputs)\n",
    "\n",
    "# 히든 상태와 셀 상태를 하나로 묶음\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "# Dense 레이어를 통해 원핫 형식으로 각 단어 인덱스를 출력\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 예측 모델 디코더 설정\n",
    "decoder_model = models.Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7neWJ76dYsAz"
   },
   "outputs": [],
   "source": [
    "# 인덱스를 문장으로 변환\n",
    "def convert_index_to_text(indexs, vocabulary): \n",
    "    \n",
    "    sentence = ''\n",
    "    \n",
    "    # 모든 문장에 대해서 반복\n",
    "    for index in indexs:\n",
    "        if index == END_INDEX:\n",
    "            # 종료 인덱스면 중지\n",
    "            break;\n",
    "        if vocabulary.get(index) is not None:\n",
    "            # 사전에 있는 인덱스면 해당 단어를 추가\n",
    "            sentence += vocabulary[index]\n",
    "        else:\n",
    "            # 사전에 없는 인덱스면 OOV 단어를 추가\n",
    "            sentence.extend([vocabulary[OOV_INDEX]])\n",
    "            \n",
    "        # 빈칸 추가\n",
    "        sentence += ' '\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "669stk8RYu5R",
    "outputId": "59c92497-00cd-4b88-94d9-6c2c60a24a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch : 1\n",
      "accuracy : 0.9266666769981384\n",
      "loss : 0.35812312364578247\n",
      "맛있게 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 2\n",
      "accuracy : 0.968999981880188\n",
      "loss : 0.14514704048633575\n",
      "가세 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 3\n",
      "accuracy : 0.9739999771118164\n",
      "loss : 0.08765653520822525\n",
      "가세 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 4\n",
      "accuracy : 0.9783333539962769\n",
      "loss : 0.06297517567873001\n",
      "가세 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 5\n",
      "accuracy : 0.9853333234786987\n",
      "loss : 0.044016480445861816\n",
      "가세 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 6\n",
      "accuracy : 0.9919999837875366\n",
      "loss : 0.027679210528731346\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 7\n",
      "accuracy : 0.9953333139419556\n",
      "loss : 0.017140144482254982\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 8\n",
      "accuracy : 0.996999979019165\n",
      "loss : 0.011556596495211124\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 9\n",
      "accuracy : 0.9990000128746033\n",
      "loss : 0.005027387291193008\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 10\n",
      "accuracy : 0.999666690826416\n",
      "loss : 0.0017524176510050893\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 11\n",
      "accuracy : 1.0\n",
      "loss : 0.0006122245686128736\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 12\n",
      "accuracy : 0.9990000128746033\n",
      "loss : 0.001641127746552229\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 13\n",
      "accuracy : 1.0\n",
      "loss : 0.00019004421483259648\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 14\n",
      "accuracy : 0.999666690826416\n",
      "loss : 0.0008484653662890196\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 15\n",
      "accuracy : 1.0\n",
      "loss : 0.00014795167953707278\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 16\n",
      "accuracy : 0.999666690826416\n",
      "loss : 0.001423284295015037\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 17\n",
      "accuracy : 1.0\n",
      "loss : 2.6036621420644224e-05\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 18\n",
      "accuracy : 1.0\n",
      "loss : 1.404542854288593e-05\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 19\n",
      "accuracy : 1.0\n",
      "loss : 2.56245239143027e-05\n",
      "여행 은 언제나 좋죠 \n",
      "\n",
      "Total Epoch : 20\n",
      "accuracy : 1.0\n",
      "loss : 8.548993719159625e-06\n",
      "여행 은 언제나 좋죠 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 에폭 반복\n",
    "for epoch in range(20):\n",
    "    print('Total Epoch :', epoch + 1)\n",
    "\n",
    "    # 훈련 시작\n",
    "    history = model.fit([x_encoder, x_decoder],\n",
    "                        y_decoder,\n",
    "                        epochs=100,\n",
    "                        batch_size=64,\n",
    "                        verbose=0)\n",
    "    \n",
    "    # 정확도와 손실 출력\n",
    "    print('accuracy :', history.history['accuracy'][-1])\n",
    "    print('loss :', history.history['loss'][-1])\n",
    "    \n",
    "    # 문장 예측 테스트\n",
    "    # (3 박 4일 놀러 가고 싶다) -> (여행 은 언제나 좋죠)\n",
    "    input_encoder = x_encoder[2].reshape(1, x_encoder[2].shape[0])\n",
    "    input_decoder = x_decoder[2].reshape(1, x_decoder[2].shape[0])\n",
    "    results = model.predict([input_encoder, input_decoder])\n",
    "    \n",
    "    # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "    # 1축을 기준으로 가장 높은 값의 위치를 구함\n",
    "    indexs = np.argmax(results[0], 1) \n",
    "    \n",
    "    # 인덱스를 문장으로 변환\n",
    "    sentence = convert_index_to_text(indexs, index_to_word)\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwJGM_ABYxEO"
   },
   "outputs": [],
   "source": [
    "# 예측을 위한 입력 생성\n",
    "def make_predict_input(sentence):\n",
    "\n",
    "    sentences = []\n",
    "    sentences.append(sentence)\n",
    "    sentences = pos_tag(sentences)\n",
    "    input_seq = convert_text_to_index(sentences, word_to_index, ENCODER_INPUT)\n",
    "    \n",
    "    return input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJgBsvDYYyvS"
   },
   "outputs": [],
   "source": [
    "# 텍스트 생성\n",
    "def generate_text(input_seq):\n",
    "    \n",
    "    # 입력을 인코더에 넣어 마지막 상태 구함\n",
    "    states = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 목표 시퀀스 초기화\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    \n",
    "    # 목표 시퀀스의 첫 번째에 <START> 태그 추가\n",
    "    target_seq[0, 0] = STA_INDEX\n",
    "    \n",
    "    # 인덱스 초기화\n",
    "    indexs = []\n",
    "    \n",
    "    # 디코더 타임 스텝 반복\n",
    "    while 1:\n",
    "        # 디코더로 현재 타임 스텝 출력 구함\n",
    "        # 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화\n",
    "        decoder_outputs, state_h, state_c = decoder_model.predict(\n",
    "                                                [target_seq] + states)\n",
    "\n",
    "        # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "        index = np.argmax(decoder_outputs[0, 0, :])\n",
    "        indexs.append(index)\n",
    "        \n",
    "        # 종료 검사\n",
    "        if index == END_INDEX or len(indexs) >= max_sequences:\n",
    "            break\n",
    "\n",
    "        # 목표 시퀀스를 바로 이전의 출력으로 설정\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = index\n",
    "        \n",
    "        # 디코더의 이전 상태를 다음 디코더 예측에 사용\n",
    "        states = [state_h, state_c]\n",
    "\n",
    "    # 인덱스를 문장으로 변환\n",
    "    sentence = convert_index_to_text(indexs, index_to_word)\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "Dt_eySPHY0dJ",
    "outputId": "2d3c1212-b761-4616-f3a5-83307c2fbfe2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[372, 366, 236, 244, 412, 183,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "input_seq = make_predict_input('3박4일 놀러가고 싶다')\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LGKwqd-PY11D",
    "outputId": "7baee97a-f36b-4a07-f1fc-8bc17391fe5e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'여행 은 언제나 좋죠 '"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 모델로 텍스트 생성\n",
    "sentence = generate_text(input_seq)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "joK8S2AqY3r9",
    "outputId": "c0bfd67c-b4c4-4c39-ea56-5757ed434d1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[372, 366, 236, 153, 244, 412, 183,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "input_seq = make_predict_input('3박4일 같이 놀러가고 싶다')\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "QiJ_ZyaXY5Tc",
    "outputId": "a0ad1572-bf99-4f0f-c339-9f7dd08ad13f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'여행 은 언제나 좋죠 '"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 모델로 텍스트 생성\n",
    "sentence = generate_text(input_seq)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzMhEELbrJ8k"
   },
   "source": [
    "# Tensorflow.Keras의 model.fit() Vs. Tensorflow Gradient Tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oenX4IgQ0BzE"
   },
   "source": [
    "케라스의 model.fit()과 Gradient Tape()를 사용한 구현의 차이를 이해."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tbBWEy2rOw1"
   },
   "source": [
    "## in Tensorflow.Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "u9EDB3G2rXvk",
    "outputId": "7e666de0-4af8-4b20-9873-895e2611fae3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n# 신경망 모델 만들기\\nmodel = tf.keras.models.Sequential()\\n# 완전 연결층을 추가\\nmodel.add(tf.keras.layers.Dense(1))\\n# 옵티마이저와 손실 함수를 지정합니다.\\nmodel.compile(optimizer = 'sgd', loss = 'mse')\\n# 훈련 데이터를 사용하여 에포크 횟수만큼 훈련\\nmodel.fit(x_train, y_train, epochs = 10)\\n\""
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 신경망 모델 만들기\n",
    "model = tf.keras.models.Sequential()\n",
    "# 완전 연결층을 추가\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "# 옵티마이저와 손실 함수를 지정합니다.\n",
    "model.compile(optimizer = 'sgd', loss = 'mse')\n",
    "# 훈련 데이터를 사용하여 에포크 횟수만큼 훈련\n",
    "model.fit(x_train, y_train, epochs = 10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K5LBZKqrpCJ"
   },
   "source": [
    "위 코드를 조금 복잡하게 작성하면 아래와 같음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AbxmKiXrd0F"
   },
   "source": [
    "## in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__5VDeenvXIN"
   },
   "source": [
    "tape_gradient() 메서드는 자동 미분 기능을 수행.  \n",
    "자동 미분에 대해서 실습을 통해 이해. 임의로 2w^2+5라는 식을 세워보고, w에 대해 미분."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LI4sHiGufUe"
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(2.)\n",
    "\n",
    "def f(w):\n",
    "  y = w**2\n",
    "  z = 2*y + 5\n",
    "  return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVYs_MgsvKbv"
   },
   "source": [
    "이제 gradients를 출력하면 w가 속한 수식을 w로 미분한 값이 저장된 것을 확인할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "QBXlVmHLusJ2",
    "outputId": "193828c5-36ce-43c9-fa96-15a5784a6d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "  z = f(w)\n",
    "\n",
    "gradients = tape.gradient(z, [w])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "NiLOCKiQrgCo",
    "outputId": "c91820ba-a402-474e-b2fa-dadfeb501357"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'\\n# 훈련할 가중치 변수를 선언\\nw = tf.Variable(tf.zeros(shape=(1)))\\nb = tf.Variable(tf.zeros(shape=(1)))\\n\\n# 경사 하강법 옵티마이저 설정\\noptimizer = tf.optimizer.SGD(lr = 0.01)\\n# 에포크만큼 훈련\\nnum_epochs = 10\\nfor step in range(num_epochs):\\n   \\n    # 예측을 해서 손실을 구하는 과정입니다. (자동 미분을 위해 연산 과정을 기록합니다.)\\n    # tape_gradient() 메서드를 사용하면 그래디언트를 자동으로 계산할 수 있도록 합니다.\\n    with tf.GradientTape() as tape:\\n        z_net = w * x_train + b # 정방향 계산\\n        z_net = tf.reshape(z_net, [-1])\\n        sqr_errors = tf.square(y_train - z_net)\\n        mean_cost = tf.reduce_mean(sqr_errors) # 손실을 계산\\n\\n    # 경사하강법으로 파라미터를 업데이트하는 과정입니다.\\n    # 1. 가중치에 대한 그래디언트 계산\\n    grads = tape.gradient(mean_cost, [w, b])\\n\\n    # 2. 가중치를 업데이트\\n    # apply_gradients() 메서드에는 그래디언트와 가중치를 튜플로 묶은 리스트를 전달해야 합니다.\\n    # 보통 zip()을 주로 사용합니다.\\n    optimizer.apply_gradient(zip(grads, [w, b]))\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 훈련할 가중치 변수를 선언\n",
    "w = tf.Variable(tf.zeros(shape=(1)))\n",
    "b = tf.Variable(tf.zeros(shape=(1)))\n",
    "\n",
    "# 경사 하강법 옵티마이저 설정\n",
    "optimizer = tf.optimizer.SGD(lr = 0.01)\n",
    "# 에포크만큼 훈련\n",
    "num_epochs = 10\n",
    "for step in range(num_epochs):\n",
    "   \n",
    "    # 예측을 해서 손실을 구하는 과정입니다. (자동 미분을 위해 연산 과정을 기록합니다.)\n",
    "    # tape_gradient() 메서드를 사용하면 그래디언트를 자동으로 계산할 수 있도록 합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        z_net = w * x_train + b # 정방향 계산\n",
    "        z_net = tf.reshape(z_net, [-1])\n",
    "        sqr_errors = tf.square(y_train - z_net)\n",
    "        mean_cost = tf.reduce_mean(sqr_errors) # 손실을 계산\n",
    "\n",
    "    # 경사하강법으로 파라미터를 업데이트하는 과정입니다.\n",
    "    # 1. 가중치에 대한 그래디언트 계산\n",
    "    grads = tape.gradient(mean_cost, [w, b])\n",
    "\n",
    "    # 2. 가중치를 업데이트\n",
    "    # apply_gradients() 메서드에는 그래디언트와 가중치를 튜플로 묶은 리스트를 전달해야 합니다.\n",
    "    # 보통 zip()을 주로 사용합니다.\n",
    "    optimizer.apply_gradient(zip(grads, [w, b]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Oi4rnt8sl54"
   },
   "source": [
    "# 단어 레벨 기계 번역기 (서브클래싱 구현) - 필수 실습\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0VOjBLPv4DJ"
   },
   "source": [
    "공식 튜토리얼 구현체 참고하기 : https://www.tensorflow.org/tutorials/text/nmt_with_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYbmRNfgsmxZ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import zipfile\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaV5WUuRRzGv"
   },
   "source": [
    "http://www.manythings.org/anki/spa-eng.zip에서는 스페인어 - 영어 번역 데이터를 제공하고 있음. 이 데이터를 가지고 토이 프로젝트로 기계 번역기를 만들기. 단, 토이프로젝트인만큼 데이터는 3만개만 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rI2G5zZVldl7"
   },
   "outputs": [],
   "source": [
    "num_samples = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRnb_opc4WEf"
   },
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "url ='http://www.manythings.org/anki/spa-eng.zip'\n",
    "filename = 'spa-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
    "    shutil.copyfileobj(r, out_file)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Swd8ReGz45LK",
    "outputId": "cdbfb9be-6eb5-4b52-889b-98029fab486d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_about.txt  aclImdb_v1.tar.gz  ratings_train.txt  spa-eng.zip\n",
      "aclImdb     ratings_test.txt   sample_data\t  spa.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiCMe3FER72c"
   },
   "source": [
    "아래는 유니코드나 특수 문자를 제거하는 전처리 코드."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgzv0NOq06Nk"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xM6UlNIa4bBI"
   },
   "outputs": [],
   "source": [
    "# spa-eng\n",
    "def preprocess_sentence(sent):\n",
    "    sent = unicode_to_ascii(sent.lower())\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "GnzXdZdo6zYM",
    "outputId": "117ddb84-2c6b-41eb-b93e-c514b4dd660e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may i borrow this book ?\n",
      "b' puedo tomar prestado este libro ?'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huw22C1blOLr"
   },
   "outputs": [],
   "source": [
    "def download_and_read():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open(\"spa.txt\", \"r\") as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            en_sent, spa_sent, _ = line.strip().split('\\t')\n",
    "\n",
    "            # source 데이터 전처리\n",
    "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n",
    "\n",
    "            # target 데이터 전처리\n",
    "            spa_sent = preprocess_sentence(spa_sent)\n",
    "            spa_sent_in = [w for w in (\"BOS \" + spa_sent).split()]\n",
    "            spa_sent_out = [w for w in (spa_sent + \" EOS\").split()]\n",
    "\n",
    "            encoder_input.append(en_sent)\n",
    "            decoder_input.append(spa_sent_in)\n",
    "            decoder_target.append(spa_sent_out)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9zOgOgAlMhc"
   },
   "outputs": [],
   "source": [
    "sents_en_in, sents_spa_in, sents_spa_out = download_and_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AwL1n5VSAsR"
   },
   "source": [
    "기본적인 전처리를 거친 후의 데이터. 데이터는 기본적으로 세 종류가 있어야 함.  \n",
    "sents_en_in은 인코더의 입력을 위한 데이터.  \n",
    "sents_spa_in은 디코더의 입력을 위한 데이터. 그렇기 때문에 시작을 의미하는 BOS 토큰이 추가됨.  \n",
    "sents_spa_out은 디코더의 레이블을 위한 데이터. 그렇기 때문에 종료를 의미하는 EOS 토큰이 추가됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "Ba6vNyirlqmZ",
    "outputId": "ba3aa831-4278-4b66-9f96-cec5591bfe93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "[['BOS', 've', '.'], ['BOS', 'vete', '.'], ['BOS', 'vaya', '.'], ['BOS', 'vayase', '.'], ['BOS', 'hola', '.']]\n",
      "[['ve', '.', 'EOS'], ['vete', '.', 'EOS'], ['vaya', '.', 'EOS'], ['vayase', '.', 'EOS'], ['hola', '.', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "print(sents_en_in[:5])\n",
    "print(sents_spa_in[:5])\n",
    "print(sents_spa_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "lVvYMsqXswB1",
    "outputId": "c4357dae-2c2c-4718-8931-33e43eed03d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents_en_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "immyE4zw7Smy"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVEFG-WRSVCj"
   },
   "source": [
    "위 세 개의 데이터에 대해서 정수 인코딩, 패딩 과정을 거침."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4syEBic7Xex"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "data_en = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "data_en = pad_sequences(data_en, padding=\"post\")\n",
    "\n",
    "tokenizer_spa = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_spa.fit_on_texts(sents_spa_in)\n",
    "tokenizer_spa.fit_on_texts(sents_spa_out)\n",
    "\n",
    "data_spa_in = tokenizer_spa.texts_to_sequences(sents_spa_in)\n",
    "data_spa_in = pad_sequences(data_spa_in, padding=\"post\")\n",
    "\n",
    "data_spa_out = tokenizer_spa.texts_to_sequences(sents_spa_out)\n",
    "data_spa_out = pad_sequences(data_spa_out, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjYlyBdDSZpF"
   },
   "source": [
    "각각 데이터의 길이는 8, 14, 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "u15szBeK7hlm",
    "outputId": "88c0163c-ffbb-48ec-ab1b-1c9d03cd783e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 8)\n",
      "(30000, 14)\n",
      "(30000, 14)\n"
     ]
    }
   ],
   "source": [
    "print(data_en.shape)\n",
    "print(data_spa_in.shape)\n",
    "print(data_spa_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bi7kSvax7Y2m"
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3HVd-H9Ti4p"
   },
   "outputs": [],
   "source": [
    "def clean_up_logs(data_dir):\n",
    "    checkpoint_dir = os.path.join(data_dir, \"checkpoints\")\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    return checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5-wIgUg7dmZ"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = clean_up_logs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ImVfkzISeul"
   },
   "source": [
    "영어와 스페인어에 대해서 각각 다른 토크나이저를 사용하였으므로 단어 집합도 따로 존재.  \n",
    "각각의 단어 집합의 크기를 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "unJjZ7fB7sk7",
    "outputId": "6ea55023-560d-4dae-89d0-7b409e2bfd8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 (en): 4817, 스페인어 단어 집합의 크기 (spa): 9334\n"
     ]
    }
   ],
   "source": [
    "vocab_size_en = len(tokenizer_en.word_index) + 1\n",
    "vocab_size_spa = len(tokenizer_spa.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 (en): {:d}, 스페인어 단어 집합의 크기 (spa): {:d}\".format(vocab_size_en, vocab_size_spa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9Sz1fAY8seA"
   },
   "outputs": [],
   "source": [
    "word2idx_en = tokenizer_en.word_index\n",
    "idx2word_en = {v:k for k, v in word2idx_en.items()}\n",
    "\n",
    "word2idx_spa = tokenizer_spa.word_index\n",
    "idx2word_spa = {v:k for k, v in word2idx_spa.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Zu8hHEwy7u8t",
    "outputId": "49e758f0-cfd9-436e-ffb9-8c0d5d51bb0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqlen (en): 8, (spa): 14\n"
     ]
    }
   ],
   "source": [
    "maxlen_en = data_en.shape[1]\n",
    "maxlen_spa = data_spa_out.shape[1]\n",
    "print(\"seqlen (en): {:d}, (spa): {:d}\".format(maxlen_en, maxlen_spa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3npVu6emjut"
   },
   "source": [
    "텐서플로우에는 데이터셋(dataset)이라는 개념이 있습니다. 이는 연속된 데이터 샘플을 나타냄.  \n",
    "훈련 데이터를 데이터셋으로 정의하고나면, 이들을 셔플한다거나 배치 크기로 훈련 데이터와 테스트 데이터를 나누는 일을 할 수 있음  \n",
    "\n",
    "dataset.shuffle(버퍼 크기)  \n",
    "여기서 버퍼크기는 데이터를 가지고 있다가 랜덤으로 반환할 공간의 크기.  \n",
    "버퍼 크기는 충분히 주는 것이 좋음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWXdylXq87Kd"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_size = num_samples // 4\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_en, data_spa_in, data_spa_out))\n",
    "dataset = dataset.shuffle(10000)\n",
    "\n",
    "# take() 메서드는 전체 데이터 중 지정한 개수의 일부 데이터로만 출력을 제한한다.\n",
    "# skip() 메서드는 일부 데이터를 건너뛰고 다음 데이터를 출력한다.\n",
    "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\n",
    "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pP9xmA7oSlSI"
   },
   "source": [
    "지금까지는 학습을 위한 데이터셋을 준비하는 과정이었다면,  \n",
    "이제부터는 모델을 설계하는 과정.  \n",
    "\n",
    "우선 인코더.  \n",
    "\n",
    "인코더는  \n",
    "Embedding -> GRU 구조로 설계하였으며  \n",
    "GRU 내부적으로 return_sequence = True, return_state = True를 사용하였으므로  \n",
    "\n",
    "GRU는 총 두 개의 리턴값을 리턴하는데,  \n",
    "output은 모든 시점의 hidden states,\n",
    "state는 마지막 시점의 hidden state.  \n",
    "\n",
    "실질적으로 모델이 동작을 정의하는 부분은 Encoder Class에서 call() 함수 내부."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaVhwpK7ToyH"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, num_timesteps, embedding_dim, encoder_dim):\n",
    "        super(Encoder, self).__init__()#(**kwargs)\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embedding = Embedding(\n",
    "            vocab_size, embedding_dim, input_length = num_timesteps)\n",
    "        self.gru = GRU(encoder_dim, return_sequences = False, return_state = True)\n",
    "\n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=state)\n",
    "        return output, state\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.encoder_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AY-jV9bQSsUy"
   },
   "source": [
    "디코더.  \n",
    "디코더는 Embedding -> GRU -> Dense구조로 설계.  \n",
    "call() 내부를 보면 GRU의 첫번째 리턴값 output인 모든 시점의 hidden state를  \n",
    "Dense의 입력으로 사용하는데, 즉, 이는 모든 시점에서 Dense를 출력층으로 사용한다는 의미."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j40iq1nHmKJf"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_timesteps, decoder_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.embedding = Embedding(\n",
    "            vocab_size, embedding_dim, input_length = num_timesteps)\n",
    "        self.gru = GRU(decoder_dim, return_sequences = True, return_state = True)\n",
    "        self.dense = Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, state)\n",
    "        x = self.dense(output)\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEOnIGZp9TfE"
   },
   "outputs": [],
   "source": [
    "# check encoder/decoder dimensions\n",
    "embedding_dim = 256\n",
    "encoder_dim, decoder_dim = 1024, 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qF6IeCLWMdv"
   },
   "source": [
    "위에서 선언한 클래스를 이용하여 인코더와 디코더를 선언.  \n",
    "각각 encoder와 decoder란 이름으로 선언했는데,  \n",
    "이제 encoder(), decoder()를 호출하면 자동으로 내부의 call() 함수가 호출되는 방식."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gn0Tvc9z7zhG"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size_en, embedding_dim, maxlen_en, encoder_dim)\n",
    "decoder = Decoder(vocab_size_spa, embedding_dim, maxlen_spa, decoder_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czQ75h7lWX_I"
   },
   "source": [
    "옵티마이저로는 SparseCategoricalCrossentropy를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJw7_5v-mLxb"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_fn(ytrue, ypred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = loss_object(ytrue, ypred, sample_weight=mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOI2CscRWetI"
   },
   "source": [
    "tf.GradientTape()를 사용하여 학습.  \n",
    "서브클래싱으로 구현한 encoder와 decoder에 대해서 .trainable_variables를 하면 학습가능한 모든 파라미터들이 리턴됨.  \n",
    "\n",
    "이에 대해서 tape.gradient를 이용해 loss 함수에 대해서 미분하고,\n",
    "apply_gradients를 이용하여 모든 파라미터들을 업데이트."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXntmDq0Tq53"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n",
    "\n",
    "    # tf.GradeintTape() 안에서 배치 하나를 위한 예측을 만들고 손실을 계산합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "        loss = loss_fn(decoder_out, decoder_pred)\n",
    "    \n",
    "    # 모든 훈련 가능한 변수에 대해서\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # 경사하강법을 수행하여 변수를 업데이트 합니다.\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApaIVLn6WvSC"
   },
   "source": [
    "아래는 예측을 위한 함수.  \n",
    "\n",
    "while True부터가 디코더가 매 스텝마다 예측을 하는 과정.  \n",
    "\n",
    "1. 현재 시점에 대해서 예측을 하는 부분  \n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)  \n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1)  \n",
    "\n",
    "2. 예측한 정수 인덱스값을 실제 단어로 바꾸는 부분 ex) 35 -> apples  \n",
    "        pred_word = idx2word_spa[decoder_pred.numpy()[0][0]]\n",
    "\n",
    "3. 현재 시점에서 예측한 단어를 번역 결과 문장에 업데이트 하는 부분  \n",
    "   ex) I like 라는 지금까지 번역한 문장이 있었는데 현재 시점에서 apples를 예측했다면 I like apples로 업데이트 됨.  \n",
    "        pred_sent_spa.append(pred_word)  \n",
    "\n",
    "4. 현재 예측한 단어가 'EOS'라면 예측을 종료  \n",
    "        if pred_word == \"EOS\":\n",
    "            break  \n",
    "\n",
    "5. 현재 시점의 예측 단어를 다음 시점의 입력으로 사용  \n",
    "        decoder_in = decoder_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXMEMyPCmOm6"
   },
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, batch_size, \n",
    "        sents_en_in, data_en, sents_spa_out, \n",
    "        word2idx_spa, idx2word_spa):\n",
    "\n",
    "    # 랜덤 인덱스 선택\n",
    "    random_id = np.random.choice(len(sents_en_in))\n",
    "    print(\"input    : \",  \" \".join(sents_en_in[random_id]))\n",
    "    print(\"label    : \", \" \".join(sents_spa_out[random_id]))\n",
    "\n",
    "    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n",
    "    decoder_out = tf.expand_dims(sents_spa_out[random_id], axis=0)\n",
    "\n",
    "    encoder_state = encoder.init_state(1)\n",
    "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "    decoder_state = encoder_state\n",
    "\n",
    "    # 입력 문장에 대해서 시작 토큰 추가\n",
    "    decoder_in = tf.expand_dims(\n",
    "        tf.constant([word2idx_spa[\"BOS\"]]), axis=0)\n",
    "    \n",
    "    pred_sent_spa = []\n",
    "\n",
    "    while True:\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n",
    "        \n",
    "        pred_word = idx2word_spa[decoder_pred.numpy()[0][0]]\n",
    "        pred_sent_spa.append(pred_word)\n",
    "        if pred_word == \"EOS\":\n",
    "            break\n",
    "        decoder_in = decoder_pred\n",
    "    \n",
    "    print(\"predicted: \", \" \".join(pred_sent_spa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iw3AY6EhmPzz"
   },
   "outputs": [],
   "source": [
    "def evaluate_bleu_score(encoder, decoder, test_dataset, \n",
    "        word2idx_spa, idx2word_spa):\n",
    "\n",
    "    bleu_scores = []\n",
    "    smooth_fn = SmoothingFunction()\n",
    "    for encoder_in, decoder_in, decoder_out in test_dataset:\n",
    "        encoder_state = encoder.init_state(batch_size)\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "\n",
    "        # compute argmax\n",
    "        decoder_out = decoder_out.numpy()\n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\n",
    "\n",
    "        for i in range(decoder_out.shape[0]):\n",
    "            ref_sent = [idx2word_spa[j] for j in decoder_out[i].tolist() if j > 0]\n",
    "            hyp_sent = [idx2word_spa[j] for j in decoder_pred[i].tolist() if j > 0]\n",
    "            # remove trailing EOS\n",
    "            ref_sent = ref_sent[0:-1]\n",
    "            hyp_sent = hyp_sent[0:-1]\n",
    "            bleu_score = sentence_bleu([ref_sent], hyp_sent, \n",
    "                smoothing_function=smooth_fn.method1)\n",
    "            bleu_scores.append(bleu_score)\n",
    "\n",
    "    return np.mean(np.array(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3wJtsJ7gmyQs",
    "outputId": "8e644b89-0766-4f2a-ffa4-1a2b6a96b96f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.5690\n",
      "input    :  it s time .\n",
      "label    :  ha llegado el momento . EOS\n",
      "predicted:  no puedo dejar . EOS\n",
      "Eval Score (BLEU): 1.720e-02\n",
      "Time taken for 1 epoch 22.669869422912598 sec\n",
      "\n",
      "Epoch: 2, Loss: 1.2758\n",
      "input    :  stay positive .\n",
      "label    :  se positivo . EOS\n",
      "predicted:  ven a la policia . EOS\n",
      "Eval Score (BLEU): 2.134e-02\n",
      "Time taken for 1 epoch 17.747346878051758 sec\n",
      "\n",
      "Epoch: 3, Loss: 1.0407\n",
      "input    :  give me back my bag .\n",
      "label    :  devuelveme mi mochila . EOS\n",
      "predicted:  devuelveme mi dinero . EOS\n",
      "Eval Score (BLEU): 2.518e-02\n",
      "Time taken for 1 epoch 17.70408797264099 sec\n",
      "\n",
      "Epoch: 4, Loss: 0.9035\n",
      "input    :  she tried it herself .\n",
      "label    :  ella misma lo intento . EOS\n",
      "predicted:  ella le demando . EOS\n",
      "Eval Score (BLEU): 2.763e-02\n",
      "Time taken for 1 epoch 17.533161163330078 sec\n",
      "\n",
      "Epoch: 5, Loss: 0.7442\n",
      "input    :  you look worn out .\n",
      "label    :  te ves extenuado . EOS\n",
      "predicted:  pareces estresado . EOS\n",
      "Eval Score (BLEU): 3.340e-02\n",
      "Time taken for 1 epoch 17.65729308128357 sec\n",
      "\n",
      "Epoch: 6, Loss: 0.6763\n",
      "input    :  hold on to the rope .\n",
      "label    :  sostenete de la cuerda . EOS\n",
      "predicted:  agarrate a la cuerda . EOS\n",
      "Eval Score (BLEU): 3.755e-02\n",
      "Time taken for 1 epoch 17.547719478607178 sec\n",
      "\n",
      "Epoch: 7, Loss: 0.5409\n",
      "input    :  we re ready to help .\n",
      "label    :  estamos listas para ayudar . EOS\n",
      "predicted:  estamos listas para jugar . EOS\n",
      "Eval Score (BLEU): 4.499e-02\n",
      "Time taken for 1 epoch 17.55451250076294 sec\n",
      "\n",
      "Epoch: 8, Loss: 0.4630\n",
      "input    :  tom is just like me .\n",
      "label    :  tom es igual que yo . EOS\n",
      "predicted:  tom es mi unico hijo . EOS\n",
      "Eval Score (BLEU): 5.166e-02\n",
      "Time taken for 1 epoch 17.549461126327515 sec\n",
      "\n",
      "Epoch: 9, Loss: 0.4232\n",
      "input    :  they re all dead .\n",
      "label    :  todos estan muertos . EOS\n",
      "predicted:  ellos estan en paris . EOS\n",
      "Eval Score (BLEU): 5.796e-02\n",
      "Time taken for 1 epoch 17.531023025512695 sec\n",
      "\n",
      "Epoch: 10, Loss: 0.3611\n",
      "input    :  how tall you are !\n",
      "label    :  que altos que estan ! EOS\n",
      "predicted:  que alto que ! EOS\n",
      "Eval Score (BLEU): 6.637e-02\n",
      "Time taken for 1 epoch 17.777252674102783 sec\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-15c6041015eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         loss = train_step(\n\u001b[0;32m---> 17\u001b[0;31m             encoder_in, decoder_in, decoder_out, encoder_state)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}, Loss: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "epochs = 30\n",
    "eval_scores = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    encoder_state = encoder.init_state(batch_size)\n",
    "\n",
    "    for batch, data in enumerate(train_dataset):\n",
    "        encoder_in, decoder_in, decoder_out = data\n",
    "        # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\n",
    "        loss = train_step(\n",
    "            encoder_in, decoder_in, decoder_out, encoder_state)\n",
    "    \n",
    "    print(\"Epoch: {}, Loss: {:.4f}\".format(epoch + 1, loss.numpy()))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    predict(encoder, decoder, batch_size, sents_en_in, data_en,\n",
    "        sents_spa_out, word2idx_spa, idx2word_spa)\n",
    "\n",
    "    eval_score = evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_spa, idx2word_spa)\n",
    "    print(\"Eval Score (BLEU): {:.3e}\".format(eval_score))\n",
    "    # eval_scores.append(eval_score)\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw0_pIIAwZHb"
   },
   "source": [
    "# Seq2Seq의 다양한 구현들 - 선택적 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9ZLzm_hwbyw"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# cannot use strftime()'s %B format since it depends on the locale\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "\n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "\n",
    "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "xrzrloB7w2kn",
    "outputId": "52058b4c-5b48-485b-b5a3-282e38e6c2e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "September 20, 7075       7075-09-20               \n",
      "May 15, 8579             8579-05-15               \n",
      "January 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
    "print(\"-\" * 50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6_-yyfz3w5Om",
    "outputId": "9980c0bb-ea50-4a84-e2e1-adadcd8cb010"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'ADFJMNOSabceghilmnoprstuvy01234567890, '"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS)))) + \"01234567890, \"\n",
    "INPUT_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2JVdvY5xFFv"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHARS = \"0123456789-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WigZlbqVxGxN"
   },
   "outputs": [],
   "source": [
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Pm5RFTL-xIi3",
    "outputId": "5cfbce7b-4fe9-4979-a27b-034319fe1b32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 11, 19, 22, 11, 16, 9, 11, 20, 38, 28, 26, 37, 38, 33, 26, 33, 31]"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(x_example[0], INPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "PFaa0BixxLWM",
    "outputId": "33d4c13e-d1be-4a16-9c34-974eff809b5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"
      ]
     },
     "execution_count": 102,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZ8CRFYXxMfo"
   },
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # using 0 as the padding token ID\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERS6B0ThxO4U"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, Y_train = create_dataset(10000)\n",
    "X_valid, Y_valid = create_dataset(2000)\n",
    "X_test, Y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "692RR3AIxRBn",
    "outputId": "aa723336-46d6-4b6a-f7ff-d5fd107b70a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 8 12 20 23 12 17 10 12 21 39 29 27 38 39 34 27 34 32], shape=(18,), dtype=int32)\n",
      "tf.Tensor([ 8  1  8  6 11  1 10 11  3  1], shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-g43wI-xcIZ"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkbj8btWxw2z"
   },
   "source": [
    "## RepeatVector를 이용한 간단한 seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "PF2qd10GxXlS",
    "outputId": "fee236f6-6a76-41b7-830e-88956eed89e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 1.7989 - accuracy: 0.3572 - val_loss: 1.3409 - val_accuracy: 0.5070\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 1.2347 - accuracy: 0.5590 - val_loss: 1.0298 - val_accuracy: 0.6319\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.9777 - accuracy: 0.6537 - val_loss: 0.7880 - val_accuracy: 0.7036\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.6808 - accuracy: 0.7391 - val_loss: 0.6054 - val_accuracy: 0.7650\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.8583 - accuracy: 0.6910 - val_loss: 0.6805 - val_accuracy: 0.7345\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.5976 - accuracy: 0.7805 - val_loss: 0.4189 - val_accuracy: 0.8407\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.3288 - accuracy: 0.8827 - val_loss: 0.2718 - val_accuracy: 0.9061\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.3135 - accuracy: 0.9025 - val_loss: 0.1868 - val_accuracy: 0.9458\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.1415 - accuracy: 0.9643 - val_loss: 0.1120 - val_accuracy: 0.9750\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0806 - accuracy: 0.9852 - val_loss: 0.1871 - val_accuracy: 0.9498\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0572 - accuracy: 0.9909 - val_loss: 0.0404 - val_accuracy: 0.9943\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0288 - accuracy: 0.9975 - val_loss: 0.0253 - val_accuracy: 0.9978\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.1073 - accuracy: 0.9772 - val_loss: 0.0557 - val_accuracy: 0.9918\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0240 - accuracy: 0.9984 - val_loss: 0.0174 - val_accuracy: 0.9987\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0119 - accuracy: 0.9996 - val_loss: 0.0114 - val_accuracy: 0.9995\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0080 - accuracy: 0.9998 - val_loss: 0.0083 - val_accuracy: 0.9997\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.0058 - accuracy: 0.9999 - val_loss: 0.0062 - val_accuracy: 0.9998\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9999\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = Y_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n",
    "                           output_dim=embedding_size,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(max_output_length),\n",
    "    decoder\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RhHJGTJxjGQ"
   },
   "outputs": [],
   "source": [
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
    "            for sequence in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muuWUw4Gxkww"
   },
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "8YbdJ-rDxm7d",
    "outputId": "48767938-9b02-4622-889c-4156aed7cd6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-113-472ea7c41409>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-113-472ea7c41409>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict_classes(X_new)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiPKwFnjxo0b"
   },
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "cf4M_EoXxq7H",
    "outputId": "b1be1bf8-e544-4d4b-96a7-42a82a147e40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-02\n",
      "1789-02-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict_classes(X_new)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTwu78sNxtPy"
   },
   "outputs": [],
   "source": [
    "max_input_length = X_train.shape[1]\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    ids = model.predict_classes(X)\n",
    "    return ids_to_date_strs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ux6brsm_x2_P",
    "outputId": "4cfaa352-0e46-4c69-ec37-bb46dc8ab0f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 117,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgsPtAQpx4tJ"
   },
   "source": [
    "## 교사 강요를 사용한 seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlezmUBSx-Tz"
   },
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def shifted_output_sequences(Y):\n",
    "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
    "\n",
    "X_train_decoder = shifted_output_sequences(Y_train)\n",
    "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
    "X_test_decoder = shifted_output_sequences(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "gsB5dZTxyAMu",
    "outputId": "880ac742-f266-443d-dfba-694b977e85d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
       "array([[12,  8,  1, ..., 10, 11,  3],\n",
       "       [12,  9,  6, ...,  6, 11,  2],\n",
       "       [12,  8,  2, ...,  2, 11,  2],\n",
       "       ...,\n",
       "       [12, 10,  8, ...,  2, 11,  4],\n",
       "       [12,  2,  2, ...,  3, 11,  3],\n",
       "       [12,  8,  9, ...,  8, 11,  3]], dtype=int32)>"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "C_EkhKOoyEZO",
    "outputId": "60af9c73-3a48-4195-c4f6-43dcf0ecf583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.6901 - accuracy: 0.3714 - val_loss: 1.4141 - val_accuracy: 0.4604\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 1.2030 - accuracy: 0.5568 - val_loss: 0.9073 - val_accuracy: 0.6794\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6147 - accuracy: 0.7874 - val_loss: 0.3639 - val_accuracy: 0.8820\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.1920 - accuracy: 0.9567 - val_loss: 0.0957 - val_accuracy: 0.9895\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0939 - accuracy: 0.9853 - val_loss: 0.0420 - val_accuracy: 0.9984\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.0286 - accuracy: 0.9994 - val_loss: 0.0225 - val_accuracy: 0.9996\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0499 - accuracy: 0.9923 - val_loss: 0.0157 - val_accuracy: 0.9999\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(INPUT_CHARS) + 1,\n",
    "    output_dim=encoder_embedding_size)(encoder_input)\n",
    "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(\n",
    "    lstm_units, return_state=True)(encoder_embedding)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(OUTPUT_CHARS) + 2,\n",
    "    output_dim=decoder_embedding_size)(decoder_input)\n",
    "decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(\n",
    "    decoder_embedding, initial_state=encoder_state)\n",
    "decoder_output = keras.layers.Dense(len(OUTPUT_CHARS) + 1,\n",
    "                                    activation=\"softmax\")(decoder_lstm_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input, decoder_input],\n",
    "                           outputs=[decoder_output])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjSY270OyIUD"
   },
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - Y_pred.shape[1]\n",
    "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
    "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
    "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
    "    return ids_to_date_strs(Y_pred[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "vydN2abuyMw3",
    "outputId": "73e09060-5412-4ce5-c784-73ff5f53d570"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 124,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_Q7hKB2yRUR"
   },
   "source": [
    "## Tensorflow addon으로 만든 seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zXVY5bpy73x"
   },
   "source": [
    "텐서플로우 애드온 프로젝트는 여러가지 seq2seq 도구를 포함하고 있어 제품 수준의 seq2seq를 쉽게 만들 수 있음.  \n",
    "우선 인코더의 LSTM에서 return_state=True를 사용한 이후는 디코더에 hidden state를 전달하기 위함.  \n",
    "LSTM을 사용하므로 cell state, hidden state 두 개를 반환.  \n",
    "\n",
    "TrainingSampler는 텐서플로우 애드온에 포함되어져 있는 여러 샘플러 중 하나.  \n",
    "이 샘플러는 각 스텝에서 디코더에게 이전 스텝의 출력이 무엇이었는지 알려줌.  \n",
    "훈련 시에는 이전 타깃 토큰의 임베딩,  \n",
    "테스트 시에는 실제로 출력되는 토큰의 임베딩."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "HQyCkpE1yUAS",
    "outputId": "fe5f98f7-59e8-4ed0-8082-1c6d0972eeaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 1.6781 - accuracy: 0.3686 - val_loss: 1.4554 - val_accuracy: 0.4324\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 1.3493 - accuracy: 0.4809 - val_loss: 1.4071 - val_accuracy: 0.4809\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 0.9173 - accuracy: 0.6697 - val_loss: 0.6324 - val_accuracy: 0.7777\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.3964 - accuracy: 0.8757 - val_loss: 0.2084 - val_accuracy: 0.9556\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.1337 - accuracy: 0.9769 - val_loss: 0.0678 - val_accuracy: 0.9944\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 0.0653 - accuracy: 0.9917 - val_loss: 0.0409 - val_accuracy: 0.9976\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.0238 - accuracy: 0.9996 - val_loss: 0.0183 - val_accuracy: 0.9997\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.0357 - accuracy: 0.9932 - val_loss: 0.0421 - val_accuracy: 0.9918\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 0.0098 - accuracy: 0.9999 - val_loss: 0.0071 - val_accuracy: 0.9999\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 12s 40ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
    "                                                 sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings,\n",
    "    initial_state=encoder_state)\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                           outputs=[Y_proba])\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=15,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "eNLR3CXVyYot",
    "outputId": "3b49dd00-539d-4a2f-897a-d9ad6ef7c069"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 126,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAOok7Q8yahn"
   },
   "outputs": [],
   "source": [
    "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "    embedding_fn=decoder_embedding_layer)\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
    "    maximum_iterations=max_output_length)\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens,\n",
    "    initial_state=encoder_state,\n",
    "    start_tokens=start_tokens,\n",
    "    end_token=0)\n",
    "\n",
    "inference_model = keras.models.Model(inputs=[encoder_inputs],\n",
    "                                     outputs=[final_outputs.sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lH41WIefyc5Z"
   },
   "outputs": [],
   "source": [
    "def fast_predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = inference_model.predict(X)\n",
    "    return ids_to_date_strs(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3l_4yHeQyegs",
    "outputId": "48f4f11a-f446-418e-be58-2a54d2e1f489"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 129,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jni9BYxpyfwG",
    "outputId": "512223cc-7f70-4147-e249-f1ba5991b841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 454 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "L4DQ5iEiyiEF",
    "outputId": "88020e0c-8caa-4006-b7b9-e6c35589eeea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 44.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHUpIID7bi8N"
   },
   "source": [
    "# Greedy Search Vs. Beam Search - 선택적 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6owvXYYCcPcR"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fymMbyfGrIMU"
   },
   "outputs": [],
   "source": [
    "# 단어 집합의 크기가 5.\n",
    "# 길이 10까지의 시퀀스가 예측된 상태라고 하였을 때, 확률 분포를 통해 각 단어를 예측한다고 해보자.\n",
    "\n",
    "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1]]\n",
    "    \n",
    "data = array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ0wtBYIc5i7"
   },
   "source": [
    "## Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syY-O2YJcH6i"
   },
   "outputs": [],
   "source": [
    "# 그리디 디코더는 가장 확률이 높은 인덱스를 리턴한다.\n",
    "def greedy_decoder(data):\n",
    "\treturn [argmax(s) for s in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3v7dIpHLcQ2W",
    "outputId": "1180e58f-747f-4629-eac8-bf14d0d9adf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 0, 4, 0, 4, 0, 4, 0, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "result = greedy_decoder(data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sniA-i8GcZrd"
   },
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxDi9mmCcfqu"
   },
   "source": [
    "주어진 확률 시퀀스와 빔 크기 k에 대해 빔 탐색을 수행하는 함수를 작성한다.  \n",
    "\n",
    "- 각 후보 시퀀스는 가능한한 모든 다음 스텝들에 대해 확장된다.  \n",
    "- 각 후보는 확률을 곱함으로써 점수가 매겨진다.  \n",
    "- 가장 확률이 높은 k 시퀀스가 선택되고, 다른 모든 후보들은 제거된다.  \n",
    "- 위 절차들을 시퀀스가 끝날때까지 반복한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDzlrM7HcbSD"
   },
   "outputs": [],
   "source": [
    "# beam search\n",
    "def beam_search_decoder(data, k):\n",
    "\tsequences = [[list(), 0.0]]\n",
    "\t# walk over each step in sequence\n",
    "\tfor row in data:\n",
    "\t\tall_candidates = list()\n",
    "\t\t# expand each current candidate\n",
    "\t\tfor i in range(len(sequences)):\n",
    "\t\t\tseq, score = sequences[i]\n",
    "\t\t\tfor j in range(len(row)):\n",
    "\t\t\t\tcandidate = [seq + [j], score - log(row[j])]\n",
    "\t\t\t\tall_candidates.append(candidate)\n",
    "\t\t# order all candidates by score\n",
    "\t\tordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "\t\t# select k best\n",
    "\t\tsequences = ordered[:k]\n",
    "\treturn sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "z2A7V9pIcmuX",
    "outputId": "34275dcb-01cc-4a4d-93f1-f7e8e10d7934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 6.931471805599453]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 7.154615356913663]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 7.154615356913663]\n"
     ]
    }
   ],
   "source": [
    "result = beam_search_decoder(data, 3)\n",
    "\n",
    "for seq in result:\n",
    "  print(seq)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Learning Spoons 6강 (seq2seq).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
